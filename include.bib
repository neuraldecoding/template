% BibTeX entries for INCLUDED studies (N=102)
% Generated from screening_results.csv
% Source: transeeg.bib

@ARTICLE{Fan2026Medvia,
	author = {Fan, Wei and Fei, Jingru and Han, Jindong and Lian, Jie and Ye, Hangting and Song, Xiaozhuang and Lv, Xin and Yi, Kun and Li, Min},
	title = {MedViA: Empowering medical time series classification with vision augmentation and multimodal fusion},
	year = {2026},
	journal = {Information Fusion},
	volume = {127},
	pages = {},
	doi = {10.1016/j.inffus.2025.103659},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016995038&doi=10.1016%2Fj.inffus.2025.103659&partnerID=40&md5=aad5761e2cb32746e49eb8ac92e84ab3},
	affiliations = {University of Auckland, School of Computer Science, Auckland, AUK, New Zealand; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Shandong University, School of Artificial Intelligence, Jinan, Shandong, China; University of Oxford Medical Sciences Division, Oxford, Oxfordshire, United Kingdom; Jilin University, School of Artificial Intelligence, Changchun, Jilin, China; The Chinese University of Hong Kong, Shenzhen, School of Data Science, Shenzhen, Guangdong, China; State Information Center, Beijing, Beijing, China; Central South University, School of Computer Science, Changsha, Hunan, China},
	abstract = {The analysis of medical time series, such as Electrocardiography (ECG) and Electroencephalography (EEG), is fundamental to clinical diagnostics and patient monitoring. Accurate and automated classification of these signals can facilitate early disease detection and personalized treatment, thereby improving patient outcomes. Although deep learning models are widely adopted, they mainly process signals as sequential numerical data. Such a single-modality approach often misses the holistic visual patterns easily recognized by clinicians from graphical charts and struggles to model the complex non-linear dynamics of physiological data. As a result, the rich diagnostic cues contained in visual representations remain largely untapped, limiting model performance. To address these limitations, we propose MedViA, a novel multimodal learning framework that empowers Medical time series classification by integrating both Vision Augmentation and numeric perception. Our core innovation is to augment the raw medical time series signals into the visual modality, enabling a dual-pathway architecture that computationally mimics the comprehensive reasoning of clinical experts. With the augmentation, MedViA then features two parallel perception branches: a Visual Perception Module, built upon a novel Multi-resolution Differential Vision Transformer, processes the augmented images to capture high-level structural patterns and diagnostically critical waveform morphologies. Concurrently, a Numeric Perception Module, which uses our proposed Temporal Kolmogorov Network to model fine-grained and non-linear dynamics directly from the raw time series. To synergistically integrate the insights from these dedicated pathways, we introduce a Medically-informed Hierarchical Multimodal Fusion strategy, which uses a late-fusion architecture and a hierarchical optimization objective to derive for the final classification. We have conducted extensive experiments on multiple public medical time series datasets, which demonstrate the superior performance of our method compared to state-of-the-art approaches. © 2025},
	author_keywords = {Healthcare informatics; Medical time series; Multimodal learning; Time series classification; Vision augmentation},
	keywords = {Biomedical signal processing; Classification (of information); Computer aided diagnosis; Computer vision; Deep learning; Electrocardiography; Electroencephalography; Fusion reactions; Learning systems; Medical imaging; Medical informatics; Patient monitoring; Patient treatment; Time series; Time series analysis; Vision; Automated classification; Clinical diagnostics; Early disease detection; Health care informatics; Medical time series; Multi-modal fusion; Multi-modal learning; Non-linear dynamics; Time series classifications; Vision augmentation; Physiological models},
	correspondence_address = {K. Yi; State Information Center, Beijing, China; email: kunyi.cn

@ARTICLE{Cao2026Eegclip,
	author = {Cao, Xuhao and Gong, Peiliang and Zhang, Liying and Zhang, Daoqiang},
	title = {EEG-CLIP: A transformer-based framework for EEG-guided image generation},
	year = {2026},
	journal = {Neural Networks},
	volume = {194},
	pages = {},
	doi = {10.1016/j.neunet.2025.108167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105018169304&doi=10.1016%2Fj.neunet.2025.108167&partnerID=40&md5=e3591c868009a1ed98057f731f8e64c6},
	affiliations = {Nanjing University of Aeronautics and Astronautics, MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, Jiangsu, China},
	abstract = {Decoding visual perception from neural signals represents a fundamental step toward advanced brain-computer interfaces (BCIs), where functional magnetic resonance imaging (fMRI) has shown promising results despite practical constraints in deployment and costs. Electroencephalography (EEG), with its superior temporal resolution, portability, and cost-effectiveness, emerges as a promising alternative for real-time brain-computer interface (BCI) applications. While existing EEG-based approaches have advanced neural decoding capabilities, they remain constrained by inadequate architectural designs, limited reconstruction fidelity, and inconsistent evaluation protocols. To address these challenges, we present EEG-CLIP, a novel Transformer-based framework that systematically addresses each limitation: (1) We introduce a specialized EEG-ViT encoder that adeptly captures the spatial and temporal characteristics of EEG signals to augment model capacity, along with a Diffusion Prior Transformer architecture to approximate the image feature distribution. (2) We employ a dual-stage reconstruction pipeline that integrates class contrastive learning and pretrained diffusion models to enhance visual reconstruction quality. (3) We establish comprehensive evaluation protocols across multiple datasets. Our framework operates through two stages: first projecting EEG signals into CLIP image space via class contrastive learning and refining them into image priors, then reconstructing perceived images through a pretrained conditional diffusion model. Comprehensive empirical analysis, including temporal window sensitivity studies and regional brain activation visualization, demonstrates the framework's robustness. We demonstrate through ablations that EEG-CLIP's performance improvements over previous methods result from specialized architecture for EEG encoding and improved training techniques. Quantitative and qualitative evaluations on ThingsEEG and Brain2Image datasets establish EEG-CLIP's state-of-the-art performance in both classification and reconstruction tasks, advancing neural signal-based visual decoding capabilities. © 2025 Elsevier Ltd},
	author_keywords = {Brain decoding; Diffusion models; Electroencephalogram; Transformer},
	keywords = {Activation analysis; Architecture; Biomedical signal processing; Brain; Brain computer interface; Brain mapping; Cost effectiveness; Decoding; Diffusion; Electrophysiology; Image coding; Image reconstruction; Interfaces (computer); Magnetic resonance imaging; Signal encoding; Brain decoding; Diffusion model; Evaluation protocol; Functional magnetic resonance imaging; Guided images; Image generations; Neural signals; Temporal resolution; Transformer; Visual perception; Electroencephalography; Article; brain analysis; brain region; computer model; cost effectiveness analysis; data visualization; diffusion; EEG CLIP; electroencephalogram; electroencephalography; functional magnetic resonance imaging; human; image analysis; image reconstruction; image segmentation; learning; mathematical analysis; neuroimaging; spatial analysis; supervised machine learning; temporal cortex; time series analysis; transformer based framework; visual stimulation; brain; brain computer interface; image processing; physiology; procedures; vision; Brain-Computer Interfaces; Humans; Image Processing, Computer-Assisted; Visual Perception},
	correspondence_address = {P. Gong; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, College of Artificial Intelligence, Nanjing University of Aeronautics and Astronautics, Nanjing, 211106, China; email: plgong

@ARTICLE{Abbasi2026Motor,
	author = {Abbasi, Hafza Faiza and Abbasi, Muhammad Ahmed and Aziz, Muhammad Zulkifal and Huang, Binwen and Fan, Zeming and Wu, Xiaohua and Yu, Xiaojun},
	title = {Motor imagery classification using parallel convolution transformer based feature extraction and RSA optimized ridge ELM classifier},
	year = {2026},
	journal = {Biomedical Signal Processing and Control},
	volume = {112},
	pages = {},
	doi = {10.1016/j.bspc.2025.108443},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013677441&doi=10.1016%2Fj.bspc.2025.108443&partnerID=40&md5=7261eb464941c5193c3367f309d0daec},
	affiliations = {Northwestern Polytechnical University, School of Automation, Xi'an, Shaanxi, China; Hainan Medical University, Key Laboratory of Tropical Translational Medicine, Haikou, Hainan, China; National Key Laboratory of Air-based Information Perception and Fusion, Henan, China; Northwestern Polytechnical University, Xi'an, Shaanxi, China},
	abstract = {Being a time-series data, EEG data has strong temporal and spatial dependencies across the various time points and channels that possess substantial information. Convolutional neural networks (CNNs) are widely employed for motor imagery (MI) classification to extract these correlations and decode electroencephalography (EEG) signals. However, the inherent small perceptual field of CNNs limits their use in extracting global dependencies, leading to substantial information loss over time. Transformers can potentially mitigate this problem with their attention mechanism capable of extracting the correlation among features in the long-sequence time-series data. This study proposes a novel end-to-end framework for EEG decoding using improved feature extraction and classification methods to enhance the MI classification performance. Specifically, a parallel architecture of transformer (TF) and CNN (TF-CNN) is proposed to extract the global dependencies and local temporal features. The local temporal features from CNN and the global correlation features from the transformer are further fused together and classified using ridge regression extreme learning machine (rELM). Moreover, reptile search algorithm (RSA) is utilized to optimize the classification performance of rELM. The proposed framework is evaluated on four public datasets i.e., BCI Competition III dataset IVa, Open BMI dataset, BCI Competition IV dataset 2a and 2b, yielding average accuracies of 92.64%, 79.26 %, 84.90% and 86.77%, respectively. Such experimental results demonstrate superior performance compared to the existing methods. In addition, the proposed method, CNN-TF-rELM-RSA, hereby named as CTER also highlights the potential of meta-heuristic algorithms to improve the generalization capabilities of the ELM classifier for EEG decoding. © 2025 Elsevier Ltd},
	author_keywords = {Brain–computer interface (BCI); Convolutional neural networks (CNNs); Electroencephalography (EEG); Extreme learning machine (ELM); Motor imagery (MI); Reptile search optimization (RSA); Transformer},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Convolution; Convolutional neural networks; Extraction; Feature extraction; Heuristic algorithms; Heuristic methods; Image classification; Interfaces (computer); Knowledge acquisition; Learning algorithms; Learning systems; Parallel architectures; Time series; Brain–computer interface; Convolutional neural network; Electroencephalography; Extreme learning machine; Learning machines; Motor imagery; Reptile search optimization; Search optimization; Transformer; Electrophysiology; Article; classification; electroencephalogram; extreme learning machine; feature extraction; human; imagery; ridge regression},
	correspondence_address = {X. Yu; School of Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, 710072, China; email: XJYU

@ARTICLE{Chen2026Daslstm,
	author = {Chen, Gengbiao and Li, Haolong and Yan, Hong},
	title = {DAS-LSTM: A dual attention-enhanced LSTM with frequency band optimization for EEG-based motor imagery classification in brain-computer interfaces},
	year = {2026},
	journal = {Biomedical Signal Processing and Control},
	volume = {112},
	pages = {},
	doi = {10.1016/j.bspc.2025.108616},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013645734&doi=10.1016%2Fj.bspc.2025.108616&partnerID=40&md5=ce7fc7d9c5b9574a536b749984a25e61},
	affiliations = {Changsha University of Science and Technology, College of Mechanical and Vehicle Engineering, Changsha, Hunan, China; Shenyang General Hospital of PLA, Department of Stomatology, Shenyang, Liaoning, China},
	abstract = {Background: Accurate classification of MI from EEG signals is crucial for non-invasive BCIs, especially for individuals with motor impairments. However, existing methods often overlook the synergies across multiple frequency bands, limiting their discriminative power. To address this limitation, we propose DAS-LSTM, a hybrid framework that integrates FBCSP for multi-band feature extraction, a simplified LSTM variant with reduced gating complexity, and a dual attention mechanism that prioritizes task-relevant temporal and spectral features. Method: The proposed model combines the foundational architecture of LSTM networks with an advanced attention mechanism. DAS-LSTM consists of two attention layers and one newly designed LSTM variant. The model's effectiveness was evaluated using the BCI-IV-2a and BCI-IV-2b datasets. Additionally, ablation experiments were conducted to assess the impact of the newly proposed LSTM variant on the model's overall performance. Conclusion: This study demonstrates the efficacy of the DAS-LSTM framework in accurately classifying motor imagery signals across different EEG categories. The findings highlight the potential of the proposed model to contribute to the development of more intuitive and natural prosthetic control systems. © 2025},
	author_keywords = {Attention; CSP (Common Spatial Pattern); EEG (Electroencephalography); LSTM (Long Short-Term Memory); MI (Motor Imagination)},
	keywords = {Biomedical signal processing; Classification (of information); Electroencephalography; Electrophysiology; Frequency bands; Image classification; Interfaces (computer); Long short-term memory; Attention; Attention mechanisms; Band optimization; Common spatial pattern; Common spatial patterns; Motor imagery classification; Motor imagination; Short term memory; Brain computer interface; Article; attention; chemical modification; classification algorithm; comparative study; confusion matrix; dual attention spatial long short term memory; electroencephalography; feature selection algorithm; frequency analysis; human; image artifact; limb movement; long short term memory network; motor imagery classification; sensory feedback; signal processing; temporal analysis},
	correspondence_address = {H. Yan; Department of Stomatology, General Hospital of Northern Theater Command, Shenyang, 110016, China; email: 343722700

@ARTICLE{Sehnan2026Frequencyguided,
	author = {Sehnan, Moeed and Li, Haoyu and Guo, Wei and Bello, Isah and Gao, Zhongke and Dang, Weidong},
	title = {A frequency-guided temporal convolutional network with transfer learning for motor imagery classification},
	year = {2026},
	journal = {Biomedical Signal Processing and Control},
	volume = {112},
	pages = {},
	doi = {10.1016/j.bspc.2025.108590},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013628729&doi=10.1016%2Fj.bspc.2025.108590&partnerID=40&md5=762e2ab6775f705a7f358f119d6e1995},
	affiliations = {Tianjin University, Tianjin, China},
	abstract = {Brain–computer interfaces (BCIs) convert brain activity patterns into commands for external devices, which is especially valuable in medical science and rehabilitation. In non-invasive BCIs, motor imagery electroencephalogram (MI-EEG) is a commonly used method for collecting brain signals. However, decoding MI-EEG signals faces the fundamental challenges of inter-subject variability and overlapping spectral-spatial features across different motor imagery classes. The proposed model is a novel strategy for decoding MI tasks using multi-scale feature extraction and a frequency-guided temporal convolutional network with transfer learning. The model extracts features using three key components: a temporal block, a multi-depth block, and an inception block. A channel attention mechanism is integrated into a temporal block to focus on the most informative channels. The multi-depth block captures multi-scale features by aggregating information across convolutional layers of varying depths. This enhances feature representation by capturing both fine-grained and abstract spatial patterns, which are crucial for accurate MI-EEG decoding. The frequency attention and the temporal convolutional block enhance temporal consistency and frequency-specific pattern recognition by amplifying task-relevant frequency information. A task-specific pretraining strategy is employed, where the model is first trained on data from all nine subjects to learn general MI-related EEG patterns, followed by subject-specific fine-tuning. On the BCI Competition IV-2a dataset, our model achieves an average classification accuracy of 81.33% (kappa: 0.75) for four classes, outperforming the state-of-the-art method by 2.05%. These results provide a precise and objective evaluation of the model's effectiveness. © 2025 Elsevier Ltd},
	author_keywords = {Brain–computer interface (BCI); Convolutional neural networks; Electroencephalogram (EEG); Motor imagery (MI); Multiple attentions mechanism; Transfer learning},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Convolution; Convolutional neural networks; Electroencephalography; Image classification; Interfaces (computer); Learning systems; Medical computing; Neurophysiology; Attention mechanisms; Brain–computer interface; Convolutional networks; Convolutional neural network; Electroencephalogram; Motor imagery; Multi-scale features; Multiple attention mechanism; Transfer learning; Classification (of information); adult; Article; convolutional neural network; electroencephalogram; feature extraction; female; human; human experiment; imagery; information processing; male; motor imagery; pattern recognition; transfer of learning},
	correspondence_address = {W. Dang; School of Electrical and Information Engineering, Tianjin University, Tianjin, Tianjin, 300072, China; email: weidongdang

@ARTICLE{Dong2026Sumamba,
	author = {Dong, Liuyuan and Xu, Chengzhi and Wang, Xuyang and Xie, Ruizhen and Lei, Guangbo and Li, Yimemg and Yang, Wanli},
	title = {SUMamba: A Mamba-based deep learning model with multi-scale feature fusion for SSVEP classification},
	year = {2026},
	journal = {Biomedical Signal Processing and Control},
	volume = {112},
	pages = {},
	doi = {10.1016/j.bspc.2025.108376},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013215687&doi=10.1016%2Fj.bspc.2025.108376&partnerID=40&md5=ff98fe0cc181fc306267e47af4d06a1e},
	affiliations = {Hubei University of Technology, School of Computing, Wuhan, Hubei, China; Central China Normal University, School of Computing, Wuhan, Hubei, China; Huazhong University of Science and Technology, School of Artificial Intelligence and Automation, Wuhan, Hubei, China},
	abstract = {Steady-state visual evoked potential (SSVEP) has become one of the core technologies for efficient human–computer interaction in brain-computer interfaces (BCI) due to its high signal-to-noise ratio and fast information transfer rate. With the development of deep learning technologies, especially the Transformer architecture, which has powerful feature extraction capabilities, it has been widely applied to EEG signal classification tasks. However, the quadratic time complexity of Transformer limits its efficiency in processing long sequence data. In contrast, Mamba, as an emerging deep learning architecture, has shown significant advantages in computational complexity and long sequence modeling. In this paper, we propose a novel architecture called SUMamba for SSVEP classification based on Mamba and multi-scale feature fusion technology. It uses the frequency-domain information of the signal as input and employs a spatial attention encoder to enhance features. Additionally, we propose an extended model called FB-SUMamba using filter bank technique to improve the model's utilization efficiency of harmonic features. Finally, we compare the model performance with six other existing methods at four time window scales ranging from 0.5 s to 2 s. Experiments show that SUMamba achieves higher average classification accuracy and information transfer rate (ITR) than other experimental methods on 40 and 12 classification tasks across three public datasets. The corresponding code can be accessed through https://github.com/dlyres/SUMamba. © 2025 Elsevier Ltd},
	author_keywords = {Brain-computer interface; Filter bank; Mamba; Multi-scale feature fusion; Steady-state visual evoked potential},
	keywords = {Architecture; Bioelectric potentials; Classification (of information); Data handling; Deep learning; Efficiency; Electrophysiology; Filter banks; Interface states; Interfaces (computer); Signal to noise ratio; Classification tasks; Features fusions; Filters bank; Information transfer rate; Learning models; Long sequences; Mamba; Multi-scale feature fusion; Multi-scale features; Steady-state visual evoked potentials; Brain computer interface; accuracy; Article; back propagation; comparative study; cross validation; deep learning; electroencephalogram; electroencephalography; human; muscle contraction; noise; spatial attention; steady state; steady state visual evoked potential; visual evoked potential; waveform},
	correspondence_address = {C. Xu; Hubei Provincial Key Laboratory of Green Intelligent Computing Power Network, School of Computer, Hubei University of Technology, Wuhan, Hubei, China; email: xcz911

@ARTICLE{Zhao2026Daquadvit,
	author = {Zhao, Zhenxi and Cao, Yingyu and Yu, Hongbin and Huang, Junfen},
	title = {DA-QuadViT: A domain adaptation motor imagery classification model based on a novel Quadruple ViT structure},
	year = {2026},
	journal = {Expert Systems with Applications},
	volume = {297},
	pages = {},
	doi = {10.1016/j.eswa.2025.129248},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013124580&doi=10.1016%2Fj.eswa.2025.129248&partnerID=40&md5=8cfbb4caa9381fbc4f95b9570985262a},
	affiliations = {Tiangong University, College of Mechanical Engineering, Tianjin, China; Beijing Institute of Petrochemical Technology, Beijing, China},
	abstract = {Motor imagery (MI) has garnered significant attention as one of the most promising paradigms in brain-computer interfaces (BCIs). Exploring algorithms for MI decoding and developing corresponding models are crucial. However, inherent physiological differences between individuals cause a mismatch between target and source domains in BCI decoding, thereby affecting accuracy. With the widespread adoption of deep learning methods in the field of BCIs, particularly the recent surge in transformer models, overcoming these challenges has become more feasible. In this study, we propose a novel Quadruple ViT structure based on the vision transformer (ViT), which facilitates feature interaction between the source and target domains to improve classification accuracy. Based on this structure, we develop a domain adaptation MI classification model, DA-QuadViT. DA-QuadViT first characterizes Electroencephalograms (EEGs) spatio-temporal features using a temporal convolutional network (TCN) and cross-channel convolutional block. It then enables inter-domain feature interaction and intra-domain feature retention through the Quadruple ViT structure and a domain attention block. Furthermore, the model's generalization is enhanced by multi-task training. Finally, we conducted extensive experiments on four public datasets: BCI competition IV dataset 2a, BCI competition IV dataset 2b, high gamma dataset, and OpenBMI. The results show that DA-QuadViT improves the accuracy of the best-performing baseline method by 2.54 %, 2.57 %, 1.31 %, and 3.36 %, respectively. These findings demonstrate that DA-QuadViT is an effective end-to-end MI classification model and has the potential for generalization to other EEG decoding tasks. © 2025 Elsevier Ltd},
	author_keywords = {Brain-computer interface (BCI); Domain adaptation; Electroencephalogram (EEG); Motor imagery (MI); Transformer},
	keywords = {Classification (of information); Communication channels (information theory); Convolution; Decoding; Deep learning; Electroencephalography; Image classification; Interfaces (computer); Learning systems; Multi-task learning; Physiological models; Brain-computer interface; Classification models; Domain adaptation; Electroencephalogram; Feature interactions; Motor imagery; Motor imagery classification; Transformer; Transformer structure; Brain computer interface},
	correspondence_address = {Y. Cao; Opto-Mechatronic Equipment Technology Beijing Area Major Laboratory, Beijing Institute of Petrochemical Technology, Beijing, No.19 qingyuan North Road, Daxing District, 102617, China; email: caoyingyu

@ARTICLE{Zhang2026Mbbitcnet,
	author = {Zhang, Zhun and Wang, Li and Li, Jin and Huang, Mingyang and Feng, Yujie},
	title = {MBBi-TCNet: Multi-branch bi-directional temporal convolutional network for EEG classification of mental imagery},
	year = {2026},
	journal = {Biomedical Signal Processing and Control},
	volume = {111},
	pages = {},
	doi = {10.1016/j.bspc.2025.108381},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012383883&doi=10.1016%2Fj.bspc.2025.108381&partnerID=40&md5=71080f9a34175de64ed44aadce30f412},
	affiliations = {Guangzhou University, Department of Electronics and Communication Engineering, Guangzhou, Guangdong, China},
	abstract = {The brain-computer interface (BCI) is a cutting-edge technology that makes the communication between the brain and external devices possible. However, due to the low signal-to-noise ratio and non-stationarity of electroencephalography (EEG) signals, the EEG-based BCIs encounter considerable challenges. In this study, a multi-branch bi-directional temporal convolutional network (MBBi-TCNet) is proposed for mental imagery decoding. By combining an attention mechanism module and a Bi-TCN module, the proposed model adopts a multi-branch network architecture to extract more useful features. By integrating the information bidirectionally, the long-range dependencies in sequences can be captured by the Bi-TCN module. The performance of the proposed model is assessed by utilizing the cross-validation method on three datasets. In the subject-dependent scenario, MBBi-TCNet achieves average classification results of 86.15 % (within-session) and 83.22 % (cross-session) on the BCIC IV2a dataset, and 80.97 % (within-session) and 86.53 % (cross-session) on the BCIC IV2b dataset, respectively. Additionally, it demonstrates an accuracy of 75.69 % (within-session) on the private dataset. In the subject-independent scenario, MBBi-TCNet also outperforms all baseline models. According to the classification results, MBBi-TCNet is superior to other state-of-the-art models. Therefore, the practical application of BCIs can be enhanced by the proposed model. © 2025 Elsevier Ltd},
	author_keywords = {Brain-computer interface (BCI); Deep learning; Electroencephalography (EEG); Mental imagery; Temporal convolutional network},
	keywords = {Arts computing; Biomedical signal processing; Bismuth alloys; Brain computer interface; Classification (of information); Computer networks; Convolution; Electroencephalography; Image classification; Interfaces (computer); Bi-directional; Brain-computer interface; Classification results; Convolutional networks; Cutting edge technology; Deep learning; Low signal-to-noise ratio; Mental imagery; Temporal convolutional network; Electrophysiology; adult; Article; cross validation; data accuracy; deep learning; electroencephalogram; electroencephalography; feature extraction; female; human; human experiment; imagery; machine learning; male; mental imagery; multibranch bidirectional temporal convolutional network; multiclass classification; normal human},
	correspondence_address = {L. Wang; School of Electronics and Communication Engineering, Guangzhou University, Guangzhou, 510006, China; email: wangli

@ARTICLE{Dinh2025Channel,
	author = {Dinh, Quang Pham Lam and Nambu, Isao},
	title = {Channel Mix-Enhanced Attention Temporal Convolutional Network for EEG-Based Motor-Imagery Classification},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {214990 - 215002},
	doi = {10.1109/ACCESS.2025.3646269},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105025410613&doi=10.1109%2FACCESS.2025.3646269&partnerID=40&md5=e823a9266cff3548bfe31b07728c272f},
	affiliations = {Nagaoka University of Technology, Nagaoka, Niigata, Japan},
	abstract = {Electroencephalogram (EEG)-based motor-imagery (MI) brain-computer interfaces (BCIs) still struggle to extract subject-invariant, neurophysiologically meaningful features. We propose the Learnable Channel-Mixing Matrix with Attention-based Temporal Convolutional Network (LATCNet), a deep architecture that explicitly combines learnable spatial mixing, attention, and temporal modeling for MI-EEG classification. LATCNet begins with a Learnable Channel-Mixing Matrix (LCMM), initialized as the identity and applied as a dense, time-shared linear transformation across EEG channels, allowing the model to discover task-specific inter-channel dependencies without hand-crafted connectivity graphs. Early and late squeeze-and-excitation (SE) blocks then adaptively re-weight the mixed signals and deep spatio-temporal features. On top of this spatial front-end, a sliding-window multi-head self-attention module and gated temporal convolutional network (TCN) layers capture long-range temporal dynamics with efficient inference. Evaluated on the BCI Competition IV 2a and 2b datasets, LATCNet achieves 89.2% and 90.5% accuracy in within-subject settings and 72.3% and 86.7% in cross-subject scenarios, consistently surpassing ATCNet and other convolutional TCN baselines. These results demonstrate that identity-initialized channel mixing, lightweight attention, and gated TCNs provide an effective design for robust, interpretable EEG-based MI decoding. © 2013 IEEE.},
	author_keywords = {ATCNet; Brain-computer interface; cross-subject; deep ensemble learning; deep learning; EEG; learnable mixing matrix; motor imagery; motor imagery; multi-class classification; squeeze-excitation; temporal convolution},
	keywords = {Biomedical signal processing; Brain; Classification (of information); Communication channels (information theory); Convolution; Convolutional neural networks; Deep learning; Electroencephalography; Image classification; Interfaces (computer); Learning systems; Linear transformations; Matrix algebra; Mixer circuits; Mixing; Network layers; Neurophysiology; ATCNet; Cross-subject; Deep ensemble learning; Ensemble learning; Learnable mixing matrix; Mixing matrix; Motor imagery; Multi-class classification; Squeeze-excitation; Temporal convolution; Brain computer interface},
	correspondence_address = {I. Nambu; Nagaoka University of Technology, Niigata, 940-2137, Japan; email: inambu

@ARTICLE{Yang2025Edanet,
	author = {Yang, Jiafeng and Wang, Li and Cai, Wenyue and Zhang, Lihan and Xie, Chengqiang and Wang, Zichen},
	title = {EDANet: Efficient domain-adaptive attention neural network for EEG classification of motor imagery},
	year = {2025},
	journal = {Expert Systems with Applications},
	volume = {294},
	pages = {},
	doi = {10.1016/j.eswa.2025.128783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009340787&doi=10.1016%2Fj.eswa.2025.128783&partnerID=40&md5=ed8d2146abb173c1f0f4ed03c7b39ea5},
	affiliations = {Guangzhou University, Department of Electronics and Communication Engineering, Guangzhou, Guangdong, China},
	abstract = {Brain-Computer Interface (BCI) is a cutting-edge technology enabling communication between the brain and external devices. However, due to the non-stationarity of electroencephalography (EEG) signals, which leads to domain discrepancies between source and target domains, and the difficulty in extracting robust features from low signal-to-noise ratio (SNR) EEG signals, the EEG-based BCIs face significant challenges. In this study, an efficient domain-adaptive attention neural network (EDANet) is proposed for motor imagery decoding. In this model, a domain-adaptive spatial filter and a bidirectional attention temporal convolutional module (Bi-ATCN) are proposed to extract more useful features. The domain-adaptive spatial filter reduces domain discrepancies by aligning covariance matrices of EEG signals across different sessions and enhances the overall SNR by emphasizing the importance of distinct electrode channels. Compared to conventional unidirectional temporal models, the proposed Bi-ATCN captures both forward and backward temporal dependencies, leading to richer temporal context modeling. Moreover, Bi-ATCN integrates an efficient bi-layer attention mechanism (EBAM) to further improve temporal feature representation. To evaluate the proposed approach, extensive experiments were conducted on two publicly available EEG datasets BCIC IV-2a and BCIC IV-2b, achieving competitive average classification accuracies of 84.11% and 86.03%, respectively. Compared to state-of-the-art models, EDANet demonstrates superior classification performance, highlighting its potential for enhancing the practical application of BCIs. © 2025 Elsevier Ltd},
	author_keywords = {Attention mechanisms; Brain-computer interface (BCI); Convolutional neural networks; Domain adaptation; Electroencephalography (EEG); Motor imagery (MI)},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Convolution; Covariance matrix; Electroencephalography; Image classification; Interfaces (computer); Neural networks; Signal to noise ratio; Adaptive spatial filter; Attention mechanisms; Brain-computer interface; Convolutional neural network; Domain adaptation; Motor imagery; Network efficient; Neural-networks; Electrophysiology},
	correspondence_address = {L. Wang; School of Electronics and Communication Engineering, Guangzhou University, Guangzhou, 510006, China; email: wangli

@ARTICLE{Mathiyazhagan2025Crossdomain,
	author = {Mathiyazhagan, Sathish and Devasena, M. S.Geetha},
	title = {Cross-domain Spatio-temporal and Multi-scale Residual Attention Mechanisms for Robust EEG Motor Imagery Classification},
	year = {2025},
	journal = {International Journal of Computational Intelligence Systems},
	volume = {18},
	number = {1},
	pages = {},
	doi = {10.1007/s44196-025-01040-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020852190&doi=10.1007%2Fs44196-025-01040-x&partnerID=40&md5=597a20bbf3f63c724397ef89076e3291},
	affiliations = {Kumaraguru College of Technology, Department of Computer Science and Engineering, Coimbatore, TN, India; Sri Ramakrishna Engineering College, Department of Computer Science and Engineering, Coimbatore, TN, India},
	abstract = {Motor imagery (MI)-based classification of EEG signals plays a crucial role in developing effective brain–machine interface (BMI) systems used in assistive control and neuro-rehabilitation. However, conventional approaches face challenges due to low signal clarity, subject-wise variability, and insufficient utilization of spatial and temporal signal dependencies. Addressing these concerns, this study proposes a unified spatial–temporal multi-scale attention mechanism (UST-MSAM), which combines cross-domain spatial–temporal attention and dynamic residual multi-scale attention. The design incorporates graph-guided attention layers to extract inter-channel spatial dynamics and utilizes frequency-adaptive attention paths to uncover salient temporal cues across EEG sub-bands. In addition, a hybrid encoder with residual attention refinements is employed to suppress irrelevant signal components and enhance the retention of critical features. The performance of the proposed method was evaluated on two benchmark datasets. For the BCI dataset, UST-MSAM achieved an accuracy of 97.5%, outperforming existing models such as BiLSTM (94.0%), ADBN-FNO (95.7%), and SSTS-Net (96.9%) by margins of 3.5%, 1.8%, and 0.6%, respectively. Similarly, on the PhysioNet dataset, it attained a peak accuracy of 96.4%, marking improvements over BiLSTM (93.0%), ADBN-FNO (94.1%), and SSTS-Net (95.9%). These results confirm consistent gains in classification accuracy, with added improvements in specificity (up to 98.7% for BCI and 96.8% for PhysioNet), precision, recall, and F1-measure across both datasets. The findings demonstrate the potential of UST-MSAM in building robust and generalizable EEG classification systems for real-world neuro-interfacing applications. © The Author(s) 2025.},
	author_keywords = {BCI dataset; Brain activity patterns; Brain–machine interface; EEG; UST-MSAM},
	keywords = {Benchmarking; Biomedical signal processing; Brain; Brain computer interface; Electroencephalography; Image classification; Neurophysiology; Attention mechanisms; BCI dataset; Brain activity patterns; Brain–machine interface; Cross-domain; Machine interfaces; Multi-scales; PhysioNet; Spatial temporals; Unified spatial–temporal multi-scale attention mechanism; Classification (of information)},
	correspondence_address = {S. Mathiyazhagan; Department of Computer Science and Engineering, Kumaraguru College of Technology, Coimbatore, 641049, India; email: sathishmathiyazhagan

@ARTICLE{Zhang2025Siamese,
	author = {Zhang, Wei and Tang, Xianlun and Dang, Xiaoyuan and Wang, Mengzhou},
	title = {Siamese network based high discriminative feature learning strategy for EEG classification of motor imagery},
	year = {2025},
	journal = {Biomedical Signal Processing and Control},
	volume = {110},
	pages = {},
	doi = {10.1016/j.bspc.2025.108315},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009493240&doi=10.1016%2Fj.bspc.2025.108315&partnerID=40&md5=738fd6f5b8d938aa5fc5a722423bbf51},
	affiliations = {Chongqing University of Posts and Telecommunications, School of Computer Science and Technology, Chongqing, Chongqing, China; Chongqing Traditional Chinese Medicine Hospital, School of General Education, Chongqing, Chongqing, China; Chongqing University of Posts and Telecommunications, School of Automation, Chongqing, Chongqing, China; Chongqing College of Mobile Communication, School of Intelligent Engineering, Chongqing, Chongqing, China},
	abstract = {Motor imagery brain computer interface is a technology that controls external devices by decoding brain motor imagery signals. It has a wide range of applications in fields such as rehabilitation medicine, virtual reality, and military. The field of motor imagery EEG classification faces challenges such as complex feature extraction, difficulty in capturing long-range dependencies, and bottlenecks in improving classification accuracy. This article proposes a novel deep learning model aimed at improving the classification performance of motor imagery electroencephalogram (EEG) signals. The model takes Siamese Neural Networks as the core framework, creatively integrates space–time convolution and multi head self-attention, and constructs two left and right sisters networks with identical structures. The spatiotemporal convolutional network is responsible for capturing the spatial distribution and temporal dynamic features of highly discriminative features in EEG signals, while the multi head self-attention module further models the long-range dependencies in the signals. At the same time, constructing a so-called triple loss function maximizes the similarity of EEG feature embeddings among different subjects, maps the EEG features of specific subjects to a common representation space, and further enhances the model's generalization ability. The experimental results show that the model achieves better classification performance than the baseline method on the BCI IV-2a benchmark dataset, especially demonstrating strong transfer and generalization abilities when dealing with cross subject motion imagery tasks. © 2025 Elsevier Ltd},
	author_keywords = {Brain computer interface; Convolution neural network; Self-attention; Transformer network},
	keywords = {Benchmarking; Biomedical signal processing; Classification (of information); Convolution; Convolutional neural networks; Deep learning; Electroencephalography; Image classification; Interfaces (computer); Learning systems; Military applications; Virtual reality; Classification performance; Convolution neural network; Discriminative features; Electroencephalogram signals; Generalization ability; Long-range dependencies; Motor imagery; Network-based; Self-attention; Transformer network; Brain computer interface; Article; benchmarking; classification; convolutional neural network; deep learning; electroencephalogram; feature extraction; human; human experiment; imagery; motion; nerve cell network; virtual reality},
	correspondence_address = {X. Tang; School of Automation, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China; email: 986071898

@ARTICLE{Fan2025Subjectadaptive,
	author = {Fan, Mingyang and Sang, Zhenhua and Wu, Jian and Guo, Yuzhu},
	title = {Subject-adaptive SSVEP decoding based on time–frequency information},
	year = {2025},
	journal = {Biomedical Signal Processing and Control},
	volume = {110},
	pages = {},
	doi = {10.1016/j.bspc.2025.108141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008722051&doi=10.1016%2Fj.bspc.2025.108141&partnerID=40&md5=2e2043ba922c05b8c6feacfb6206deb4},
	affiliations = {Beihang University, School of Automation Science and Electrical Engineering, Beijing, China; Beihang University, Beijing, China; Tsinghua University, IT service department, Beijing, China; Tsinghua University, School of Healthcare Management, Beijing, China; Tsinghua University, Department of Neurology, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Institute for Precision Medicine, Beijing, China},
	abstract = {Steady-State Visual Evoked Potential (SSVEP) based Brain–Computer Interface (BCI) has been widely used. While unsupervised methods like Filter Bank Canonical Correlation Analysis (FBCCA) perform well in long time windows, it performances significantly declines in short time windows. Supervised methods such as Task Related Component Analysis (TRCA), on the other hand, perform well in short time windows but exhibit weak under cross-subject generalization. To address these issues, this paper introduces the self-attention mechanism from the Transformer into the SSVEP decoding task to enhance the model's cross-subject adaptability. To learn individualized SSVEP features, this method fully leverages the spatiotemporal, frequency, and phase information in EEG, using segment embedding and position embedding to differentiate these features. Additionally, a token as additional channel information is incorporated to gather other channels’ information for classification. The proposed approach achieved promising results on two commonly used public SSVEP datasets, demonstrating better performance in short time windows and cross-subject conditions compared to traditional unsupervised and supervised models, as well as supervised deep learning models. © 2025 Elsevier Ltd},
	author_keywords = {Domain generalization; SSVEP; Transformer},
	keywords = {Bioelectric potentials; Brain computer interface; Classification (of information); Correlation methods; Deep learning; Electrophysiology; Image segmentation; Interfaces (computer); Learning systems; Supervised learning; Unsupervised learning; Channel information; Domain generalization; Embeddings; Filters bank; Generalisation; Short time windows; Steady-state visual evoked potentials; Time frequency information; Transformer; Unsupervised method; Interface states; Article; clinical classification; correlation analysis; decoding; deep learning; electroencephalography; embedding; feature extraction; frequency analysis; human; medical information; spatiotemporal analysis; steady state; steady state visual evoked potential; task related component analysis; visual evoked potential},
	correspondence_address = {J. Wu; School of Healthcare Management, Tsinghua University, Beijing, 100084, China; email: wujianxuanwu

@ARTICLE{Oikonomou2025Multiheadeegmodelcls,
	author = {Oikonomou, Vangelis P.},
	title = {MultiHeadEEGModelCLS: Contextual Alignment and Spatio-Temporal Attention Model for EEG-Based SSVEP Classification},
	year = {2025},
	journal = {Electronics (Switzerland)},
	volume = {14},
	number = {22},
	pages = {},
	doi = {10.3390/electronics14224394},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023103276&doi=10.3390%2Felectronics14224394&partnerID=40&md5=9ae426460d42173c82951dfd922c79f7},
	affiliations = {Centre for Research and Technology-Hellas, Thessaloniki, Macedonia, Greece},
	abstract = {Steady-State Visual Evoked Potentials (SSVEPs) offer a robust basis for brain–computer interface (BCI) systems due to their high signal-to-noise ratio, minimal user training requirements, and suitability for real-time decoding. In this work, we propose MultiHeadEEGModelCLS, a novel Transformer-based architecture that integrates context-aware representation learning into SSVEP decoding. The model employs a dual-stream spatio-temporal encoder to process both the input EEG trial and a contextual signal (e.g., template or reference trial), enhanced by a learnable classification ([CLS]) token. Through self-attention and cross-attention mechanisms, the model aligns trial-level representations with contextual cues. The architecture supports multi-task learning via signal reconstruction and context-informed classification heads. Evaluation on benchmark datasets (Speller and BETA) demonstrates state-of-the-art performance, particularly under limited data and short time window scenarios, achieving higher classification accuracy and information transfer rates (ITR) compared to existing deep learning methods such as the multi-branch CNN (ConvDNN). Our method achieved an ITR of 283 bits/min and 222 bits/min for the Speller and BETA datasets, and a ConvDNN of 238 bits/min and 181 bits/min. These results highlight the effectiveness of contextual modeling in enhancing the robustness and efficiency of SSVEP-based BCIs. © 2025 by the author.},
	author_keywords = {brain-computer interface (BCI); context-aware representation learning; EEG (electrencephalography); multi-task learning; SSVEP (steady-state visual evoked potentials); transformer-based architecture},
	keywords = {Benchmarking; Bioelectric potentials; Biomedical signal processing; Brain mapping; Computer architecture; Data transfer rates; Decoding; Deep learning; Electroencephalography; Electrophysiology; Interface states; Interfaces (computer); Learning systems; Multi-task learning; Signal reconstruction; Transfer learning; Brain-computer interface; Context-Aware; Context-aware representation learning; Electrencephalography; Information transfer rate; Multitask learning; Spatio-temporal; Steady-state visual evoked potential; Steady-state visual evoked potentials; Transformer-based architecture; Brain computer interface; Signal to noise ratio},
	correspondence_address = {V.P. Oikonomou; Information Technologies Institute, Centre for Research and Technology Hellas, CERTH-ITI, Thessaloniki, 6th km Charilaou-Thermi Road, 57001, Greece; email: viknmu

@ARTICLE{Fares2025Understanding,
	author = {Fares, Ahmed H.},
	title = {Understanding What the Brain Sees: Semantic Recognition from EEG Responses to Visual Stimuli Using Transformer},
	year = {2025},
	journal = {AI (Switzerland)},
	volume = {6},
	number = {11},
	pages = {},
	doi = {10.3390/ai6110288},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022821153&doi=10.3390%2Fai6110288&partnerID=40&md5=e95d428d1a02139a70d7198d9129056b},
	affiliations = {Faculty of Engineering, New Borg El Arab, Alexandria, Egypt; Faculty of Engineering at Shoubra, Cairo, Cairo, Egypt},
	abstract = {Understanding how the human brain processes and interprets multimedia content represents a frontier challenge in neuroscience and artificial intelligence. This study introduces a novel approach to decode semantic information from electroencephalogram (EEG) signals recorded during visual stimulus perception. We present DCT-ViT, a spatial–temporal transformer architecture that pioneers automated semantic recognition from brain activity patterns, advancing beyond conventional brain state classification to interpret higher level cognitive understanding. Our methodology addresses three fundamental innovations: First, we develop a topology-preserving 2D electrode mapping that, combined with temporal indexing, generates 3D spatial–temporal representations capturing both anatomical relationships and dynamic neural correlations. Second, we integrate discrete cosine transform (DCT) embeddings with standard patch and positional embeddings in the transformer architecture, enabling frequency-domain analysis that quantifies activation variability across spectral bands and enhances attention mechanisms. Third, we introduce the Semantics-EEG dataset comprising ten semantic categories extracted from visual stimuli, providing a benchmark for brain-perceived semantic recognition research. The proposed DCT-ViT model achieves 72.28% recognition accuracy on Semantics-EEG, substantially outperforming LSTM-based and attention-augmented recurrent baselines. Ablation studies demonstrate that DCT embeddings contribute meaningfully to model performance, validating their effectiveness in capturing frequency-specific neural signatures. Interpretability analyses reveal neurobiologically plausible attention patterns, with visual semantics activating occipital–parietal regions and abstract concepts engaging frontal–temporal networks, consistent with established cognitive neuroscience models. To address systematic misclassification between perceptually similar categories, we develop a hierarchical classification framework with boundary refinement mechanisms. This approach substantially reduces confusion between overlapping semantic categories, elevating overall accuracy to 76.15%. Robustness evaluations demonstrate superior noise resilience, effective cross-subject generalization, and few-shot transfer capabilities to novel categories. This work establishes the technical foundation for brain–computer interfaces capable of decoding semantic understanding, with implications for assistive technologies, cognitive assessment, and human–AI interaction. Both the Semantics-EEG dataset and DCT-ViT implementation are publicly released to facilitate reproducibility and advance research in neural semantic decoding. © 2025 by the author.},
	author_keywords = {attention mechanism; brain–computer interface; discrete cosine transform; EEG; embeddings; semantics recognition; spatial–temporal modeling; transformer; vision transformer},
	publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
	language = {English},
	abbrev_source_title = {AI.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Shirodkar2025Eventrelated,
	author = {Shirodkar, Vaishali R. and Edla, Damodar Reddy and Kumari, Annu and Chintala, Sridhar},
	title = {Event-related desynchronization detection and electroencephalography motor imagery classification using vision transformer},
	year = {2025},
	journal = {Intelligent Data Analysis},
	volume = {29},
	number = {6},
	pages = {1598 - 1614},
	doi = {10.1177/1088467X251324336},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021854852&doi=10.1177%2F1088467X251324336&partnerID=40&md5=15b605c3740e2f2d369d132e7872678c},
	affiliations = {National Institute of Technology Goa, Department of Computer Science and Engineering, Ponda, GA, India; Goa College of Engineering, Department of Information Technology, Ponda, GA, India; SR University, School of Computer Science and Artificial Intelligence, Warangal, TS, India},
	abstract = {The primary objective of this research is to enhance the classification accuracy of motor imagery (MI) electroencephalography signals to improve brain–computer interfaces (BCIs) for communication among individuals with mobility limitations. The main challenge is finding the correct frequency bands, efficient time-frequency representations, and accurately classifying these representations. Various classification models are available but have not achieved high accuracy, which is a key evaluation matrix. This research provides a method that first identifies the best frequency range utilizing several fast-converging optimization approaches. After filtering with the identified band, a continuous wavelet transform was used with complex Morlet to obtain temporal frequency representations, which are effective sources of feature representation for MI tasks. These scalograms were then classified using the advanced deep learning model vision transformer, which is well-known for its ability to extract and select features using the attention mechanism. The proposed technique achieved remarkable accuracy, attaining 97.33% on a widely recognized dataset and 89.89% on another dataset, outperforming comparable research. Integrating modern signal processing and a cutting-edge deep model enhances accuracy, allows for neuroprosthetic device control, and offers up new avenues for research in the BCI arena. © The Author(s) 2025},
	author_keywords = {complex Morlet transform; event-related desynchronization; Motor imagery; optimization; vision transformer},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Cutting; Deep learning; Electroencephalography; Electrophysiology; Image classification; Interfaces (computer); Neural prostheses; Wavelet transforms; Classification accuracy; Complex morlet transform; Efficient time; Event-related desynchronization; Morlet transform; Motor imagery; Motor imagery classification; Optimisations; Primary objective; Vision transformer; Optimization},
	correspondence_address = {V. Shirodkar; Department of Computer Science and Engineering, National Institute of Technology Goa, Cuncolim, Goa, India; email: vaishali

@ARTICLE{Zhou2025Joint,
	author = {Zhou, Wenhui and Qu, Nian and Yang, Chi and Li, Yunrui and Mo, Liangyan and Lin, Lili and Dai, Guojun},
	title = {Joint temporal-frequency-channel attention learning for EEG-based visual object classification},
	year = {2025},
	journal = {Journal of the Franklin Institute},
	volume = {362},
	number = {17},
	pages = {},
	doi = {10.1016/j.jfranklin.2025.108128},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019738734&doi=10.1016%2Fj.jfranklin.2025.108128&partnerID=40&md5=f0c8fb4476a81172eff72b1d27bbda06},
	affiliations = {Hangzhou Dianzi University, School of Computer Science and Technology, Hangzhou, Zhejiang, China; Zhejiang Gongshang University, School of Electronic and Information Engineering, Hangzhou, Zhejiang, China; Ningbo City College of Vocational Technology, School of Information and Intelligent Engineering, Ningbo, Zhejiang, China},
	abstract = {Electroencephalography (EEG) based visual object classification task refers to the recognition and classification of objects from EEG signals elicited by visual stimuli, which is an emerging attempt in recent years to use artificial intelligence to analyze and decode brain activities. However, most existing studies only focus on the temporal domain analysis of EEG signals while neglecting the distinct frequency information. Neuroscience research has shown that different visual perception processes may elicit enhanced or diminished activity within specific frequency ranges. Taking into account this fact, this paper proposes a joint temporal-frequency-channel attention learning framework to thoroughly explore the distinct features of EEG signals interwoven in these three domains. Specifically, it integrates Transformer modules and domain-attention mechanisms through dual-branch and cascaded structures to aggregate multi-domain features interwoven across the temporal, frequency, and channel domains of EEG signals. This design is a joint cross-domain attention learning paradigm for visual EEG classification and significantly improves the classification accuracy. Experimental results have demonstrated the superiority of our method with a state-of-the-art classification accuracy rate of up to 99.40%[jls-end-space/]. The source codes are available athttps://github.com/qn757275373/JTFCL-EEG. © 2025 The Franklin Institute. Published by Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies.},
	author_keywords = {EEG; Joint learning; Object classification; Temporal-frequency-channel fusion; Visual stimuli},
	keywords = {Artificial intelligence; Behavioral research; Biomedical signal processing; Brain; Classification (of information); Data mining; Electrophysiology; Frequency domain analysis; Neurology; Object recognition; Signal analysis; Text processing; Brain activity; Classification accuracy; Classification tasks; Frequency channels; Joint learning; Object classification; Temporal-frequency; Temporal-frequency-channel fusion; Visual object classification; Visual stimulus; Electroencephalography},
	correspondence_address = {L. Lin; School of Information and Electronic Engineering, Zhejiang Gongshang University, Hangzhou, China; email: lili_lin

@ARTICLE{Li2025Novel,
	author = {Li, Jingjing and Lee, Chinghung and Duan, Dingna and Zhou, Yanhong and Xie, Xueguang and Wan, Xianglong and Liu, Tian'ge and Li, Danyang and Yu, Hao and Wan Hasan, Wan Zuha and Song, Haiqing and Wen, Dong},
	title = {A novel AI-driven EEG images emotion recognition generalized classification model for cross-subject analysis},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {68},
	pages = {},
	doi = {10.1016/j.aei.2025.103744},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013319610&doi=10.1016%2Fj.aei.2025.103744&partnerID=40&md5=8f46e8fdbb292b9b585c57ecea9a4543},
	affiliations = {Yanshan University, School of Information Science and Engineering, Qinhuangdao, Hebei, China; Xi'an Jiaotong University, School of Public Policy and Administration, Xi'an, Shaanxi, China; University of Science and Technology Beijing, School of Intelligence Science and Technology, Beijing, China; University of Science and Technology Beijing, Beijing, China; Hebei Normal University of Science and Technology, School of Mathematics and Information Science and Technology, Hebei, China; University of Science and Technology Beijing, Sports Department, Beijing, China; Universiti Putra Malaysia, Department of Electrical and Electronic Engineering, Serdang, Selangor, Malaysia; Xuanwu Hospital, Capital Medical University, Department of Neurology, Beijing, China},
	abstract = {AI algorithms can effectively perform encoding and decoding analysis on Electroencephalography (EEG) signals, which are widely used in emotion recognition due to their ability to reflect brain activity characteristics. However, the significant inter-individual variability in EEG signals complicates cross-subject analysis, thereby limiting their generalizability. In this work, we propose a novel AI-driven EEG general classification model called the Siamese Cluster Transformer Model with Shared Architecture (SCTMSA). The SCTMSA model includes three primary components: (1) A Siamese subnet with a shared convolutional structure for extracting similarity features. (2) A transformer module with dual-head self-attention to capture global dependencies. (3) Based on traditional hard-assignment k-means clustering, we introduce a learnable soft-assignment mechanism and dynamic centroid updating strategy to achieve adaptive low-dimensional compression of the feature space. We validated the proposed model on three public dataset and one private datasets, with results demonstrating that the SCTMSA model effectively captures characteristic similarities. Experimental analysis and interpretability results show that Gamma and Beta frequency bands are most relevant to emotion recognition, highlighting their significant role in representing cross-subject emotional brain activity. This research offers a new perspective on reducing individual differences in EEG and advancing emotion classification tasks. © 2025 Elsevier Ltd},
	author_keywords = {Cross-subject; Electroencephalography (EEG); Shared architecture; Siamese cluster transformer model with shared architecture; Similarity features},
	keywords = {Architecture; Behavioral research; Biomedical signal processing; Brain; Classification (of information); Clustering algorithms; Electroencephalography; Emotion Recognition; Image classification; Memory architecture; Neurophysiology; Psychology computing; Architecture modeling; Brain activity; Classification models; Cross-subject; Emotion recognition; Shared architecture; Siamese cluster transformer model with shared architecture; Similarity feature; Transformer modeling; Electrophysiology},
	correspondence_address = {D. Wen; School of Intelligence Science and Technology, University of Science and Technology Beijing, Beijing, 100083, China; email: aiwd168

@ARTICLE{Wang2025Mvcnet,
	author = {Wang, Ziwei and Li, Siyang and Chen, Xiaoqing and Wu, Dongrui},
	title = {MVCNet: Multi-view contrastive network for motor imagery classification},
	year = {2025},
	journal = {Knowledge-Based Systems},
	volume = {328},
	pages = {},
	doi = {10.1016/j.knosys.2025.114205},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012826935&doi=10.1016%2Fj.knosys.2025.114205&partnerID=40&md5=88709718947dd65c8d0027bfcdf590f8},
	affiliations = {Huazhong University of Science and Technology, School of Artificial Intelligence and Automation, Wuhan, Hubei, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Zhongguancun Laboratory, Beijing, Beijing, China},
	abstract = {Electroencephalography (EEG)-based brain-computer interfaces (BCIs) enable neural interaction by decoding brain activity for external communication. Motor imagery (MI) decoding has received significant attention due to its intuitive mechanism. However, most existing models rely on single-stream architectures and overlook the multi-view nature of EEG signals, leading to limited performance and generalization. We propose a multi-view contrastive network (MVCNet), a dual-branch architecture that parallelly integrates CNN and Transformer blocks to capture both local spatial-temporal features and global temporal dependencies. To enhance the informativeness of training data, MVCNet incorporates a unified augmentation pipeline across time, frequency, and spatial domains. Two contrastive modules are further introduced: a cross-view contrastive module that enforces consistency of original and augmented views, and a cross-model contrastive module that aligns features extracted from both branches. Final representations are fused and jointly optimized by contrastive and classification losses. Experiments on five public MI datasets across three scenarios demonstrate that MVCNet consistently outperforms nine state-of-the-art MI decoding networks, highlighting its effectiveness and generalization ability. MVCNet provides a robust solution for MI decoding by integrating multi-view information and dual-branch modeling, contributing to the development of more reliable BCI systems. © 2025 Elsevier B.V.},
	author_keywords = {Brain-computer interface; Contrastive learning; Convolutional neural networks; Data augmentation; Motor imagery},
	keywords = {Biomedical signal processing; Brain; Classification (of information); Contrastive Learning; Convolutional neural networks; Electroencephalography; Electrophysiology; Image classification; Interfaces (computer); Network architecture; Neurons; Brain activity; Convolutional neural network; Data augmentation; External communications; Motor imagery; Motor imagery classification; Multi-views; Neural interactions; Performance; Stream architecture; Brain computer interface},
	correspondence_address = {D. Wu; Key Laboratory of the Ministry of Education for Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, Huazhong University of Science and Technology, 430074, China; email: drwu

@ARTICLE{Liang2025Mvcformer,
	author = {Liang, Yining and Meng, Ming and Gao, Yunyuan and Xi, Xugang},
	title = {MVC-former adaptation: A multi-view convolution transformer-based domain adaptation framework for cross-subject motor imagery classification},
	year = {2025},
	journal = {Neurocomputing},
	volume = {649},
	pages = {},
	doi = {10.1016/j.neucom.2025.130875},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009033393&doi=10.1016%2Fj.neucom.2025.130875&partnerID=40&md5=1099965d040210bc585c9518d97eced6},
	affiliations = {Hangzhou Dianzi University, School of Automation, Hangzhou, Zhejiang, China},
	abstract = {Owing to individual difference, it is challenging to decode the target subject's mental intentions by applying existing models in cross-subject Brain-Computer Interface tasks. The transfer learning methods have shown promising performance in this field, but they still suffer from poor presentations on temporal correlations characteristics of EEG signals. This paper proposes a multi-view convolution-transformer based domain adaptation framework for cross-subject motor imagery classification tasks. Firstly, to exploit the frequency diversity of EEG signals, we decomposed EEG signals into several overlapping frequency views and extracted frequency-related spatial and temporal features by parallel spatiotemporal convolution block. Subsequently, we use transformer blocks to extract long-range dependencies and narrow the marginal distribution between source and target domains. Eventually, a classifier and a domain discriminator were used for domain adaptation, and a mixed loss was employed to align conditional distributions. We conducted model validation on the BCI Competition IV 2a and 2b datasets and achieved average accuracies of 77.8 % and 80.1 %, respectively. The experimental results show that our proposed framework outperforms traditional deep adversarial domain adaptive methods. © 2025},
	author_keywords = {Brain-computer interface; Domain adaptation; EEG classification; Motor imagery; Multi-band; Transformer},
	keywords = {Biomedical signal processing; Classification (of information); Convolution; Distribution transformers; Electroencephalography; Image classification; Interfaces (computer); Learning systems; Adaptation framework; Domain adaptation; EEG classification; EEG signals; Individual Differences; Motor imagery; Motor imagery classification; Multi band; Multi-views; Transformer; Brain computer interface; ablation therapy; accuracy; adaptation; algorithm; Article; artifact; artificial neural network; classification; classification algorithm; classifier; computer simulation; continuous wavelet transform; convolutional neural network; cross subject motor imagery classification; data processing; deep learning; electroencephalogram; electroencephalography; entropy; feature extraction; female; functional magnetic resonance imaging; generative pretrained transformer; human; human experiment; imagery; information processing; learning algorithm; machine learning; male; multi view convolution transformer; nerve cell network; nuclear magnetic resonance imaging; signal processing; spatial analysis; spatiotemporal analysis; support vector machine; temporal attention; training; transfer of learning},
	correspondence_address = {M. Meng; School of Automation, Hangzhou Dianzi University, Hangzhou, China; email: mnming

@ARTICLE{Pfeffer2025Trends,
	author = {Pfeffer, Maximilian Achim and Wong, Johnny Kwok Wai and Ling, Sai Ho},
	title = {Trends and Limitations in Transformer-Based BCI Research},
	year = {2025},
	journal = {Applied Sciences (Switzerland)},
	volume = {15},
	number = {20},
	pages = {},
	doi = {10.3390/app152011150},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105020236496&doi=10.3390%2Fapp152011150&partnerID=40&md5=0492e3e15ea25f3aa3dd81f0d2c63fd7},
	affiliations = {University of Technology Sydney, Faculty of Engineering and Information Technology, Sydney, NSW, Australia; University of Technology Sydney, Architecture and Building, Sydney, NSW, Australia},
	abstract = {Transformer-based models have accelerated EEG motor imagery (MI) decoding by using self-attention to capture long-range temporal structures while complementing spatial inductive biases. This systematic survey of Scopus-indexed works from 2020 to 2025 indicates that reported advances are concentrated in offline, protocol-heterogeneous settings; inconsistent preprocessing, non-standard data splits, and sparse efficiency frequently reporting cloud claims of generalization and real-time suitability. Under session- and subject-aware evaluation on the BCIC IV 2a/2b dataset, typical performance clusters are in the high-80% range for binary MI and the mid-70% range for multi-class tasks with gains of roughly 5–10 percentage points achieved by strong hybrids (CNN/TCN–Transformer; hierarchical attention) rather than by extreme figures often driven by leakage-prone protocols. In parallel, transformer-driven denoising—particularly diffusion–transformer hybrids—yields strong signal-level metrics but remains weakly linked to task benefit; denoise → decode validation is rarely standardized despite being the most relevant proxy when artifact-free ground truth is unavailable. Three priorities emerge for translation: protocol discipline (fixed train/test partitions, transparent preprocessing, mandatory reporting of parameters, FLOPs, per-trial latency, and acquisition-to-feedback delay); task relevance (shared denoise → decode benchmarks for MI and related paradigms); and adaptivity at scale (self-supervised pretraining on heterogeneous EEG corpora and resource-aware co-optimization of preprocessing and hybrid transformer topologies). Evidence from subject-adjusting evolutionary pipelines that jointly tune preprocessing, attention depth, and CNN–Transformer fusion demonstrates reproducible inter-subject gains over established baselines under controlled protocols. Implementing these practices positions transformer-driven BCIs to move beyond inflated offline estimates toward reliable, real-time neurointerfaces with concrete clinical and assistive relevance. © 2025 by the authors.},
	author_keywords = {artificial intelligence; brain-computer interfaces; deep learning; diffusion; EEG; neural decoding; noise removal; self-attention; signal processing; transformers},
	keywords = {Biomedical signal processing; Decoding; Deep neural networks; Diffusion; Electric transformer testing; Electric transformers; Electroencephalography; Interfaces (computer); Learning systems; Signal denoising; De-Noise; Deep learning; Motor imagery; Neural decoding; Noises removal; Offline; Real- time; Self-attention; Signal-processing; Transformer; Brain computer interface},
	correspondence_address = {S.H. Ling; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, 2007, Australia; email: steve.ling

@ARTICLE{Pfeffer2025Evolving,
	author = {Pfeffer, Maximilian Achim and Nguyen, Anh Hoang Phuc and Kim, Kyunghun and Wong, Johnny Kwok Wai and Ling, Sai Ho},
	title = {Evolving optimized transformer-hybrid systems for robust BCI signal processing using genetic algorithms},
	year = {2025},
	journal = {Biomedical Signal Processing and Control},
	volume = {108},
	pages = {},
	doi = {10.1016/j.bspc.2025.107883},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004260461&doi=10.1016%2Fj.bspc.2025.107883&partnerID=40&md5=cc99736e47018816e45a68fda2063ba0},
	affiliations = {University of Technology Sydney, Faculty of Engineering and Information Technology, Sydney, NSW, Australia; The University of Texas at Austin, Department of Neuroscience, Austin, TX, United States; University of Technology Sydney, Architecture and Building, Sydney, NSW, Australia},
	abstract = {Integrating transformer-based architectures in brain-computer interface (BCI) systems has demonstrated significant potential in addressing challenges such as noisy electroencephalography (EEG) signals, inter-subject variability, and low signal-to-noise ratios. This study introduces a Genetic Algorithm (GA)-optimized framework to evolve high-performing transformer-hybrid architectures for EEG-based motor imagery (MI) classification. Leveraging a real-valued genome encoding scheme, the GA dynamically explored a diverse architectural search space comprising convolutional, transformer, and noise-infusion layers. Key findings demonstrate the efficacy of the proposed framework. The GA-derived architectures achieved a validation accuracy of 89.26%±6.1% on Dataset I, significantly surpassing traditional models such as EEGNet (70.00%) and t-CTrans (78.98%). On Dataset II, the GA-heuristic models achieved a validation accuracy of 84.52%±9.62 and a kappa score of 79.37%±12.82%, outperforming state-of-the-art models such as CTNet (82.52%±9.61%). Statistical analysis of the genetic algorithm revealed that genome complexity (genome length) significantly influenced model performance (F = 34.10, p < 0.00001), with larger genomes enabling richer feature extraction capabilities. Furthermore, the prevalence of transformer layers (n_trans) emerged as the most critical architectural component, significantly impacting not only validation accuracy (F = 12.10, p = 0.0019) but also kappa scores (F = 10.97, p = 0.0028). The proposed framework demonstrated strong generalization across datasets, maintaining well-separated clusters in t-SNE visualizations, particularly in test data, highlighting its adaptability to unseen conditions. By achieving state-of-the-art performance and validating key design parameters, this study sets a new benchmark for EEG-based BCI systems, showcasing the potential of GA-optimized transformer-hybrid architectures for more adaptive and generalizable solutions. © 2025 The Authors},
	author_keywords = {Brain-computer interface; Deep learning; Electroencephalography; Genetic algorithm; Motor imagery; Network optimization; Neural decoding; Signal processing; Transformer},
	keywords = {Deep learning; Electric transformer testing; Frequency modulation; Image analysis; Image coding; Image compression; Pulse modulation; Signal modulation; Hybrid architectures; Interface system; Low signal-to-noise ratio; Motor imagery; Motor imagery classification; Network optimization; Neural decoding; Signal-processing; Transformer; Network coding; Article; classifier; comparative study; controlled study; convolutional neural network; deep learning; electroencephalography; feature extraction; genetic algorithm; genome; human; imagery; prevalence; signal processing; transformer hybrid system; validation study},
	correspondence_address = {S.H. Ling; Faculty of Engineering and Information Technology, University of Technology Sydney, Broadway, Ultimo, 2007, NSW, CB11 81-113, Australia; email: steve.ling

@ARTICLE{Chen2025Novel,
	author = {Chen, Wenhui and Xu, Shunwu and Hu, Qingqing and Peng, Yiran and Zhang, Hong and Zhang, Jian and Chen, Zhaowen},
	title = {A Novel Deep Learning Model for Motor Imagery Classification in Brain–Computer Interfaces},
	year = {2025},
	journal = {Information (Switzerland)},
	volume = {16},
	number = {7},
	pages = {},
	doi = {10.3390/info16070582},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011642128&doi=10.3390%2Finfo16070582&partnerID=40&md5=eb542665b9669fab401135b6ed4328aa},
	affiliations = {Fujian Polytechnic Normal University, Key Laboratory of Nondestructive Testing, Fuqing, Fujian, China; Macau University of Science and Technology, Faculty of Humanities and Arts, Taipa, Macao; Macau University of Science and Technology, Faculty of Innovation Engineering, Taipa, Macao},
	abstract = {Recent advancements in decoding electroencephalogram (EEG) signals for motor imagery tasks have shown significant potential. However, the intricate time–frequency dynamics and inter-channel redundancy of EEG signals remain key challenges, often limiting the effectiveness of single-scale feature extraction methods. To address this issue, we propose the Dual-Branch Blocked-Integration Self-Attention Network (DB-BISAN), a novel deep learning framework for EEG motor imagery classification. The proposed method includes a Dual-Branch Feature Extraction Module designed to capture both temporal features and spatial patterns across different scales. Additionally, a novel Blocked-Integration Self-Attention Mechanism is employed to selectively highlight important features while minimizing the impact of redundant information. The experimental results show that DB-BISAN achieves state-of-the-art performance. Also, ablation studies confirm that the Dual-Branch Feature Extraction and Blocked-Integration Self-Attention Mechanism are critical to the model’s performance. Our approach offers an effective solution for motor imagery decoding, with significant potential for the development of efficient and accurate brain–computer interfaces. © 2025 by the authors.},
	author_keywords = {brain–computer interfaces; deep learning; EEG signals},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Electroencephalography; Extraction; Feature extraction; Image classification; Integration; Interfaces (computer); Learning systems; Attention mechanisms; Deep learning; Electroencephalogram signals; Feature extraction methods; Features extraction; Frequency dynamics; Learning models; Motor imagery classification; Motor imagery tasks; Time frequency},
	correspondence_address = {H. Zhang; Key Laboratory of Nondestructive Testing, Fujian Polytechnic Normal University, Fuqing, 350300, China; email: zhangh

@ARTICLE{Hu2025Dualbranch,
	author = {Hu, Hao and Zhou, Zhiyong and Zhang, Zihan and Yuan, Wenyu},
	title = {Dual-Branch Spatio-Temporal-Frequency Fusion Convolutional Network with Transformer for EEG-Based Motor Imagery Classification},
	year = {2025},
	journal = {Electronics (Switzerland)},
	volume = {14},
	number = {14},
	pages = {},
	doi = {10.3390/electronics14142853},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011611695&doi=10.3390%2Felectronics14142853&partnerID=40&md5=fee72716c7d5b5218fdb4b9713abb83d},
	affiliations = {Shanghai Dianji University, School of Mechanical Engineering, Shanghai, China; Shanghai Dianji University, School of Design and Art, Shanghai, China; Shanghai Dianji University, School of Electronic Engineering, Shanghai, China},
	abstract = {The decoding of motor imagery (MI) electroencephalogram (EEG) signals is crucial for motor control and rehabilitation. However, as feature extraction is the core component of the decoding process, traditional methods, often limited to single-feature domains or shallow time-frequency fusion, struggle to comprehensively capture the spatio-temporal-frequency characteristics of the signals, thereby limiting decoding accuracy. To address these limitations, this paper proposes a dual-branch neural network architecture with multi-domain feature fusion, the dual-branch spatio-temporal-frequency fusion convolutional network with Transformer (DB-STFFCNet). The DB-STFFCNet model consists of three modules: the spatiotemporal feature extraction module (STFE), the frequency feature extraction module (FFE), and the feature fusion and classification module. The STFE module employs a lightweight multi-dimensional attention network combined with a temporal Transformer encoder, capable of simultaneously modeling local fine-grained features and global spatiotemporal dependencies, effectively integrating spatiotemporal information and enhancing feature representation. The FFE module constructs a hierarchical feature refinement structure by leveraging the fast Fourier transform (FFT) and multi-scale frequency convolutions, while a frequency-domain Transformer encoder captures the global dependencies among frequency domain features, thus improving the model’s ability to represent key frequency information. Finally, the fusion module effectively consolidates the spatiotemporal and frequency features to achieve accurate classification. To evaluate the feasibility of the proposed method, experiments were conducted on the BCI Competition IV-2a and IV-2b public datasets, achieving accuracies of 83.13% and 89.54%, respectively, outperforming existing methods. This study provides a novel solution for joint time-frequency representation learning in EEG analysis. © 2025 by the authors.},
	author_keywords = {brain–computer interface (BCI); electroencephalogram (EEG); frequency features; motor imagery (MI); Transformer},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Convolution; Decoding; Extraction; Feature extraction; Frequency domain analysis; Image classification; Interfaces (computer); Network architecture; Neural networks; Signal encoding; Time domain analysis; Brain–computer interface; Convolutional networks; Electroencephalogram; Features extraction; Frequency features; Motor imagery; Spatiotemporal feature; Spatiotemporal frequency; Transformer; Classification (of information); Electroencephalography; Fast Fourier transforms},
	correspondence_address = {Z. Zhou; School of Art and Design, Shanghai DianJi University, Shanghai, 201306, China; email: zhouzhiyong

@ARTICLE{Mirzaei2025Eeg,
	author = {Mirzaei, Sayeh and Ghasemi, Parisa and Bakhtyari, Mohammadreza},
	title = {EEG Motor imagery classification based on a ConvLSTM Autoencoder framework augmented by attention BiLSTM},
	year = {2025},
	journal = {Multimedia Tools and Applications},
	volume = {84},
	number = {19},
	pages = {20545 - 20566},
	doi = {10.1007/s11042-024-19850-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199274992&doi=10.1007%2Fs11042-024-19850-0&partnerID=40&md5=189fb37fa48d038f0d125131863df844},
	affiliations = {University of Tehran, School of Science and Engineering, Tehran, Tehran, Iran},
	abstract = {Brain signals have recently gained popularity in brain-computer interface (BCI) systems, providing valuable insights into various brain functionalities. Analyzing brain activity signals can reveal motion imagination, and electroencephalography (EEG) signals are appropriate to recognize motor imagery (MI) tasks. However, MI-EEG classification remains challenging due to the limited spatial resolution of EEG. In this study, we present a pioneering architecture for MI-EEG classification, comprising a Convolutional Long Short-Term Memory Autoencoder (ConvLSTMAE) for efficient feature extraction and an Attention-augmented Bidirectional Long Short-Term Memory (AtBiLSTM) classifier. ConvLSTMAE captures spatiotemporal patterns in EEG signals, producing a compact latent representation. Subsequently, AtBiLSTM, incorporating an attention mechanism to enhance the model's focus on critical signal components, processes the learned representations, effectively capturing bidirectional temporal dependencies. Our method excels in motor imagery classification on the BCI competition IV 2a dataset, achieving an accuracy of 89.70% and a kappa value of 87.96%, outperforming existing methods. We systematically evaluate the impact of STFT, revealing a 10.91% accuracy improvement when transforming temporal signals to the frequency domain. Furthermore, replacing a Support Vector Machine with AtBiLSTM enhances accuracy by 17.74%, showcasing the effectiveness of our designed architecture. This research contributes to advancing EEG-based MI classification, offering promise for neuroscientific insights and the development of efficient brain-computer interfaces. The proposed AtBi-ConvLSTMAE framework not only addresses limitations in MI-EEG classification but also exhibits superior performance, lower standard deviations, and improved generalizability. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
	author_keywords = {Attention; Autoencoder; ConvLSTM; EEG; Motor imagery},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Electrophysiology; Frequency domain analysis; Image classification; Memory architecture; Neurophysiology; Support vector machines; Attention; Auto encoders; Brain activity; Brain signals; ConvLSTM; Interface system; Motor imagery; Motor imagery classification; Motor imagery tasks; Spatial resolution; Electroencephalography},
	correspondence_address = {S. Mirzaei; School of Engineering Science, College of Engineering, University of Tehran, Tehran, Iran; email: s.mirzaei

@ARTICLE{Ahmadi2025Universal,
	author = {Ahmadi, Hossein and Mesin, Luca},
	title = {Universal semantic feature extraction from EEG signals: a task-independent framework},
	year = {2025},
	journal = {Journal of Neural Engineering},
	volume = {22},
	number = {3},
	pages = {},
	doi = {10.1088/1741-2552/add08f},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004765263&doi=10.1088%2F1741-2552%2Fadd08f&partnerID=40&md5=8cfdb45737c56771fed0c4d93d6120b7},
	affiliations = {Politecnico di Torino, Department of Electronics and Telecommunications, Turin, TO, Italy},
	abstract = {Objective. Extracting universal, task-independent semantic features from electroencephalography (EEG) signals remains an open challenge. Traditional approaches are often task-specific, limiting their generalization across different EEG paradigms. This study aims to develop a robust, unsupervised framework for learning high-level, task-independent neural representations. Approach. We propose a novel framework integrating convolutional neural networks, AutoEncoders, and Transformers to extract both low-level spatiotemporal patterns and high-level semantic features from EEG signals. The model is trained in an unsupervised manner to ensure adaptability across diverse EEG paradigms, including motor imagery (MI), steady-state visually evoked potentials (SSVEPs), and event-related potentials (ERPs, specifically P300). Extensive analyses, including clustering, correlation, and ablation studies, are conducted to validate the quality and interpretability of the extracted features. Main results. Our method achieves state-of-the-art performance, with average classification accuracies of 83.50% and 84.84% on MI datasets (BCICIV_2a and BCICIV_2b), 98.41% and 99.66% on SSVEP datasets (Lee2019-SSVEP and Nakanishi2015), and an average AUC of 91.80% across eight ERP datasets. t-distributed stochastic neighbor embedding and clustering analyses reveal that the extracted features exhibit enhanced separability and structure compared to raw EEG data. Correlation studies confirm the framework’s ability to balance universal and subject-specific features, while ablation results highlight the near-optimality of the selected model configuration. Significance. This work establishes a universal framework for task-independent semantic feature extraction from EEG signals, bridging the gap between conventional feature engineering and modern deep learning methods. By providing robust, generalizable representations across diverse EEG paradigms, this approach lays the foundation for advanced brain-computer interface applications, cross-task EEG analysis, and future developments in semantic EEG processing. © 2025 The Author(s). Published by IOP Publishing Ltd.},
	author_keywords = {EEG decoding; neural representations; semantic feature extraction; task-independent features; Transformer},
	keywords = {Deep neural networks; Image analysis; Image coding; Image compression; Image enhancement; Unsupervised learning; Electroencephalography decoding; Event related potentials; Motor imagery; Neural representations; Semantic feature extractions; Semantic features; Steady-state visually evoked potential; Task-independent feature; Traditional approachs; Transformer; Convolutional neural networks; area under the curve; Article; artifact correction; autoencoder; classification; cluster analysis; convolutional neural network; correlational study; decoding; deep learning; electroencephalogram; electroencephalography; event related potential; evoked response; feature extraction; female; filter design; human; human experiment; imagery; intermethod comparison; male; mathematical analysis; model; neurophysiology; receiver operating characteristic; semantics; signal processing; steady state; visual evoked potential; adult; artificial neural network; brain computer interface; imagination; physiology; procedures; Adult; Brain-Computer Interfaces; Electroencephalography; Evoked Potentials, Visual; Female; Humans; Imagination; Male; Neural Networks, Computer; Semantics},
	correspondence_address = {H. Ahmadi; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, 10129, Italy; email: hossein.ahmadi

@ARTICLE{Dahiya2025Deep,
	author = {Dahiya, Ritu and G, Mamatha and Jawale, Shila and Das, Santanu and Choudhary, Sagar and Rathod, Vinod Motiram and Rajput, Bhawna Janghel},
	title = {Deep learning-based multi-brain capsule network for Next-Gen Clinical Emotion recognition using EEG signals},
	year = {2025},
	journal = {Neuroscience Informatics},
	volume = {5},
	number = {2},
	pages = {},
	doi = {10.1016/j.neuri.2025.100203},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004393110&doi=10.1016%2Fj.neuri.2025.100203&partnerID=40&md5=891b2fdbf20d2ade56171c34ab05f782},
	affiliations = {Chhotu Ram Arya College, Department of Chemistry, Sonipat, India; Sri Siddhartha Institute of Business Management, Department of Management Studies, Tumkur, India; Somaiya Vidyavihar University, Department of Computer Engineering, Mumbai, MH, India; Seshadripuram First Grade College, Department of Biotechnology, Bengaluru, India; Maulana Azad National Institute of Technology, Department of Mathematics, Bhopal, MP, India; Bharati Vidyapeeth (Deemed to be University), Department of Engineering and Technology, Pune, MH, India; Rungta College of Engineering & Technology, Bhilai, Department of Computer Science and Engineering, Bhilai, CG, India},
	abstract = {Deep learning techniques are crucial for next-generation clinical applications, particularly in Next-Gen Clinical Emotion recognition. To enhance classification accuracy, we propose an Attention mechanism based Capsule Network Model (At-CapNet) for Multi-Brain Region. EEG-tNIRS signals were collected using Next-Gen Clinical Emotion-inducing visual stimuli to construct the TYUT3.0 dataset, from which EEG and tNIRS features were extracted and mapped into matrices. A multi-brain region attention mechanism was applied to integrate EEG and tNIRS features, assigning different weights to features from distinct brain regions to obtain high-quality primary capsules. Additionally, a capsule network module was introduced to optimize the number of capsules entering the dynamic routing mechanism, improving computational efficiency. Experimental validation on the TYUT3.0 Next-Gen Clinical Emotion dataset demonstrates that integrating EEG and tNIRS improves recognition accuracy by 1.53% and 14.35% compared to single-modality signals. Moreover, the At-CapNet model achieves an average accuracy improvement of 4.98% over the original CapsNet model and outperforms existing CapsNet-based Next-Gen Clinical Emotion recognition models by 1% to 5%. This research contributes to the advancement of non-invasive neurotechnology for precise Next-Gen Clinical Emotion recognition, with potential implications for next-generation clinical diagnostics and interventions. © 2025 The Author(s)},
	author_keywords = {Capsule network; Clinical emotion recognition; Deep learning; EEG; Transactional NIRS},
	keywords = {adult; article; brain; brain region; deep learning; diagnosis; electroencephalogram; electroencephalography; emotion; female; human; male; microcapsule},
	correspondence_address = {B.J. Rajput; Department of Computer Science and Engineering, Rungta College of Engineering and Technology, Bhilai, Chhattisgarh, India; email: bhawnajanghel2849

@ARTICLE{Shi2025Deep,
	author = {Shi, Wuxiang and Li, Yurong and Zheng, Nan and Hong, Wenyao and Zhao, Zhenhua and Chen, Wensheng and Xue, Xiaojing and Chen, Ting},
	title = {A deep learning framework leveraging spatiotemporal feature fusion for electrophysiological source imaging},
	year = {2025},
	journal = {Computer Methods and Programs in Biomedicine},
	volume = {266},
	pages = {},
	doi = {10.1016/j.cmpb.2025.108767},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002495106&doi=10.1016%2Fj.cmpb.2025.108767&partnerID=40&md5=faa12c49244634d58516816e08603c75},
	affiliations = {Fuzhou University, College of Electrical Engineering and Automation, Fuzhou, Fujian, China; Fuzhou University, Fuzhou, Fujian, China; Fujian Provincial Hospital, Fuzhou, Fujian, China},
	abstract = {Background and Objectives: Electrophysiological source imaging (ESI) is a challenging technique for noninvasively measuring brain activity, which involves solving a highly ill-posed inverse problem. Traditional methods attempt to address this challenge by imposing various priors, but considering the complexity and dynamic nature of the brain activity, these priors may not accurately reflect the true attributes of brain sources. In this study, we propose a novel deep learning-based framework, spatiotemporal source imaging network (SSINet), designed to provide accurate spatiotemporal estimates of brain activity using electroencephalography (EEG). Methods: SSINet integrates a residual network (ResBlock) for spatial feature extraction and a bidirectional LSTM for capturing temporal dynamics, fused through a Transformer module to capture global dependencies. A channel attention mechanism is employed to prioritize active brain regions, improving both the accuracy of the model and its interpretability. Additionally, a weighted loss function is introduced to address the spatial sparsity of the brain activity. Results: We evaluated the performance of SSINet through numerical simulations and found that it outperformed several state-of-the-art ESI methods across various conditions, such as varying numbers of sources, source range, and signal-to-noise ratio levels. Furthermore, SSINet demonstrated robust performance even with electrode position offsets and changes in conductivity. We also validated the model on three real EEG datasets: visual, auditory, and somatosensory stimuli. The results show that the source activity reconstructed by SSINet aligns closely with the established physiological basis of brain function. Conclusions: SSINet provides accurate and stable source imaging results. © 2025 Elsevier B.V.},
	author_keywords = {Deep learning; Electroencephalography (EEG); Electrophysiological source imaging (ESI); Ill-posed inverse problem},
	keywords = {Brain; Brain mapping; Electrophysiology; Inverse problems; Physiological models; Brain activity; Deep learning; Dynamic nature; Electroencephalography; Electrophysiological source imaging; Features fusions; ILL-posed inverse problem; Learning frameworks; Source imaging; Spatiotemporal feature; Article; auditory evoked potential; autoencoder; brain electrophysiology; brain function; brain region; comparative study; controlled study; deep learning; electroencephalogram; electroencephalography; feature extraction; feed forward neural network; human; long short term memory network; low resolution brain electromagnetic tomography; mean absolute error; multilayer perceptron; occipital lobe; region growing (imaging); signal noise ratio; somatosensory evoked potential; spatiotemporal analysis; spatiotemporal source imaging network; striate cortex; visual evoked potential; visual information; algorithm; artificial neural network; brain; brain mapping; computer simulation; diagnostic imaging; image processing; physiology; procedures; signal processing; Algorithms; Brain Mapping; Computer Simulation; Deep Learning; Humans; Image Processing, Computer-Assisted; Neural Networks, Computer; Signal Processing, Computer-Assisted; Signal-To-Noise Ratio; Spatio-Temporal Analysis},
	correspondence_address = {Y. Li; College of Electrical Engineering and Automation, Fuzhou University, Fuzhou, China; email: liyurong

@ARTICLE{Acharya2025Eegconvnext,
	author = {Acharya, Madhav R. and Deo, Ravinesh Chand and Barua, Prabal Datta and Devi, Aruna and Tao, Xiaohui},
	title = {EEGConvNeXt: A novel convolutional neural network model for automated detection of Alzheimer's Disease and Frontotemporal Dementia using EEG signals},
	year = {2025},
	journal = {Computer Methods and Programs in Biomedicine},
	volume = {262},
	pages = {},
	doi = {10.1016/j.cmpb.2025.108652},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217228476&doi=10.1016%2Fj.cmpb.2025.108652&partnerID=40&md5=aa3ca61715c6acba8a8bb918b3d7d520},
	affiliations = {University of Southern Queensland, Physics and Computing, Toowoomba, QLD, Australia; Cogninet Australia, Sydney, NSW, Australia; University of Technology Sydney, Faculty of Engineering and Information Technology, Sydney, NSW, Australia; Anglia Ruskin University, School of Computing and Information Science, Cambridge, Cambridgeshire, United Kingdom; Australian Institute of Higher Education, Sydney, NSW, Australia; University of New England Australia, School of Science and Technology, Armidale, NSW, Australia; Taylor's University Malaysia, School of Biosciences, Subang Jaya, Selangor, Malaysia; SRM Institute of Science and Technology, School of Computing, Kattankulathur, TN, India; Kumamoto University, School of Science and Technology, Kumamoto, Kumamoto, Japan; The University of Sydney, Sydney, NSW, Australia; University of Southern Queensland, Physics and Computing, Toowoomba, QLD, Australia; University of the Sunshine Coast, School of Education and Tertiary Access, Sippy Downs, QLD, Australia; University of Southern Queensland, Physics and Computing, Toowoomba, QLD, Australia},
	abstract = {Background and objective: Deep learning models have gained widespread adoption in healthcare for accurate diagnosis through the analysis of brain signals. Neurodegenerative disorders like Alzheimer's Disease (AD) and Frontotemporal Dementia (FD) are increasingly prevalent due to age-related brain volume reduction. Despite advances, existing models often lack comprehensive multi-class classification capabilities and are computationally expensive. This study addresses these gaps by proposing EEGConvNeXt, a novel convolutional neural network (CNN) model for detecting AD and FD using electroencephalogram (EEG) signals with high accuracy. Materials and method: In this research, we employ an open-access EEG signal public dataset containing three distinct classes: AD, FD, and control subjects. We then constructed a newly proposed EEGConvNeXt model comprised of a 2-dimensional CNN algorithm that firstly converts the EEG signals into power spectrogram-based images. Secondly, these images were used as input for the proposed EEGConvNeXt model for automated classification of AD, FD, and a control outcome. The proposed EEGConvNeXt model is therefore a lightweight model that contributes to a new image classification CNN structure based on the transformer model with four primary stages: a stem, a main model, downsampling, and an output stem. Results: The EEGConvNeXt model achieved a classification accuracy of ∼95.70% for three-class detection (AD, FD, and control), validated using a hold-out strategy. Binary classification cases, such as AD versus FD and FD versus control, achieved accuracies exceeding 98%, demonstrating the model's robustness across scenarios. Conclusions: The proposed EEGConvNeXt model demonstrates high classification performance with a lightweight architecture suitable for deployment in resource-constrained settings. While the study establishes a novel framework for AD and FD detection, limitations include reliance on a relatively small dataset and the need for further validation on diverse populations. Future research should focus on expanding datasets, optimizing architecture, and exploring additional neurological disorders to enhance the model's utility in clinical applications. © 2025},
	author_keywords = {AD detection; EEG analysis; EEGConvNeXt; Signal processing; Transformer-like CNN},
	keywords = {Convolutional neural networks; Electrotherapeutics; Frequency division multiplexing; Image enhancement; Neurodegenerative diseases; Neurons; Radial basis function networks; Alzheimer disease detection; Alzheimers disease; Convolutional neural network; Disease detection; EEGConvNeXt; Electroencephalogram analysis; Electroencephalogram signals; Frontotemporal dementias; Signal-processing; Transformer-like convolutional neural network; Electroencephalography; adult; algorithm; Alzheimer disease; Article; artificial neural network; binary classification; brain size; continuous wavelet transform; controlled study; convolutional neural network; deep learning; diagnostic accuracy; diagnostic test accuracy study; discrete wavelet transform; electroencephalogram; electroencephalography; empirical mode decomposition; female; Fourier transform; frontotemporal dementia; Hilbert Huang transform; human; k nearest neighbor; major clinical study; male; multiclass classification; parallel factor analysis tensor decomposition; sensitivity and specificity; signal processing; support vector machine; diagnosis; diagnostic imaging; pathophysiology; procedures; Algorithms; Alzheimer Disease; Deep Learning; Frontotemporal Dementia; Humans; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	publisher = {Elsevier Ireland Ltd},
	issn = {01692607},
	coden = {CMPBE},
	pmid = {39938252},
	language = {English},
	abbrev_source_title = {Comput. Methods Programs Biomed.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{Zhao2025Deep,
	author = {Zhao, Jinke and Liu, Mingliang},
	title = {A deep temporal network for motor imagery classification based on multi-branch feature fusion and attention mechanism},
	year = {2025},
	journal = {Biomedical Signal Processing and Control},
	volume = {100},
	pages = {},
	doi = {10.1016/j.bspc.2024.107163},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208760909&doi=10.1016%2Fj.bspc.2024.107163&partnerID=40&md5=b67dae62feafc2dd8990b69fc8b97492},
	affiliations = {Heilongjiang University, Department of Automation, Harbin, Heilongjiang, China},
	abstract = {Objective: In recent years, the Brain-Computer Interface (BCI) technology has witnessed rapid advancements. Motor Imagery (MI), as one of the BCI paradigms, has found extensive applications in domains such as rehabilitation, entertainment, and neuroscience. How to conduct effective classification of it has emerged as one of the primary research issues. Electroencephalography (EEG) serves as an essential tool for studying the classification of MI. However, the existing models are incapable of fully extracting effective motion information from the interfered electroencephalogram data, leading to the final classification effect falling short of the expected goals. In response to this problem, we propose a deep temporal network based on multi-branch feature fusion and attention mechanism. This network incorporates a combination of multi-branch feature fusion, feature expansion, attention, and temporal decoding modules. Methods: First, primary features of EEG signals are extracted using a multi-branch convolutional neural network, followed by feature fusion. Subsequently, feature augmentation and attention mechanisms are employed to reduce noise interference while highlighting essential MI intentions. Finally, a temporal decoding module is utilized to deeply explore temporal information in MI data and perform classification. Results: The model performance was tested on the BCI_IV_2a, BCI_IV_2b, and OPenBMI datasets using both subject-specific and subject-independent experimental methods. The model achieved significant performance improvements on all three datasets, achieving accuracy of 81.21%, 93.12%, and 75.9%, respectively, better than other baseline models. Conclusion: Experimental results indicate that the proposed model leverages deep learning techniques for the classification of different types of MI, providing a reference framework for the development of more efficient MI-BCI systems. © 2024},
	author_keywords = {Electroencephalogram; Motor Imagery; Multi-head self- attention; Multibranch Feature Fusion; Temporal Networks},
	keywords = {Brain computer interface; Brain mapping; Convolutional neural networks; Deep neural networks; Electrotherapeutics; Image reconstruction; Attention mechanisms; Decoding module; Features fusions; Fusion mechanism; Motor imagery; Motor imagery classification; Multi-head self- attention; Multibranch; Multibranch feature fusion; Temporal networks; Electroencephalography; Article; classification algorithm; convolutional neural network; deep learning; electroencephalogram; electroencephalography; feature extraction; female; human; human experiment; imagery; male; motor imagery; multi-branch feature fusion; multi-head self-attention; normal human; signal processing},
	correspondence_address = {M. Liu; Department of Automation, Harbin, Heilongjiang University, 150000, China; email: 2002039

@ARTICLE{Guo2025Crosssession,
	author = {Guo, Shuai and Wang, Yi and Zhang, Xin and Tang, Baoping},
	title = {A cross-session non-stationary attention-based motor imagery classification method with critic-free domain adaptation},
	year = {2025},
	journal = {Biomedical Signal Processing and Control},
	volume = {100},
	pages = {},
	doi = {10.1016/j.bspc.2024.107122},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208669260&doi=10.1016%2Fj.bspc.2024.107122&partnerID=40&md5=f078bb1b354c88aaa4a79beb96390b08},
	affiliations = {Chongqing University, College of Mechanical and Vehicle Engineering, Chongqing, China; Chongqing University, Chongqing, China; Chongqing University, State Key Laboratory of Mechanical Transmission, Chongqing, China},
	abstract = {Recent studies increasingly employ deep learning to decode electroencephalogram (EEG) signals. While deep learning has improved the performance of motor imagery (MI) classification to some extent, challenges remain due to significant variances in EEG data across sessions and the limitations of convolutional neural networks (CNNs). EEG signals are inherently non-stationary, traditional multi-head attention typically uses normalization methods to reduce non-stationarity and improve performance. However, non-stationary factors are crucial inherent properties of EEG signals and provide valuable guidance for decoding temporal dependencies in EEG signals. In this paper, we introduce a novel CNN combined with the Non-stationary Attention (NSA) and Critic-free Domain Adaptation Network (NSDANet), tailored for decoding MI signals. This network starts with temporal–spatial convolution devised to extract spatial–temporal features from EEG signals. It then obtains multi-modal information from average and variance perspectives. We devise a new self-attention module, the Non-stationary Attention (NSA), to capture the non-stationary temporal dependencies of MI-EEG signals. Furthermore, to align feature distributions between the source and target domains, we propose a critic-free domain adaptation network that uses the Nuclear-norm Wasserstein discrepancy (NWD) to minimize the inter-domain differences. NWD complements the original classifier by acting as a critic without a gradient penalty. This integration leverages discriminative information for feature alignment, thus enhancing EEG decoding performance. We conducted extensive cross-session experiments on both BCIC IV 2a and BCIC IV 2b dataset. Results demonstrate that the proposed method outperforms some existing approaches. © 2024 Elsevier Ltd},
	author_keywords = {Attention mechanism; Brain-computer interface; Domain adaptation; Motor imagery},
	keywords = {Brain computer interface; Deep learning; Image classification; Image enhancement; Attention mechanisms; Classification methods; Convolutional neural network; Domain adaptation; Electroencephalogram signals; Motor imagery; Motor imagery classification; Nonstationary; Normalization methods; Performance; Convolutional neural networks; adversarial learning; Article; back propagation; classification algorithm; classifier; comparative study; convolutional neural network; critic-free domain adaptation network; data accuracy; data quality; deep learning; electroencephalography; feature extraction; human; imagery; machine learning; motor imagery; non-stationary attention; nuclear-norm wasserstein discrepancy; signal processing; visual feedback},
	correspondence_address = {Y. Wang; College of Mechanical and Vehicle Engineering, Chongqing University, Chongqing, 400044, China; email: wycqdx

@ARTICLE{Ma2025Fbatcnet,
	author = {Ma, Shuaishuai and Lv, Jidong and Li, Wenjie and Liu, Yan and Zou, Ling and Dai, Yakang},
	title = {FBATCNet: A Temporal Convolutional Network with Frequency Band Attention for Decoding Motor Imagery EEG},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {11265 - 11279},
	doi = {10.1109/ACCESS.2025.3525528},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215839088&doi=10.1109%2FACCESS.2025.3525528&partnerID=40&md5=f9d3245b20b888afea3fcc2956b38de8},
	affiliations = {Changzhou University, School of Microelectronics and Control Engineering, Changzhou, Jiangsu, China; Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Department of Medical Imaging, Suzhou, Jiangsu, China; Suzhou Guoke Medical Science & Technology Development Co. Ltd, Suzhou, Jiangsu, China},
	abstract = {Motor imagery-based brain-computer interfaces (MI-BCIs) hold significant promise for upper limb rehabilitation in stroke patients. However, traditional MI paradigm primarily involves various limbs and fails to effectively address unilateral upper limb rehabilitation needs. In addition, compared to decoding MI-EEG signals from different limbs, decoding MI-EEG signals from same limb faces more challenges. We introduced a novel tri-class fine motor imagery (FMI) paradigm and collected electroencephalogram (EEG) data from 20 healthy subjects for decoding research. Furthermore, we proposed a frequency band attention-based temporal convolutional network (FBATCNet) for MI-EEG decoding. First, an innovative use of the channel attention mechanism adaptively assigned weights to segmented EEG frequency bands, improving the frequency resolution of MI-EEG signals. Subsequently, convolutional block further integrated frequency-domain features and extracted spatial features. Finally, a temporal convolution block was utilized to capture advanced temporal features. The proposed model achieved accuracy of 84.73% on BCI Competition IV-2a (Dataset 1) and 66.06% on the private FMI dataset (Dataset 2). In the classification of subject dependent, the FBATCNet is better than the baseline methods mentioned in this paper. These results confirm that the FBATCNet is feasible and offer fresh insights for designing and applying FMI-BCI. © 2013 IEEE.},
	author_keywords = {Brain-computer interface; EEG; fine motor imagery; temporal convolutional network; upper limb rehabilitation},
	keywords = {Arthroplasty; Artificial limbs; Brain computer interface; Brain mapping; Convolutional neural networks; Electrotherapeutics; Frequency shift keying; Image reconstruction; Neuromuscular rehabilitation; Pulse amplitude modulation; Attention mechanisms; Convolutional networks; Electroencephalogram signals; Fine motor imagery; Fine motors; Healthy subjects; Motor imagery; Stroke patients; Temporal convolutional network; Upper-limb rehabilitation; Frequency domain analysis},
	correspondence_address = {L. Zou; Changzhou University, School of Microelectronics and Control Engineering, Changzhou, Jiangsu, 213164, China; email: zouling

@ARTICLE{Zhang2025Xcflstmsatnet,
	author = {Zhang, Tingting and Yan, Xu and Chen, Xin and Mao, Yi},
	title = {XCF-LSTMSATNet: A Classification Approach for EEG Signals Evoked by Dynamic Random Dot Stereograms},
	year = {2025},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {33},
	pages = {502 - 513},
	doi = {10.1109/TNSRE.2025.3529991},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215587514&doi=10.1109%2FTNSRE.2025.3529991&partnerID=40&md5=26e415154286efd8739231bdb171982c},
	affiliations = {Hohai University, Key Laboratory of Maritime Intelligent Network Information Technology, Nanjing, Jiangsu, China; Medical School of Nanjing University, Department of Cardiology, Nanjing, Jiangsu, China},
	abstract = {Stereovision is the visual perception of depth derived from the integration of two slightly different images from each eye, enabling understanding of the three-dimensional space. This capability is deeply intertwined with cognitive brain functions. To explore the impact of stereograms with varied motions on brain activities, we collected Electroencephalography (EEG) signals evoked by Dynamic Random Dot Stereograms (DRDS). To effectively classify the EEG signals induced by DRDS, we introduced a novel hybrid neural network model, XCF-LSTMSATNet, which integrates an XGBoost Channel Feature Optimization Module with the EEGNet and an LSTM Self-Attention Modules. Initially, in the channel selection phase, XGBoost is employed for preliminary classification and feature weight analysis, which can enhance our channel selection strategy. Following this, EEGNet employs deep convolutional layers to extract spatial features, while separable convolutions are subsequently used to derive high-dimensional spatial-temporal features. Meanwhile, the LSTMSAT Module, with its capability to learn long-term dependencies in time-series signals, is deployed to capture temporal continuity information. The incorporation of the self-attention mechanism further amplifies the model's ability to grasp long-distance dependencies and enables dynamic weight allocation to the extracted features. In the end, both temporal and spatial features are integrated into the classification module, enabling precise prediction across three categories of EEG signals. The proposed XCF-LSTMSATNet was extensively tested on both a custom dataset and the public datasets SRDA and SRDB. The results demonstrate that the model exhibits solid classification performance across all three datasets, effectively showcasing its robustness and generalization capabilities. © 2001-2011 IEEE.},
	author_keywords = {DRDS; EEG; EEGNet; LSTM; XGBoost},
	keywords = {Biomedical signal processing; Convolutional neural networks; Depth perception; Feature Selection; Image coding; Image enhancement; Image understanding; Photomapping; Stereo image processing; Stereo vision; Channel selection; Classification approach; Dynamic random dot stereogram; EEGNet; LSTM; Random dot stereograms; Spatial features; Three dimensional space; Visual perception; Xgboost; Electroencephalography; adult; article; artificial neural network; brain function; classification; cognition; controlled study; diagnosis; electroencephalogram; electroencephalography; hand strength; human; male; motion; prediction; time series analysis},
	correspondence_address = {Y. Mao; Hohai University, Key Laboratory of Maritime Intelligent Cyberspace Technology, Ministry of Education, Nanjing, 210000, China; email: maoyi

@ARTICLE{Li2025Temporalspectral,
	author = {Li, Xujin and Wei, Wei and Qiu, Shuang and He, Huiguang},
	title = {A temporal–spectral fusion transformer with subject-specific adapter for enhancing RSVP-BCI decoding},
	year = {2025},
	journal = {Neural Networks},
	volume = {181},
	pages = {},
	doi = {10.1016/j.neunet.2024.106844},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208199774&doi=10.1016%2Fj.neunet.2024.106844&partnerID=40&md5=0bc84a9b8cbc1df4927457863dc229dd},
	affiliations = {Institute of Automation Chinese Academy of Sciences, Brain-inspired Cognitive Intelligence Lab, Beijing, Beijing, China; University of Chinese Academy of Sciences, School of Future Technology, Beijing, China; University of Chinese Academy of Sciences, School of Artificial Intelligence, Beijing, China},
	abstract = {The Rapid Serial Visual Presentation (RSVP)-based Brain–Computer Interface (BCI) is an efficient technology for target retrieval using electroencephalography (EEG) signals. The performance improvement of traditional decoding methods relies on a substantial amount of training data from new test subjects, which increases preparation time for BCI systems. Several studies introduce data from existing subjects to reduce the dependence of performance improvement on data from new subjects, but their optimization strategy based on adversarial learning with extensive data increases training time during the preparation procedure. Moreover, most previous methods only focus on the single-view information of EEG signals, but ignore the information from other views which may further improve performance. To enhance decoding performance while reducing preparation time, we propose a Temporal-Spectral fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a cross-view interaction module is proposed to facilitate information transfer and extract common representations across two-view features extracted from EEG temporal signals and spectrogram images. Then, an attention-based fusion module fuses the features of two views to obtain comprehensive discriminative features for classification. Furthermore, a multi-view consistency loss is proposed to maximize the feature similarity between two views of the same EEG signal. Finally, we propose a subject-specific adapter to rapidly transfer the knowledge of the model trained on data from existing subjects to decode data from new subjects. Experimental results show that TSformer-SA significantly outperforms comparison methods and achieves outstanding performance with limited training data from new subjects. This facilitates efficient decoding and rapid deployment of BCI systems in practical use. © 2024},
	author_keywords = {Adapter-based fine-tuning; Brain–Computer Interface (BCI); Multi-view learning; Rapid Serial Visual Presentation (RSVP); Transformer},
	keywords = {Brain computer interface; Interfaces (computer); Adapter-based fine-tuning; Brain–computer interface; Fine tuning; Multi-view learning; Performance; Rapid serial visual presentation; Rapid serial visual presentations; Subject-specific; Transformer; Two views; Decoding; adult; adversarial learning; Article; artificial neural network; convolutional neural network; deep learning; electroencephalography; event related potential; female; geometry; human; image retrieval; learning algorithm; male; normal human; phase preservation neural network; Rapid Serial Visual Presentation; remote sensing; spectroscopy; vision; algorithm; brain; brain computer interface; photostimulation; physiology; procedures; signal processing; Adult; Algorithms; Brain; Brain-Computer Interfaces; Electroencephalography; Humans; Male; Photic Stimulation; Signal Processing, Computer-Assisted; Visual Perception},
	correspondence_address = {S. Qiu; Key Laboratory of Brain Cognition and Brain-inspired Intelligence Technology, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; email: shuang.qiu

@ARTICLE{Sireesha2025Eegbcibased,
	author = {Sireesha, V. and Tallapragada, V. V.Satyanarayana and Naresh, M. Venkata and Kumar, G. V.Pradeep},
	title = {EEG-BCI-based motor imagery classification using double attention convolutional network},
	year = {2025},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering},
	volume = {28},
	number = {5},
	pages = {581 - 600},
	doi = {10.1080/10255842.2023.2298369},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181033061&doi=10.1080%2F10255842.2023.2298369&partnerID=40&md5=7c0a039759904c27ea7b8b1bfc2db19c},
	affiliations = {GITAM University, Department of Computer Science and Engineering, Visakhapatnam, AP, India; Mohan Babu University, Department of Electrical & Computer Engineering, Tirupati, AP, India; Matrusri Engineering College, Department of Electrical & Computer Engineering, Hyderabad, TS, India; Chaitanya Bharathi Institute of Technology, Department of Electrical & Computer Engineering, Hyderabad, TS, India},
	abstract = {This article aims to improve and diversify signal processing techniques to execute a brain-computer interface (BCI) based on neurological phenomena observed when performing motor tasks using motor imagery (MI). The noise present in the original data, such as intermodulation noise, crosstalk, and other unwanted noise, is removed by Modify Least Mean Square (M-LMS) in the pre-processing stage. Traditional LMSs were unable to extract all the noise from the images. After pre-processing, the required features, such as statistical features, entropy features, etc., were extracted using Common Spatial Pattern (CSP) and Pearson’s Correlation Coefficient (PCC) instead of the traditional single feature extraction model. The arithmetic optimization algorithm cannot select the features accurately and fails to reduce the feature dimensionality of the data. Thus, an Extended Arithmetic operation optimization (ExAo) algorithm is used to select the most significant attributes from the extracted features. The proposed model uses Double Attention Convolutional Neural Networks (DAttnConvNet) to classify the types of EEG signals based on optimal feature selection. Here, the attention mechanism is used to select and optimize the features to improve the classification accuracy and efficiency of the model. In EEG motor imagery datasets, the proposed model has been analyzed under class, which obtained an accuracy of 99.98% in class Baseline (B), 99.82% in class Imagined movement of a right fist (R) and 99.61% in class Imagined movement of both fists (RL). In the EEG dataset, the proposed model can obtain a high accuracy of 97.94% compared to EEG datasets of other models. © 2024 Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {arithmetic operation optimization; common spatial pattern; convolutional net; double attention; EEG signal; modified least mean square; Motor imagery; pearson correlation coefficient},
	keywords = {Biomedical signal processing; Classification (of information); Convolution; Correlation methods; Electroencephalography; Feature extraction; Image classification; Image enhancement; Arithmetic operation optimization; Arithmetic operations; Common spatial patterns; Convolutional net; Double attention; EEG signals; Least mean squares; Modified least mean square; Motor imagery; Operations optimization; Pearson correlation coefficients; Brain computer interface; accuracy; algorithm; arithmetic; Article; attention; channel wise attention; Common Spatial Pattern; convolutional neural network; correlation coefficient; double attention convolutional network; electroencephalography; entropy; Extended Arithmetic operation optimization; feature extraction; feature selection; forced switching mechanism; imagery; Modify Least Mean Square; motor imagery classification; noise; Random math optimizer probability; signal processing; simulation; time wise attention; artificial neural network; brain computer interface; human; imagination; physiology; procedures; Algorithms; Brain-Computer Interfaces; Humans; Imagination; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	correspondence_address = {V.V.S. Tallapragada; Department of ECE, Mohan Babu University, Tirupati, Andhra Pradesh, India; email: satya.tvv

@ARTICLE{Meng2025Asastgcn,
	author = {Meng, Ming and Yu, Peiqi and She, Qingshan and Xi, Xugang and Kong, Wanzeng},
	title = {ASA-STGCN: Adaptive Sparse Awareness-Spatiotemporal Graph Convolutional Network for Multi-Class Motor Imagery EEG Classification},
	year = {2025},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	pages = {},
	doi = {10.1109/JBHI.2025.3643173},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105024807435&doi=10.1109%2FJBHI.2025.3643173&partnerID=40&md5=172e056b88ef15c11bc27b1ee75908fc},
	affiliations = {Hangzhou Dianzi University, School of Automation, Hangzhou, Zhejiang, China; Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou, Zhejiang, China},
	abstract = {Graph Convolutional Networks (GCNs) have shown promise in motor imagery electroencephalogram (EEG) signals classification by modeling spatial dynamics and brain connectivity. However, over-smoothing remains a challenge, leading to homogenized node features and reduced discrimination. To address this, we propose an Adaptive Sparse Awareness-Spatiotemporal Graph Convolutional Network (ASA-STGCN) that combines adaptive sparse graph convolution with attention mechanisms. Notably, a Graph Sparse Convolutional Network (GSCN) in the Adaptive Sparse Awareness Spatial Module (ASAM) enhances brain region feature selection, while the Graph Node Neighborhood Awareness Layer (GNNAL) applies self-attention to reinforce critical topological relationships. The Multi-scale Temporal Convolution Module (MTCM) captures both transient and sustained temporal dependencies. Experimental results achieve accuracies of 97.2%±3.4% (binary) and 83.6%±4.9% (four-class) on BCIC-IV-2a, 96.6%±3.1% (binary) on BCIC-III IVa, and 83.41%±4.3 (binary) on OpenBMI. Discussion confirms the model's effectiveness and its potential to support EEG-based neurorehabilitation and clinical brain computer interface applications. © 2013 IEEE.},
	author_keywords = {Adaptive Sparse Awareness; Electroencephalogram (EEG); Graph Convolutional Network (GCN); Motor Imagery (MI)},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Convolution; Convolutional neural networks; Feature Selection; Image classification; Interfaces (computer); Medical computing; Undirected graphs; Adaptive sparse awareness; Convolutional networks; Electroencephalogram; Electroencephalogram signals; Graph convolutional network; Motor imagery; Network adaptive; Signal classification; Spatio-temporal graphs; Electroencephalography},
	correspondence_address = {W. Kong; Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, Hangzhou, 310018, China; email: kongwanzeng

@ARTICLE{Wang2025Dbconformer,
	author = {Wang, Ziwei and Wang, Hongbin and Jia, Tianwang and He, Xingyi and Li, Siyang and Wu, Dongrui},
	title = {DBConformer: Dual-Branch Convolutional Transformer for EEG Decoding},
	year = {2025},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	pages = {},
	doi = {10.1109/JBHI.2025.3622725},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019981029&doi=10.1109%2FJBHI.2025.3622725&partnerID=40&md5=3eea6222b7520cffffd5550a1c370905},
	affiliations = {Huazhong University of Science and Technology, School of Artificial Intelligence and Automation, Wuhan, Hubei, China},
	abstract = {Electroencephalography (EEG)-based brain computer interfaces (BCIs) transform spontaneous/evoked neural activity into control commands for external communication. While convolutional neural networks (CNNs) remain the mainstream backbone for EEG decoding, their inherently short receptive field makes it difficult to capture long-range temporal dependencies and global inter-channel relationships. Recent CNN-Transformer (Conformer) hybrids partially address this issue, but most adopt a serial design, resulting in suboptimal integration of local and global features, and often overlook explicit channel-wise modeling. To address these limitations, we propose DBConformer, a dual-branch convolutionalTrans former network tailored for EEG decoding. It integrates a temporal Conformer to model long-range temporal dependencies and a spatial Conformer to extract inter-channel interactions, capturing both temporal dynamics and spatial patterns in EEG signals. A lightweight channel attention module further refines spatial representations by assigning data-driven importance to EEG channels. Extensive experiments under four evaluation settings on three paradigms, including motor imagery, seizure detection, and steady state visual evoked potential, demonstrated that DBCon former consistently outperformed 13 competitive baseline models, with over an eight-fold reduction in parameters than current high-capacity EEG Conformer architecture. Furthermore, the visualization results confirmed that the features extracted by DBConformer are physiologically in terpretable and aligned with prior knowledge. The superior performance and interpretability of DBConformer make it reliable for accurate, robust, and explainable EEG decoding. © 2013 IEEE.},
	author_keywords = {Brain-computer interface; convolutional neural networks; electroen cephalography; motor imagery; seizure detection; Transformer},
	keywords = {Bioelectric potentials; Biomedical signal processing; Brain; Brain mapping; Communication channels (information theory); Convolution; Convolutional neural networks; Decoding; Electroencephalography; Electrophysiology; Interfaces (computer); Network architecture; Neurons; Channel relationships; Control command; Convolutional neural network; Electroen cephalography; External communications; Motor imagery; Neural activity; Receptive fields; Seizure-detection; Transformer; Brain computer interface},
	correspondence_address = {D. Wu; Huazhong University of Science and Technology, Ministry of Education Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Wuhan, 430074, China; email: drwu

@ARTICLE{Zhu2025Idprotoformer,
	author = {Zhu, Jiabin and Jin, Xuanyu and Kong, Wanzeng},
	title = {ID-ProtoFormer: A Dynamic Identity Prototype-Infused Transformer for SSVEP-Based Biometric Recognition},
	year = {2025},
	journal = {IEEE Signal Processing Letters},
	volume = {32},
	pages = {4104 - 4108},
	doi = {10.1109/LSP.2025.3622525},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105019652510&doi=10.1109%2FLSP.2025.3622525&partnerID=40&md5=d66e8355944c759a23d6456212a14c33},
	affiliations = {Hangzhou Dianzi University, College of Computer Science, Hangzhou, Zhejiang, China; State Key Laboratory of Brain-Machine Intelligence, Hangzhou, China},
	abstract = {Steady-State Visual Evoked Potentials (SSVEP) provide high signal-to-noise ratio and reliable responses, enabling robust identity recognition. However, the non-stationary nature of EEG signals introduces substantial cross-session variability. Most existing methods rely on sample-specific representations, which are sensitive to non-identity-related factors and fail to capture stable identity features. In this letter, we propose ID-Protoformer, a Transformer-based model infused with dynamic identity prototypes to enhance SSVEP-based biometric recognition. Specifically, the model first encodes raw EEG signals into informative spatio-temporal embeddings to produce fine-grained token representations. It then emphasizes identity-discriminative features through adaptive token reweighting and stabilizes representations by integrating subject-specific prototypes. By infusing subject-aware prototypes into the token representation space, ID-Protoformer effectively enhances identity-discriminative features and improves robustness across sessions. Extensive experiments demonstrate that ID-Protoformer consistently outperforms existing methods, indicating the benefit of subject-aware modeling for robust and generalizable EEG biometrics across sessions. © 1994-2012 IEEE.},
	author_keywords = {biometrics; deep learning; Electroencephalography; steady-state visual evoked potential; transformer},
	keywords = {Bioelectric potentials; Biomedical signal processing; Deep learning; Electrophysiology; Signal to noise ratio; Biometric recognition; Discriminative features; Dynamic identities; EEG signals; High signal-to-noise ratio; Identity recognition; Nonstationary; Steady-state visual evoked potentials; Transformer; Biometrics; Electroencephalography},
	correspondence_address = {W. Kong; Hangzhou Dianzi University, College of Computer Science, Hangzhou, 310018, China; email: kongwanzeng

@ARTICLE{Balendra2025Transformed,
	author = {Balendra, null and Sharma, Neeraj K. and Sharma, Shiru},
	title = {Transformed wavelets for motor imagery EEG classification using hybrid CNN-modified vision transformer: an exploratory study of MI EEG},
	year = {2025},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering},
	pages = {},
	doi = {10.1080/10255842.2025.2563351},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017513718&doi=10.1080%2F10255842.2025.2563351&partnerID=40&md5=4b30fea00b8df23e18a27731ca06bb6f},
	affiliations = {Indian Institute of Technology (BHU) Varanasi, School of Biomedical Engineering, Varanasi, UP, India},
	abstract = {Wavelets capture signal characteristics across time and frequency, but traditional wavelets suffer from high time–bandwidth products (TBP), limiting feature discrimination in EEG classification. We propose transformed wavelets with improved TBP and frequency bandwidth, outperforming Morlet by 0.04 and 0.20, respectively. Using datasets BCI Competition IV 2a, 2b, and CLA, we evaluated both fundamental and transformed wavelets with a modified vision transformer (MViT). Enhanced scalograms generated through local mean and principal component analysis (PCA) consistently outperformed raw scalograms. A hybrid convolutional neural network (CNN)–MViT achieved 82.35% inter-subject and 89.02% intra-subject accuracy, with 3–4% average gains in motor imagery EEG decoding. © 2025 Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {continuous wavelet transform; electroencephalogram (EEG); motor imagery; vision transformer; Wavelets},
	keywords = {Biomedical signal processing; Convolutional neural networks; Electroencephalography; Image classification; Wavelet transforms; Across time; Continuous Wavelet Transform; Convolutional neural network; Electroencephalogram; Exploratory studies; Motor imagery; Signal characteristic; Time-bandwidth products; Vision transformer; Wavelet; Bandwidth; Principal component analysis; article; bandwidth; classification; clinical article; continuous wavelet transform; convolutional neural network; diagnosis; electroencephalogram; electroencephalography; female; human; hybrid; imagery; principal component analysis; vision},
	publisher = {Taylor and Francis Ltd.},
	issn = {10255842},
	pmid = {40999875},
	language = {English},
	abbrev_source_title = {Comput. Methods Biomech. Biomed. Eng.},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Huang2025Promptguided,
	author = {Huang, Wei and Li, Hengjiang and Qin, Fan and Wu, Diwei and Cheng, Kaiwen and Chen, Huafu},
	title = {A Prompt-Guided Generative Language Model for Unifying Visual Neural Decoding Across Multiple Subjects and Tasks},
	year = {2025},
	journal = {International Journal of Neural Systems},
	pages = {},
	doi = {10.1142/S0129065725500686},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017304894&doi=10.1142%2FS0129065725500686&partnerID=40&md5=d4de1de8b0d508bd553ab5dceaaa2ae6},
	affiliations = {University of Electronic Science and Technology of China, Brain-Computer Interface & Brain-Inspired Intelligence, Chengdu, Sichuan, China; Sichuan International Studies University, College of Language Intelligence, Chongqing, Chongqing, China; University of Electronic Science and Technology of China, Brain-Computer Interface & Brain-Inspired Intelligence, Chengdu, Sichuan, China},
	abstract = {Visual neural decoding not only aids in elucidating the neural mechanisms underlying the processing of visual information but also facilitates the advancement of brain-computer interface technologies. However, most current decoding studies focus on developing separate decoding models for individual subjects and specific tasks, an approach that escalates training costs and consumes a substantial amount of computational resources. This paper introduces a Prompt-Guided Generative Visual Language Decoding Model (PG-GVLDM), which uses prompt text that includes information about subjects and tasks to decode both primary categories and detailed textual descriptions from the visual response activities of multiple individuals. In addition to visual response activities, this study also incorporates a multi-head cross-attention module and feeds the model with whole-brain response activities to capture global semantic information in the brain. Experiments on the Natural Scenes Dataset (NSD) demonstrate that PG-GVLDM attains an average category decoding accuracy of 66.6% across four subjects, reflecting strong cross-subject generalization, and achieves text decoding scores of 0.342 (METEOR), 0.450 (Sentence-Transformer), 0.283 (ROUGE-1), and 0.262 (ROUGE-L), establishing state-of-the-art performance in text decoding. Furthermore, incorporating whole-brain response activities significantly enhances decoding performance by enabling the integration of distributed neural signals into coherent global semantic representations, underscoring its methodological importance for unified neural decoding. This research not only represents a breakthrough in visual neural decoding methodologies but also provides theoretical and technical support for the development of generalized brain-computer interfaces. © 2025 The Author(s).},
	author_keywords = {functional magnetic resonance imaging; generative language model; Visual decoding},
	keywords = {Brain; Brain computer interface; Brain models; Computational linguistics; Decoding; Interfaces (computer); Learning systems; Natural language processing systems; Neural networks; Semantics; Visual languages; Brain response; Functional magnetic resonance imaging; Generative language model; Language model; Neural decoding; Neural mechanisms; Visual decoding; Visual information; Visual response; Whole brains; Magnetic resonance imaging},
	correspondence_address = {K. Cheng; College of Language Intelligence, Sichuan International Studies University, Chongqing, 400031, China; email: kevincheng

@ARTICLE{Mariamuna2025Sstaf,
	author = {Maria Muna, Ummay and Hasan Shawon, Md Mehedi and Jobayer, Md D. and Akter, Sumaiya and Rahman Sabuj, Saifur},
	title = {SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {158855 - 158869},
	doi = {10.1109/ACCESS.2025.3605265},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105015186925&doi=10.1109%2FACCESS.2025.3605265&partnerID=40&md5=79440f22901ab6327810af13ec146380},
	affiliations = {BRAC University, Department of Electrical and Electronic Engineering, Dhaka, Bangladesh; A. James Clark School of Engineering, College Park, MD, United States},
	abstract = {Brain–computer interfaces (BCI) in electroencephalography (EEG)-based motor imagery classification offer promising solutions in neurorehabilitation and assistive technologies by enabling communication between the brain and external devices. However, the non-stationary nature of EEG signals and significant inter-subject variability cause substantial challenges for developing robust cross-subject classification models. This paper introduces a novel Spatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically designed for upper-limb motor imagery classification. Our architecture consists of a spectral transformer and a spatial transformer, followed by a transformer block and a classifier network. Each module is integrated with attention mechanisms that dynamically attend to the most discriminative patterns across multiple domains, such as spectral frequencies, spatial electrode locations, and temporal dynamics. The short-time Fourier transform is incorporated to extract features in the time-frequency domain to make it easier for the model to obtain a better feature distinction. We evaluated our SSTAF Transformer model on two publicly available datasets, the EEGMMIDB dataset and BCI Competition IV-2a. SSTAF Transformer achieves an accuracy of 76. 83% and 68. 30% in the data sets, respectively, outperforms traditional CNN-based architectures and a few existing transformer-based approaches. © 2013 IEEE.},
	author_keywords = {Attention mechanisms; brain–computer interfaces; cross-subject; electroencephalography; motor imagery; neurorehabilitation; spectral-spatial-temporal features; transformer networks},
	keywords = {Assistive technology; Biomedical signal processing; Brain; Brain mapping; Classification (of information); Electrophysiology; Frequency domain analysis; Image classification; Interfaces (computer); Neuromuscular rehabilitation; Attention mechanisms; Cross-subject; Motor imagery; Motor imagery classification; Neurorehabilitation []; Nonstationary; Spatial-temporal features; Spectral-spatial-temporal feature; Transformer network; Brain computer interface; Electroencephalography},
	correspondence_address = {Md.M. Hasan Shawon; BRAC University, Department of Electrical and Electronic Engineering, Dhaka, 1212, Bangladesh; email: mehedi.shawon

@ARTICLE{Abbasi2025Trinet,
	author = {Abbasi, Hafza Faiza and Ahmed Abbasi, Muhammad and Jianbo, Shen and Liping, Xiang and Yu, Xiaojun},
	title = {TriNet: A Hybrid Feature Integration Approach for Motor Imagery Classification in Brain-Computer Interface},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {115406 - 115418},
	doi = {10.1109/ACCESS.2025.3585180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010140176&doi=10.1109%2FACCESS.2025.3585180&partnerID=40&md5=cd3af03d76c4a3120560ae7d4ecb3ac7},
	affiliations = {Northwestern Polytechnical University, Xi'an, Shaanxi, China; Jincheng People's Hospital, Department of Neurosurgery, Jincheng, Shanxi, China; Jincheng Vocational Technical College, Jincheng, China},
	abstract = {Brain-computer interface (BCI) is inevitably a promising technology holding the potential to revolutionize the world with its wide range of applications. From healthcare to innovative computer gaming, integrating BCI for intelligent control has become an emergent scope. However, optimizing motor imagery (MI) classification in non-invasive BCI remains a significant challenge due to the poor quality of the acquired signal. In this paper, we propose a unified approach for MI classification by combining features from three diverse domains. Initially, the EEG data is preprocessed using bandpass filtering to extract the relevant EEG signals. Next, the preprocessed signal is fed simultaneously to three branches to extract three distinct categories of features from the signal. Specifically, spectral features are extracted using the fast-Fourier transform (FFT) and a spatial transformer is utilized to extract spatial features from the EEG data. Moreover, the third branch extracts temporal features using an encoder-decoder architecture. The features obtained using the three branches are concatenated together to obtain a comprehensive features set which is finally classified using extreme learning machine (ELM). Our proposed approach which uses a novel combination of features from three distinct domains is hereby named TriNet and is validated using two benchmark datasets BCI Competition IV-2a and BCI Competition IV-2b. The experimental results show an accuracy of 87.30% and 92.64% respectively on BCI IV-2a and BCI IV-2b datasets in subject-specific classification. Moreover, TriNet is also tested in subject-independent classification setup, and average classification accuracies of 63.92% and 78.60% are obtained on BCI IV-2a and 2b datasets respectively which is an improvement of 8 to 10% compared to the existing methods. The classification performance and computational cost comparisons demonstrate the superior performance of TriNet compared to the existing methods highlighting its potential to enhance MI classification in BCI. © 2013 IEEE.},
	author_keywords = {brain-computer interface (BCI); electroencephalography (EEG); encoder-decoder; extreme learning machine (ELM); fast Fourier transform (FFT); motor imagery (MI); self-attention; TriNet},
	keywords = {Bandpass filters; Benchmarking; Biomedical signal processing; Classification (of information); Decoding; Electroencephalography; Electrophysiology; Fast Fourier transforms; Image classification; Intelligent control; Interfaces (computer); Knowledge acquisition; Knowledge transfer; Learning systems; Machine learning; Medical computing; Signal encoding; Brain-computer interface; Encoder-decoder; Extreme learning machine; Fast fourier; Fast fourier transform; Learning machines; Motor imagery; Self-attention; Trinetp; Brain computer interface},
	correspondence_address = {H.F. Abbasi; Northwestern Polytechnical University, School of Automation, Xi’an, Shaanxi, 710072, China; email: faizaabbasi3015

@ARTICLE{Jia2025Braincomputer,
	author = {Jia, Yingjie and Shi, Tianwei and Ren, Ling and Cui, Wenhua},
	title = {A Brain-Computer Interface Four-class Classification Algorithm Integrating a Custom Spiking Neural Network with Attention Mechanisms},
	year = {2025},
	journal = {IAENG International Journal of Computer Science},
	volume = {52},
	number = {7},
	pages = {2381 - 2390},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009733596&partnerID=40&md5=09dc0a31c74c6cdf4d2fbd2b094bcc4b},
	affiliations = {University of Science and Technology Liaoning, School of Computer Science and Software Engineering, Anshan, Liaoning, China},
	abstract = {This paper addresses the challenges of feature extraction and classification accuracy in brain-computer interface (BCI) systems based on motor imagery tasks. We propose the SAMPN-Network model, which integrates a custom spiking neural network with a soft attention mechanism (SoftAttentionLayer), alongside the Simplicityformer classifier that incorporates a multi-head attention mechanism (MHA). The resulting classification algorithm, named Custom Spiking Neural Network Layer with Soft Attention Mechanism and Multi-Head Attention for Classification (SNA-MHC), is specifically designed to optimize classification accuracy in BCI systems. In our approach, raw EEG signals corresponding to motor imagery (MI) tasks are first normalized and then transformed into discrete spike trains using threshold encoding to make them suitable for processing by Spiking Neural Networks (SNN). These spike signals are subsequently processed by the SAMPN-Network model, which performs feature extraction by integrating a soft attention mechanism with the SNN module. The SNN module utilizes pulse neurons to encode and enhance the temporal information in EEG signals. Concurrently, the soft attention mechanism calculates attention weights to automatically focus on critical segments of the EEG signals associated with MI tasks while suppressing background noise and irrelevant temporal information, thereby extracting more precise time-series features. Following time-sequence feature extraction, a Multi-Head Attention Mechanism performs parallel attention computation across time domain, frequency domain, and more abstract feature spaces. This approach captures interdependencies between features across different dimensions and enhances the discriminative power of the classifier. Finally, the integrated features are processed by a Softmax classifier to perform four-class classification of MI tasks. Experimental results demonstrate that the proposed SNA-MHC model outperforms existing state-of-the-art models in terms of classification accuracy on both the TechBrain and BCI Competition IV2a datasets. Specifically, SNA-MHC achieves an average classification accuracy improvement of 13.02%, 4.41%, 8.46%, 15.05%, and 15.88%, respectively, when compared to other algorithmic models. Furthermore, when compared to traditional CNN and SNN models, SNA-MHC exhibits superior energy efficiency while maintaining classification accuracy, further validating its robust performance. © (2025), (International Association of Engineers). All rights reserved.},
	author_keywords = {Brain-Computer Interface (BCI); EEG Signal Classification; Motor Imagery (MI); Multi-Head Attention Mechanism; Soft Attention Mechanism; Spiking Neural Network (SNN)},
	keywords = {Biomedical signal processing; Classification (of information); Electroencephalography; Encoding (symbols); Extraction; Feature extraction; Frequency domain analysis; Image classification; Interfaces (computer); Multilayer neural networks; Network layers; Neurons; Signal encoding; Time domain analysis; Attention mechanisms; Brain-computer interface; EEG signals classification; Motor imagery; Multi-head attention mechanism; Neural-networks; Soft attention mechanism; Spiking neural network; Brain computer interface},
	correspondence_address = {T.-W. Shi; School of Computer Science and Software Engineering, University of Science and Technology LiaoNing, Anshan, 114051, China; email: tianweiabbcc

@ARTICLE{Yu2025Armbcisys,
	author = {Yu, Feng and Rao, Zhongrui and Chen, Neng and Liu, Li and Jiang, Minghua},
	title = {ArmBCIsys: Robot Arm BCI System With Time-Frequency Network for Multiobject Grasping},
	year = {2025},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	volume = {36},
	number = {10},
	pages = {18327 - 18341},
	doi = {10.1109/TNNLS.2025.3579332},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009027845&doi=10.1109%2FTNNLS.2025.3579332&partnerID=40&md5=3ec3ebf377cb5ce3ef8f72bdc9c847a1},
	affiliations = {Wuhan Textile University, Engineering Research Center of Hubei Province for Clothing Information, Wuhan, Hubei, China; Wuhan Textile University, School of Computer Science and Artificial Intelligence, Wuhan, Hubei, China},
	abstract = {Brain-computer interface (BCI) offers a direct communication and control channel between the human brain and external devices, presenting new pathways for individuals with physical disabilities to operate robotic arms for complex tasks. However, achieving multiobject grasping tasks under low signal-to-noise ratio (SNR) consumer-grade EEG signals is a significant challenge due to the lack of robust decoding algorithms and precise visual tracking methods. This article proposes, ArmBCIsys, an integrated robotic arm system that combines a novel dual-branch frequency-enhanced network (DBFENet) to robustly decode EEG signals under noisy conditions with the high-precision vision-guided grasping module. The proposed DBFENet designs the scaling temporal convolution block (STCB) to extract multiscale spatiotemporal features from the time domain, while the designed DropScale projected Transformer (DSPT) utilizes discrete cosine transform (DCT) to capture key frequency-domain features, significantly improving decoding robustness. We fine-tune the masked-attention mask Transformer (Mask2Former) model on the Jacquard dataset and incorporate the multiframe centroid-intersection over union (IoU) tracking algorithm to build visual grasp segmenter (VisGraspSeg), enabling reliable segmentation and dynamic tracking for diverse daily objects. Experimental validations on both self-built code-modulated visual evoked potential (c-VEP) dataset (1344 samples) and two public c-VEP datasets demonstrate that DBFENet achieves the state-of-the-art recognition performance, and the system integrates the DBFENet and proposed vision-guided module and ensures stable multiobject selecting and automatic object grasping in dynamic environments, extending promising applications in healthcare robotics, assistive technology, and industrial automation. The self-built dataset has been made publicly accessible at https://github.com/wtu1020/ ArmBCIsys-Self-built-cVEP-Dataset © 2025 IEEE.},
	author_keywords = {Brain-computer interface (BCI); frequency-enhanced network; multiobject selecting; robotic arm; vision-guided grasping},
	keywords = {ARM processors; Assistive technology; Automatic guided vehicles; Brain; Brain computer interface; Computer vision; Decoding; Frequency domain analysis; Interfaces (computer); Man machine systems; Medical computing; Time domain analysis; Visual servoing; Brain–computer interface; EEG signals; Frequency-enhanced network; Interface system; Multiobject; Multiobject selecting; Robot arms; Time frequency; Vision-guided grasping; Visual evoked potential; Robotic arms; adult; algorithm; arm; artificial neural network; brain computer interface; devices; electroencephalography; hand strength; human; physiology; procedures; robotics; signal noise ratio; Adult; Algorithms; Arm; Brain-Computer Interfaces; Electroencephalography; Hand Strength; Humans; Neural Networks, Computer; Robotics; Signal-To-Noise Ratio},
	correspondence_address = {M. Jiang; Wuhan Textile University, School of Computer Science and Artificial Intelligence, Engineering Research Center of Hubei Province for Clothing Information, Wuhan, Hubei, 430200, China; email: minghuajiang

@ARTICLE{Raza2025Adaptive,
	author = {Raza, Aaqib and Yusoff, Mohd Zuki},
	title = {An Adaptive Convolutional Neural Network With Spatio-Temporal Attention and Dynamic Pathways (ACNN-STADP) for Robust EEG-Based Motor Imagery Classification},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {106387 - 106405},
	doi = {10.1109/ACCESS.2025.3580145},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008648364&doi=10.1109%2FACCESS.2025.3580145&partnerID=40&md5=a626573c1bc1e34e2a827756a92d3983},
	affiliations = {Universiti Teknologi PETRONAS, Department of Electrical and Electronic Engineering, Seri Iskandar, Perak, Malaysia},
	abstract = {Electroencephalogram (EEG)-based Brain-Computer Interfaces (BCIs) have gained substantial attention, particularly for motor imagery (MI) that facilitates direct brain-to-device communication without any muscular movement. However, existing classification models face limitations such as inter-subject variability, lack of generalizability, high computational demands, low signal-to-noise ratios, and inefficient feature extraction, which impede their robustness and accuracy. Moreover, advanced deep learning models often utilize rigid architectures with fixed spatial-temporal filters, restricting their adaptability to dynamic EEG patterns. To address these challenges, this paper proposes an Adaptive Convolutional Neural Network with Spatio-Temporal Attention and Dynamic Pathways (ACNN-STADP), which introduces a novel dynamic pathway mechanism and adaptive attention strategy for robust MI-EEG decoding. The proposed model integrates a Dynamic Pathway Convolution Network (DPCN) for adaptive feature extraction, incorporating a Dynamic Gating Controller (DGC) and Dynamic Adaptive Spatio-Temporal (DAST) blocks to efficiently capture multi-scale spatial and temporal dependencies. Additionally, an Adaptive Attention Fusion (AAF) module employs Dual Multi-Head Self-Attention (DMHSA) and a U-Net-inspired Adaptive Fusion Block (AFB) to enhance feature integration and improve classification performance. Furthermore, the model introduces three key innovations: Dynamic Multi-Scale Convolutional Learning for adaptive kernel selection, Unified Spatio-Temporal Attention (USTA) for efficient feature recalibration, and AFB for multi-scale feature fusion while preserving long-range dependencies. The model is validated on BCI Competition IV Dataset 2a, achieving a peak accuracy of 90.77%, and further evaluated across six additional MI-EEG datasets, demonstrating an overall average accuracy above 78.98%. ACNN-STADP significantly improves generalization, reduces computational complexity, and enhances real-time applicability, establishing a robust multi-dataset adaptive deep learning framework for EEG-based MI classification. © 2013 IEEE.},
	author_keywords = {adaptive CNN; Brain-computer interface (BCI); classification; dynamic pathways convolution; electroencephalogram (EEG); motor imagery (MI); unified spatio-temporal attention},
	keywords = {Adaptive control systems; Biomedical signal processing; Brain; Brain computer interface; Complex networks; Convolution; Convolutional neural networks; Deep learning; Electroencephalography; Extraction; Feature Selection; Image classification; Interfaces (computer); Learning systems; Memory architecture; Signal to noise ratio; Adaptive CNN; Brain-computer interface; Convolutional neural network; Dynamic pathway convolution; Dynamic pathways; Electroencephalogram; Motor imagery; Spatio-temporal; Unified spatio-temporal attention; Classification (of information)},
	correspondence_address = {M.Z. Yusoff; Universiti Teknologi PETRONAS (UTP), Centre for Intelligent Signal and Imaging Research (CISIR), Electrical and Electronic Engineering Department, Seri Iskandar, 32610, Malaysia; email: mzuki_yusoff

@ARTICLE{Chen2025Mindgpt,
	author = {Chen, Jiaxuan and Qi, Yu and Wang, Yueming and Pan, Gang},
	title = {MindGPT: Interpreting What You See With Non-Invasive Brain Recordings},
	year = {2025},
	journal = {IEEE Transactions on Image Processing},
	volume = {34},
	pages = {3281 - 3293},
	doi = {10.1109/TIP.2025.3572784},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007308816&doi=10.1109%2FTIP.2025.3572784&partnerID=40&md5=ca4410944f0c7d867e0b164f69c6b128},
	affiliations = {College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Zhejiang University, MOE Frontier Science Center for Brain Science and Brain-machine Integration, Hangzhou, Zhejiang, China; Zhejiang University, Hangzhou, Zhejiang, China},
	abstract = {Decoding of seen visual contents with non-invasive brain recordings has important scientific and practical values. Efforts have been made to recover the seen images from brain signals. However, most existing approaches cannot faithfully reflect the visual contents due to insufficient image quality or semantic mismatches. Compared with reconstructing pixel-level visual images, speaking is a more efficient and effective way to explain visual information. Here we introduce a non-invasive neural decoder, termed MindGPT, which interprets perceived visual stimuli into natural languages from functional Magnetic Resonance Imaging (fMRI) signals in an end-to-end manner. Specifically, our model builds upon a visually guided neural encoder with a cross-attention mechanism. By the collaborative use of data augmentation techniques, this architecture permits us to guide latent neural representations towards a desired language semantic direction in a self-supervised fashion. Through doing so, we found that the neural representations of the MindGPT are explainable, which can be used to evaluate the contributions of visual properties to language semantics. Our experiments show that the generated word sequences truthfully represented the visual information (with essential details) conveyed in the seen stimuli. The results also suggested that with respect to language decoding tasks, the higher visual cortex (HVC) is more semantically informative than the lower visual cortex (LVC), and using only the HVC can recover most of the semantic information. © 1992-2012 IEEE.},
	author_keywords = {functional magnetic resonance imaging; multimodal representation learning; Neural decoding; self-supervised learning; text reconstruction},
	keywords = {Neuroimaging; Visual languages; Functional magnetic resonance imaging; Language semantics; Multi-modal; Multimodal representation learning; Neural decoding; Neural representations; Text reconstruction; Visual content; Visual cortexes; Visual information; Image coding; adult; algorithm; brain; brain mapping; diagnostic imaging; female; human; image processing; language; male; nuclear magnetic resonance imaging; physiology; procedures; semantics; signal processing; vision; visual cortex; young adult; Adult; Algorithms; Brain; Brain Mapping; Female; Humans; Image Processing, Computer-Assisted; Language; Magnetic Resonance Imaging; Male; Semantics; Signal Processing, Computer-Assisted; Visual Cortex; Visual Perception; Young Adult},
	correspondence_address = {Y. Qi; Zhejiang University, Affiliated Mental Health Center, Hangzhou Seventh People’s Hospital, MOE Frontier Science Center for Brain Science and Brain-Machine Integration, Hangzhou, 310027, China; email: qiyu

@ARTICLE{Tong2025Mmpi,
	author = {Tong, Jinze and Chen, Wanzhong},
	title = {MMPI Net: A Novel Multimodal Model Considering the Similarities Between Perception and Imagination for Image Evoked EEG Decoding},
	year = {2025},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {29},
	number = {8},
	pages = {5549 - 5560},
	doi = {10.1109/JBHI.2025.3554664},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001374297&doi=10.1109%2FJBHI.2025.3554664&partnerID=40&md5=1adfe17a7859c3963189b6955b9e6650},
	affiliations = {Jilin University, School of Communication Engineering, Changchun, Jilin, China},
	abstract = {In recent years, non-invasive electroencephalography (EEG) has been widely used to decode high-level cognitive functions, such as visual perception and imagination. The processes of visual perception and imagination in the human brain have been shown to share similar neural circuits and activation patterns in cognitive science. However, current research predominantly focuses on single cognitive processes, overlooking the natural commonalities between these processes and the insights that multimodal approaches can provide. To address this, this study proposes a novel multimodal model, MMPI Net, for jointly decoding EEG signals of visual image perception and imagination. MMPI Net comprises four components: Primitive Feature Extraction for Perception and Imagination (PFE), Cross-Semantic Feature Fusion (CSFF), Joint Semantic Feature Decoder (JSFD), and Semantic Classification (SC). To ensure the effectiveness of PFEM, an Improved Channel Attention Mechanism is introduced, which employs multiple parallel convolutional branches to enhance the extraction of important information and utilizes a Diverse Branch Block approach to reduce the parameter count. In the CSFF module, a cross-attention-based fusion method is designed to effectively capture and utilize intermodal information. In the JSFD phase, a Kolmogorov-Arnold Network is incorporated and coupled with linear layers to improve classification performance. Finally, a linear layer with Softmax is used as the SC module. Experimental results on two publicly available datasets show that, compared to models that use a single cognitive process, MMPI Net achieves average accuracy improvements of 14.22% and 106.1%, demonstrating its effectiveness © 2013 IEEE.},
	author_keywords = {decoding; Electroencephalogram; imagination; multimodal; perception},
	keywords = {Decoding; Image enhancement; Cognitive functions; Cognitive process; Features fusions; Human brain; Imagination; Multi-modal; Multimodal models; Semantic classification; Semantic features; Visual perception; Electroencephalography; article; decoding; electroencephalogram; electroencephalography; feature extraction; human; imagination; Minnesota Multiphasic Personality Inventory; nerve cell network; retina image; adult; algorithm; artificial neural network; brain; female; male; physiology; procedures; signal processing; vision; young adult; Adult; Algorithms; Brain; Female; Humans; Male; Neural Networks, Computer; Signal Processing, Computer-Assisted; Visual Perception; Young Adult},
	correspondence_address = {W. Chen; Jilin University, School of College of Communication Engineering, Changchun, 130012, China; email: chenwz

@ARTICLE{Li2025Neuron,
	author = {Li, Dongdong and Huang, Shengyao and Xie, Li and Wang, Zhe and Xu, Jiazhen},
	title = {Neuron Perception Inspired EEG Emotion Recognition With Parallel Contrastive Learning},
	year = {2025},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	volume = {36},
	number = {8},
	pages = {14049 - 14062},
	doi = {10.1109/TNNLS.2025.3546283},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000217168&doi=10.1109%2FTNNLS.2025.3546283&partnerID=40&md5=ba420ba1fc48788ce2edcf8fb72b6921},
	affiliations = {East China University of Science and Technology, Department of Computer Science and Engineering, Shanghai, China; Qingdao University, Cancer Institute, Qingdao, Shandong, China},
	abstract = {Considerable interindividual variability exists in electroencephalogram (EEG) signals, resulting in challenges for subject-independent emotion recognition tasks. Current research in cross-subject EEG emotion recognition has been insufficient in uncovering the shared neural underpinnings of affective processing in the human brain. To address this issue, we propose the parallel contrastive multisource domain adaptation (PCMDA) model, inspired by the neural representation mechanism in the ventral visual cortex. Our model employs a neuron-perception-inspired contrastive learning architecture for EEG-based emotion recognition in subject-independent scenarios. A two-stage alignment methodology is employed for the purpose of aligning numerous source domains with the target domain. This approach integrates a parallel contrastive loss (PCL) which simulates the self-supervised learning mechanism inherent in the neural representation of the human brain. Furthermore, a self-attention mechanism is integrated to extract emotion weights for each frequency band. Extensive experiments were conducted on three publicly available EEG emotion datasets, SJTU emotion EEG dataset (SEED), database for emotion analysis using physiological signals (DEAP), and finer-grained affective computing EEG dataset (FACED), to evaluate our proposed method. The results demonstrate that the PCMDA effectively utilizes the unique EEG features and frequency band information of each subject, leading to improved generalization across different subjects in comparison to other methods. © 2012 IEEE.},
	author_keywords = {Contrastive learning; electroencephalogram (EEG); emotion recognition; multisource domain adaptation (DA); visual perception},
	keywords = {Adversarial machine learning; Brain; Emotion Recognition; Neurons; Physiological models; Self-supervised learning; Supervised learning; 'current; Domain adaptation; Electroencephalogram; Electroencephalogram signals; Emotion recognition; Human brain; Multi-Sources; Multisource domain adaptation; Neural representations; Visual perception; Contrastive Learning; algorithm; artificial neural network; electroencephalography; emotion; factual database; human; machine learning; nerve cell; physiology; procedures; Algorithms; Databases, Factual; Electroencephalography; Emotions; Humans; Machine Learning; Neural Networks, Computer},
	correspondence_address = {D. Li; East China University of Science and Technology, Department of Computer Science and Engineering, Shanghai, 200237, China; email: ldd

@ARTICLE{Zeynali2024Eegbased,
	author = {Zeynali, Mahsa and Narimani, Haniyeh and Seyedarabi, Hadi},
	title = {EEG-based identification and cryptographic key generation system using extracted features from transformer-based models},
	year = {2024},
	journal = {Signal, Image and Video Processing},
	volume = {18},
	number = {12},
	pages = {9331 - 9346},
	doi = {10.1007/s11760-024-03549-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206655185&doi=10.1007%2Fs11760-024-03549-8&partnerID=40&md5=fb3577277d496458cfa4f4a63b96604e},
	affiliations = {University of Tabriz, Faculty of Electrical and Computer Engineering, Tabriz, East Azerbaijan, Iran},
	abstract = {Biometric systems use the unique behavioral or physical characteristics of a user to verify their claimed identity. Due to the high probability of forgery or theft of traditional passwords and keys, there is a decreasing tendency to use them in security systems. By using biometric indicators, it becomes impossible to forge or steal them. Electroencephalogram (EEG) signals meet the basic requirements of biometric indicators, making them suitable for use in authentication and crypto-biometric systems. In this paper, the first step involves extracting features from recorded EEG signals using transformer-based models within an identification system. In the second step, the extracted features are imported into a key generation system. The proposed method maps the features of each user to different segments. The distributions of the segment indexes are then used to generate repeatable keys from EEG features in future sessions. The Transformer-based identification system achieved a mean accuracy of 99.8%, and the key generation system achieved a 0.1% mean Half Total Error Rate (HTER) using five different categories of visual stimulus. The high accuracy of the proposed identification system and the low error rate of the proposed key generation system indicate that features extracted by the Transformers are a good choice for visual stimulus EEG-based biometric systems. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.},
	author_keywords = {Biometric; Cryptography; Deep learning; Electroencephalogram (EEG); Identification; Transformer},
	keywords = {Authentication; Cryptography; Distribution transformers; Biometric systems; Deep learning; Electroencephalogram; Electroencephalogram signals; Generation systems; Identification; Identification keys; Key generation; Transformer; Visual stimulus},
	correspondence_address = {H. Seyedarabi; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; email: seyedarabi

@ARTICLE{Chowdhury2024Attention,
	author = {Chowdhury, Ritesh Sur and Bose, Shirsha and Ghosh, Sayantani and Konar, Amit},
	title = {Attention Induced Dual Convolutional-Capsule Network (AIDC-CN): A deep learning framework for motor imagery classification},
	year = {2024},
	journal = {Computers in Biology and Medicine},
	volume = {183},
	pages = {},
	doi = {10.1016/j.compbiomed.2024.109260},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206530416&doi=10.1016%2Fj.compbiomed.2024.109260&partnerID=40&md5=3c3f705ecb81574e66d43e0474518367},
	affiliations = {Jadavpur University, Department of Electronics and Telecommunication Engineering, Kolkata, WB, India; Technische Universität München, Munich, Bayern, Germany},
	abstract = {In recent times, Electroencephalography (EEG)-based motor imagery (MI) decoding has garnered significant attention due to its extensive applicability in healthcare, including areas such as assistive robotics and rehabilitation engineering. Nevertheless, the decoding of EEG signals presents considerable challenges owing to their inherent complexity, non-stationary characteristics, and low signal-to-noise ratio. Notably, deep learning-based classifiers have emerged as a prominent focus for addressing the EEG signal decoding process. This study introduces a novel deep learning classifier named the Attention Induced Dual Convolutional-Capsule Network (AIDC-CN) with the specific aim of accurately categorizing various motor imagination class labels. To enhance the classifier's performance, a dual feature extraction approach leveraging spectrogram and brain connectivity networks has been employed, diversifying the feature set in the classification task. The main highlights of the proposed AIDC-CN classifier includes the introduction of a dual convolution layer to handle the brain connectivity and spectrogram features, addition of a novel self-attention module (SAM) to accentuate the relevant parts of the convolved spectrogram features, introduction of a new cross-attention module (CAM) to refine the outputs obtained from the dual convolution layers and incorporation of a Gaussian Error Linear Unit (GELU) based dynamic routing algorithm to strengthen the coupling among the primary and secondary capsule layers. Performance analysis undertaken on four public data sets depict the superior performance of the proposed model with respect to the state-of-the-art techniques. The code for this model is available at https://github.com/RiteshSurChowdhury/AIDC-CN. © 2024 Elsevier Ltd},
	author_keywords = {Capsule network; Deep learning; Electroencephalography (EEG); Motor imagery},
	keywords = {Brain mapping; Deep learning; Image coding; Image reconstruction; Assistive rehabilitations; Assistive robotics; Brain connectivity; Capsule network; Electroencephalography; Learning frameworks; Motor imagery; Motor imagery classification; Spectrograms; algorithm; article; classification; classifier; deep learning; diagnosis; electroencephalogram; electroencephalography; feature extraction; human; imagery; imagination; robotics; signal noise ratio; artificial neural network; attention; brain; brain computer interface; physiology; procedures; signal processing; Attention; Brain; Brain-Computer Interfaces; Deep Learning; Humans; Imagination; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	correspondence_address = {A. Konar; Artificial Intelligence Laboratory, Department of Electronics and Telecommunication Engineering, Jadavpur University, Kolkata, West Bengal, 700032, India; email: konaramit

@ARTICLE{Xu2024Enhancing,
	author = {Xu, Fangzhou and Shi, Weiyou and Lv, Chengyan and Sun, Yuan and Guo, Shuai and Feng, Chao and Zhang, Yang and Jung, Tzyy Ping and Leng, Jiancai},
	title = {Enhancing Motor Imagery Classification with Residual Graph Convolutional Networks and Multi-Feature Fusion},
	year = {2024},
	journal = {International Journal of Neural Systems},
	volume = {35},
	number = {1},
	pages = {},
	doi = {10.1142/S0129065724500692},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209761434&doi=10.1142%2FS0129065724500692&partnerID=40&md5=4f0b46d2c0a5cb1d57a5348615081c07},
	affiliations = {Qilu University of Technology, International School for Optoelectronic Engineering, Jinan, Shandong, China; Qilu Hospital of Shandong University, Rehabilitation Center, Jinan, Shandong, China; Shandong University of Traditional Chinese Medicine, Rehabilitation and Physical Therapy Department, Jinan, Shandong, China; Institute for Neural Computation, Swartz Center for Computational Neuroscience, La Jolla, CA, United States},
	abstract = {Stroke, an abrupt cerebrovascular ailment resulting in brain tissue damage, has prompted the adoption of motor imagery (MI)-based brain-computer interface (BCI) systems in stroke rehabilitation. However, analyzing electroencephalogram (EEG) signals from stroke patients poses challenges. To address the issues of low accuracy and efficiency in EEG classification, particularly involving MI, the study proposes a residual graph convolutional network (M-ResGCN) framework based on the modified S-transform (MST), and introduces the self-attention mechanism into residual graph convolutional network (ResGCN). This study uses MST to extract EEG time-frequency domain features, derives spatial EEG features by calculating the absolute Pearson correlation coefficient (aPcc) between channels, and devises a method to construct the adjacency matrix of the brain network using aPcc to measure the strength of the connection between channels. Experimental results involving 16 stroke patients and 16 healthy subjects demonstrate significant improvements in classification quality and robustness across tests and subjects. The highest classification accuracy reached 94.91% and a Kappa coefficient of 0.8918. The average accuracy and F1 scores from 10 times 10-fold cross-validation are 94.38% and 94.36%, respectively. By validating the feasibility and applicability of brain networks constructed using the aPcc in EEG signal analysis and feature encoding, it was established that the aPcc effectively reflects overall brain activity. The proposed method presents a novel approach to exploring channel relationships in MI-EEG and improving classification performance. It holds promise for real-time applications in MI-based BCI systems. © 2024 World Scientific Publishing Company.},
	author_keywords = {Brain network; modified residual graph convolutional network; modified S -transform; self-attention mechanism; stroke},
	keywords = {Brain mapping; Convolutional neural networks; Electroencephalography; Electrotherapeutics; Image enhancement; Image reconstruction; Patient rehabilitation; Photomapping; Attention mechanisms; Brain networks; Convolutional networks; Modified residual graph convolutional network; Modified S -transform; Motor imagery; Pearson correlation coefficients; S-transforms; Self-attention mechanism; Stroke; Frequency domain analysis; adult; aged; artificial neural network; brain; brain computer interface; cerebrovascular accident; electroencephalography; female; human; imagination; male; middle aged; pathophysiology; physiology; procedures; signal processing; stroke rehabilitation; Adult; Aged; Brain; Brain-Computer Interfaces; Female; Humans; Imagination; Male; Middle Aged; Neural Networks, Computer; Signal Processing, Computer-Assisted; Stroke Rehabilitation},
	correspondence_address = {F. Xu; International School for Optoelectronic Engineering, Qilu University of Technology (Shandong Academy of Sciences), Jinan, 250353, China; email: xfz

@ARTICLE{Zhao2024Eeg,
	author = {Zhao, Qian and Zhao, Dandan and Yin, Wuliang},
	title = {EEG Emotion Recognition Based on GADF and AMB-CNN Model},
	year = {2024},
	journal = {International Journal of Numerical Modelling: Electronic Networks, Devices and Fields},
	volume = {37},
	number = {6},
	pages = {},
	doi = {10.1002/jnm.70000},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210085656&doi=10.1002%2Fjnm.70000&partnerID=40&md5=c20e7c70c3da72e9f808bca8ade5be42},
	affiliations = {Qufu Normal University, College of Engineering, Qufu, Shandong, China; The University of Manchester, School of Electrical and Electronic Engineering, Manchester, Greater Manchester, United Kingdom},
	abstract = {Deep learning has achieved better results in natural language processing, computer vision, and other fields. Nowadays, more deep learning algorithms have also been applied in brain-based emotion recognition. In the studies on brain-based emotion recognition, deep learning models typically use one-dimensional time series as the input and cannot fully leverage the advantages of the models in image classification or recognition. To address this issue, based on the publicly available SEED and DEAP datasets, the Gramian angular difference field (GADF) method was proposed to construct two-dimensional image representation datasets: SEED-GADF and DEAP-GADF datasets, in the paper. Additionally, a convolutional attention mechanism model (AMB-CNN) was introduced and its classification performance was validated on SEED-GADF and DEAP-GADF datasets. AMB-CNN achieved an average accuracy of 90.8%, a recall rate of 90%, and AUC of 96.86% on SEED-GADF. On DEAP-GADF, the average accuracy, recall rate, and AUC respectively reached 96.06%, 96.06%, and 98.58% in the valence dimension and 96.11%, 96.11%, and 98.73% in the arousal dimension. Finally, the comparison results with various algorithms and ablation experiments proved the superiority of the proposed model. © 2024 John Wiley & Sons Ltd.},
	author_keywords = {attention mechanism block; CNN; EEG; emotion recognition; Gramian angular difference field},
	keywords = {Adversarial machine learning; Emotion Recognition; Attention mechanism block; Attention mechanisms; CNN models; Emotion recognition; Gramian angular difference field; Gramians; Language processing; Learning models; Natural languages; Recall rate; Contrastive Learning},
	correspondence_address = {D. Zhao; College of Engineering, Qufu Normal University, Rizhao, China; email: cgzdd123

@ARTICLE{Yang2024Novel,
	author = {Yang, Guangyu and Liu, Jinguo},
	title = {A novel multi-scale fusion convolutional neural network for EEG-based motor imagery classification},
	year = {2024},
	journal = {Biomedical Signal Processing and Control},
	volume = {96},
	pages = {},
	doi = {10.1016/j.bspc.2024.106645},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199269395&doi=10.1016%2Fj.bspc.2024.106645&partnerID=40&md5=078379d0cc26e4a8039b409fa665fba0},
	affiliations = {Shenyang Institute of Automation Chinese Academy of Sciences, State Key Laboratory of Robotics, Shenyang, Liaoning, China; Chinese Academy of Sciences, Institutes for Robotics and Intelligent Manufacturing, Beijing, Beijing, China; University of Chinese Academy of Sciences, Beijing, China},
	abstract = {Brain-computer interfaces based on motor imagery have played important roles in motor rehabilitation, brain function regulation, disease monitoring, etc. However, due to the low signal-to-noise ratio and spatial resolution of the EEG, their decoding performance still needs to be further improved. In this paper, we propose a multi-scale fusion convolutional neural network model (MSFCNNet) for four classification tasks involving motor imagery EEG signals. Based on EEGNet, we add an attention module and a two-dimensional dilated convolution layer to construct networks of different scales and carry out network fusion. The attention module utilizes the multi-head self-attention mechanism to highlight the most valuable features. The two-dimensional dilated convolution layer recognizes features effectively and increases the receptive field of the model. In addition, the multi-scale network fusion strategy further improves the decoding performance of the model. Our proposed model is tested on the BCI Competition IV-2a dataset and the High-Gamma dataset. Under subject-dependent and subject-independent models, the classification accuracy of the proposed model on the BCI Competition IV-2a dataset reaches 87.16% and 71.03%, respectively. And in the High-Gamma dataset, the classification accuracy of the proposed model reached 94.43% in the subject-dependent model. Compared with the filter bank common spatial pattern (FBCSP) algorithm and other deep network models, our model achieves a certain improvement in classification accuracy. The experimental results show that MSFCNNet has better decoding performance and stronger robustness. © 2024 Elsevier Ltd},
	author_keywords = {Brain-computer interface (BCI); Convolutional neural network (CNN); Electroencephalogram (EEG); Motor imagery (MI); Multi-head self-attention (MHSA); Multi-scale fusion convolutional neural network (MSFCNNet)},
	keywords = {Biomedical signal processing; Classification (of information); Convolution; Convolutional neural networks; Decoding; Electroencephalography; Image classification; Image enhancement; Network layers; Neural network models; Signal to noise ratio; Brain-computer interface; Convolutional neural network; Electroencephalogram; Motor imagery; Multi-head self-attention; Multi-scale fusion convolutional neural network; Multiscale fusion; Brain computer interface; algorithm; Article; classification; controlled study; convolutional neural network; electroencephalogram; imagery; receptive field; signal noise ratio},
	correspondence_address = {J. Liu; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Science, Shenyang, 110016, China; email: liujinguo

@ARTICLE{Wang2024Augmentation,
	author = {Wang, Yuqi and Li, Weidong and Liang, Yuchen and Pham, D. T.},
	title = {Augmentation and classification of motor imagery electroencephalogram signals for human–robot collaborative disassembly},
	year = {2024},
	journal = {International Journal of Advanced Manufacturing Technology},
	volume = {133},
	number = {11-12},
	pages = {5927 - 5948},
	doi = {10.1007/s00170-024-14098-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198080367&doi=10.1007%2Fs00170-024-14098-2&partnerID=40&md5=9dc8a4ae261a9aeea1dde8302e96a2e5},
	affiliations = {Wuhan University of Technology, School of Transportation and Logistics Engineering, Wuhan, Hubei, China; University of Shanghai for Science and Technology, School of Mechanical Engineering, Shanghai, Shanghai, China; Jiangsu University, Zhenjiang, Jiangsu, China; University of Birmingham, Department of Mechanical Engineering, Birmingham, West Midlands, United Kingdom},
	abstract = {As an essential step in remanufacturing end-of-life (EoL) products, disassembly is performed to retrieve high-value parts and materials for use in subsequent remanufacturing processes. Human–robot collaboration (HRC) supported by a brain-machine interface (BMI) can provide an intelligent and versatile solution to address the requirements of disassembly. An intuitive control function of a robot enabled by BMI and human brainwaves (e.g. motor imagery electroencephalogram (MI-EEG)) will be useful for the human to remotely guide the robot to conduct disassembly operations under complicated conditions. To realise such a BMI-enabled HRC disassembly system, it is critical to develop an effective classifier of MI-EEG signals. Nevertheless, the major challenges are that it is possible only to acquire limited MI-EEG signals for system training, and there are weak features and a low signal-to-noise ratio in the signals causing the classification accuracy to deteriorate. To tackle these challenges, in this research, two novel models are developed. The novelties of the models are reflected from the following aspects: (i) to overcome the difficulty in collecting a large number of high-quality MI-EEG signals, a deep convolutional generative adversarial network (DCGAN)-based optimisation model, namely, SAN-DCGAN, has been designed to carry out signal augmentation. The SAN-DCGAN model incorporates new improvements, including a redesigned DCGAN, soft thresholding with an attention mechanism component (SA component), and a spectral normalisation mechanism (SN). (ii) A multi-branch and multi-scale convolutional neural network (MM-CNN)-based model is developed to classify the augmented MI-EEG signals to support robot control. Experiments were conducted to validate the models. Experimental results show that with MI-EEG signal augmentation, the average classification accuracy reached 81.52%, which is higher than the results obtained using other existing models. The research was successfully demonstrated in case studies for disassembling bolts, rigid busbars, and flexible cables from electric vehicle batteries. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.},
	author_keywords = {Disassembly; End-of-life product; Generative adversarial networks; Motor imagery electroencephalogram},
	keywords = {Automata theory; Biomedical signal processing; Brain computer interface; Convolution; Generative adversarial networks; Image classification; Intelligent robots; Signal to noise ratio; Classification accuracy; Disassembly; Electroencephalogram signals; End-of-life products; Human robots; Human-robot collaboration; Machine interfaces; Motor imagery; Motor imagery electroencephalogram; Remanufacturing process; Electroencephalography},
	correspondence_address = {W. Li; School of Mechanical Engineering, University of Shanghai for Science and Technology, Shanghai, China; email: weidongli

@ARTICLE{Lian2024Endtoend,
	author = {Lian, Shidong and Li, Zheng},
	title = {An end-to-end multi-task motor imagery EEG classification neural network based on dynamic fusion of spectral-temporal features},
	year = {2024},
	journal = {Computers in Biology and Medicine},
	volume = {178},
	pages = {},
	doi = {10.1016/j.compbiomed.2024.108727},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196195968&doi=10.1016%2Fj.compbiomed.2024.108727&partnerID=40&md5=c697d97575740f2eb93b51ac66502c95},
	affiliations = {Beijing Normal University, Beijing, China; Beijing Normal University, International Academic Center of Complex Systems, Beijing, China; Beijing Normal University, Center for Cognition and Neuroergonomics, Beijing, China; Beijing Normal University, Department of Psychology, Beijing, China},
	abstract = {Electroencephalograph (EEG) brain-computer interfaces (BCI) have potential to provide new paradigms for controlling computers and devices. The accuracy of brain pattern classification in EEG BCI is directly affected by the quality of features extracted from EEG signals. Currently, feature extraction heavily relies on prior knowledge to engineer features (for example from specific frequency bands); therefore, better extraction of EEG features is an important research direction. In this work, we propose an end-to-end deep neural network that automatically finds and combines features for motor imagery (MI) based EEG BCI with 4 or more imagery classes (multi-task). First, spectral domain features of EEG signals are learned by compact convolutional neural network (CCNN) layers. Then, gated recurrent unit (GRU) neural network layers automatically learn temporal patterns. Lastly, an attention mechanism dynamically combines (across EEG channels) the extracted spectral-temporal features, reducing redundancy. We test our method using BCI Competition IV-2a and a data set we collected. The average classification accuracy on 4-class BCI Competition IV-2a was 85.1 % ± 6.19 %, comparable to recent work in the field and showing low variability among participants; average classification accuracy on our 6-class data was 64.4 % ± 8.35 %. Our dynamic fusion of spectral-temporal features is end-to-end and has relatively few network parameters, and the experimental results show its effectiveness and potential. © 2024 Elsevier Ltd},
	author_keywords = {Attention model; Brain-computer interface (BCI); Compact convolution neural network; Electroencephalograph (EEG); Gated recurrent unit neural network; Motor imagery},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Convolution; Deep neural networks; Electroencephalography; Image classification; Multilayer neural networks; Network layers; Recurrent neural networks; Statistical tests; Attention model; Brain-computer interface; Compact convolution neural network; Convolution neural network; Electroencephalograph; End to end; Gated recurrent unit neural network; Motor imagery; Neural-networks; Temporal features; Extraction; adult; Article; classification; convolutional neural network; deep neural network; electroencephalogram; electrooculogram; feature extraction; female; gated recurrent unit network; human; human experiment; imagery; male; motor performance; normal human; standardization; artificial neural network; brain; brain computer interface; electroencephalography; imagination; physiology; procedures; signal processing; Brain; Brain-Computer Interfaces; Humans; Imagination; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	correspondence_address = {Z. Li; Center for Cognition and Neuroergonomics, State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Zhuhai, China; email: lz

@ARTICLE{Long2024Skmmfmnet,
	author = {Long, Jiawen and Fang, Zhixiang and Wang, Lubin},
	title = {SK-MMFMNet: A multi-dimensional fusion network of remote sensing images and EEG signals for multi-scale marine target recognition},
	year = {2024},
	journal = {Information Fusion},
	volume = {108},
	pages = {},
	doi = {10.1016/j.inffus.2024.102402},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189863852&doi=10.1016%2Fj.inffus.2024.102402&partnerID=40&md5=062fbca6cff37d0639ce7846108a6ce0},
	affiliations = {Wuhan University, Wuhan, Hubei, China},
	abstract = {Intelligent recognition of multi-scale marine targets remains pivotal in studying marine resources and transportation. Multi-scale marine target recognition faces challenges such as blurred image, noise interference, varied target sizes, and random target positions. However, these hardly affect the judgment of human brain which could adeptly capture multi-scale targets and disregard noise interference. Therefore, this study proposes an innovative approach to recognize multi-scale marine targets through taking full advantages of the texture, color and structural information provided by remote sensing images and the quick classification ability of human brains, called Selective Kernel & Multi-dimensional Multimodal Data Fusion Module Network (SK-MMFMNet), which fuses remote sensing images and electroencephalography (EEG) signals to improve the accuracy of classifying multi-scale marine targets. In this study, we construct a multi-scale marine target dataset, which includes both remote sensing images of islands, wind turbines, and ships and their corresponding EEG signals from subjects while viewing remote sensing images. Then, the proposed approach extends the Multimodal Transfer Module (MMTM) based on attention mechanism to a dual fusion module across channel and spatial dimensions to fusing MobileNetV3 and EEGNet. Also, we embed the Selective Kernel Module into MobileNetV3 for addressing multi-scale features. The average experimental results across the three multi-scale marine target sub-dataset show that SK-MMFMNet exhibited accuracy improvements of 2.88 %, 21.60 %, and 1.08 %, moreover, F1-Score increments of 24.60 %, 162.22 %, and 14.32 % compared to MobileNetV3, EEGNet, and MMTMNet (MMTM-based fusion network). Visual analysis via Grad-CAM demonstrates that benefiting from EEG signals and Selective Kernel Module, our proposed SK-MMFMNet adjusts the network attention to exactly focus on the multi-scale target area, and thus achieves the best performance. Meanwhile, T-SNE visualization also proves the effectiveness of the three fusion modules and EEG signals for feature extraction. This study offers a valuable and promising insight for intelligent recognition of multi-scale marine targets. © 2024 Elsevier B.V.},
	author_keywords = {Convolutional neural networks; EEG signals; Marine target recognition; Multi-scale target; Multimodal data fusion},
	keywords = {Biomedical signal processing; Classification (of information); Convolutional neural networks; Data fusion; Electrophysiology; Image classification; Image enhancement; Large datasets; Remote sensing; Textures; Convolutional neural network; Electroencephalography signal; Fusion modules; Marine target recognition; Multi dimensional; Multi-scale target; Multi-scales; Multimodal data fusion; Remote sensing images; Target recognition; Electroencephalography},
	correspondence_address = {Z. Fang; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; email: zxfang

@ARTICLE{Niu2024Timefrequency,
	author = {Niu, Xu and Lü, Na and Yan, Ruofan and Luo, Huan},
	title = {A time-frequency map generation network embedded with spectral self-attention for motor imagery classification},
	year = {2024},
	journal = {Biomedical Signal Processing and Control},
	volume = {93},
	pages = {},
	doi = {10.1016/j.bspc.2024.106206},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188750447&doi=10.1016%2Fj.bspc.2024.106206&partnerID=40&md5=e0e6966d96f9bf20528286e368d119f5},
	affiliations = {Xi'an Jiaotong University, Systems Engineering Institute, Xi'an, Shaanxi, China},
	abstract = {End-to-end networks have achieved remarkable success in Motor Imagery (MI) classification by directly extracting features from raw Electroencephalogram (EEG) signals and performing classification. The self-attention mechanism has been successfully introduced to improve the performance of these networks further. However, the features extracted from existing end-to-end networks lack interpretable and comprehensive spectral information, making it difficult to develop spectral self-attention mechanisms. To overcome this challenge, we proposed a novel end-to-end network called Time-Frequency Map Generation Network (TFGNet), which utilizes the frequency domain information of raw EEG signals to generate interpretable time–frequency maps. TFGNet has shown comparable performance to State-of-the-Art (SotA) methods on MI classification tasks. Subsequently, the first spectral attention module for end-to-end networks for discriminative analysis of frequency components is presented. Extensive experiments have shown that using this module in the TFGNet increases classification accuracy by approximately 3% for classification tasks involving multiple objects. The visualization results have also illustrated the process of generating time–frequency maps and the mechanism of spectral self-attention. © 2024 Elsevier Ltd},
	author_keywords = {Deep learning; Motor imagery; Spectral self-attention; Time-frequency map},
	keywords = {Biomedical signal processing; Deep learning; Frequency domain analysis; Image classification; Attention mechanisms; Electroencephalogram signals; End-to-end network; Map generation; Motor imagery; Motor imagery classification; Performance; Spectral self-attention; Time-frequency map; Electroencephalography; article; attention; classification; deep learning; electroencephalogram; electroencephalography; human; human experiment; imagery; normal human},
	correspondence_address = {N. Lu; Systems Engineering Institute, School of Automation Science and Engineering, Xi'an Jiaotong University, Xi'an, 710049, China; email: lvna2009

@ARTICLE{Yang2024Diagonal,
	author = {Yang, Kaijun and Wang, Jihong and Yang, Liantao and Bian, Lifeng and Luo, Zijiang and Yang, Chen},
	title = {A diagonal masking self-attention-based multi-scale network for motor imagery classification},
	year = {2024},
	journal = {Journal of Neural Engineering},
	volume = {21},
	number = {3},
	pages = {},
	doi = {10.1088/1741-2552/ad5405},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196082324&doi=10.1088%2F1741-2552%2Fad5405&partnerID=40&md5=a42bd48267a8d4100e3c277983282ca9},
	affiliations = {Guizhou University, Power Systems Engineering Research Center, Guiyang, Guizhou, China; Fudan University, Frontier Institute of Chip and System, Shanghai, China; Institute of Intelligent Manufacturing, Foshan, China},
	abstract = {Objective. Electroencephalography (EEG)-based motor imagery (MI) is a promising paradigm for brain-computer interface (BCI), but the non-stationarity and low signal-to-noise ratio of EEG signals make it a challenging task. Approach. To achieve high-precision MI classification, we propose a Diagonal Masking Self-Attention-based Multi-Scale Network (DMSA-MSNet) to fully develop, extract, and emphasize features from different scales. First, for local features, a multi-scale temporal-spatial block is proposed to extract features from different receptive fields. Second, an adaptive branch fusion block is specifically designed to bridge the semantic gap between these coded features from different scales. Finally, in order to analyze global information over long ranges, a diagonal masking self-attention block is introduced, which highlights the most valuable features in the data. Main results. The proposed DMSA-MSNet outperforms state-of-the-art models on the BCI Competition IV 2a and the BCI Competition IV 2b datasets. Significance. Our study achieves rich information extraction from EEG signals and provides an effective solution for MI classification. © 2024 IOP Publishing Ltd},
	author_keywords = {convolutional neural network; fusion; motor imagery; multi-scale; self-attention},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Convolutional neural networks; Electrophysiology; Image classification; Semantics; Signal to noise ratio; Convolutional neural network; High-precision; Local feature; Low signal-to-noise ratio; Motor imagery; Motor imagery classification; Multi-scales; Non-stationarities; Receptive fields; Self-attention; Electroencephalography; adult; Article; convolutional neural network; deep learning; dimensionality reduction; electroencephalography; feature extraction; human; imagery; masking; signal noise ratio; artificial neural network; brain computer interface; classification; imagination; movement (physiology); physiology; procedures; Brain-Computer Interfaces; Humans; Imagination; Movement; Neural Networks, Computer},
	correspondence_address = {J. Wang; Power Systems Engineering Research Center, Ministry of Education, College of Big Data and Information Engineering, Guizhou University, Guiyang, 550025, China; email: wjihong20

@ARTICLE{Qian2024Neurodm,
	author = {Qian, Dongguan and Zeng, Hong and Cheng, Wenjie and Liu, Yu and Bikki, Taha and Pan, Jianjiang},
	title = {NeuroDM: Decoding and visualizing human brain activity with EEG-guided diffusion model},
	year = {2024},
	journal = {Computer Methods and Programs in Biomedicine},
	volume = {251},
	pages = {},
	doi = {10.1016/j.cmpb.2024.108213},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192974953&doi=10.1016%2Fj.cmpb.2024.108213&partnerID=40&md5=e6225017bad6a99d8001def08cab2c3a},
	affiliations = {Hangzhou Dianzi University, School of Computer Science and Technology, Hangzhou, Zhejiang, China; Hangzhou Dianzi University, School of Sciences, Hangzhou, Zhejiang, China},
	abstract = {Background and Objective: Brain–Computer Interface (BCI) technology has recently been advancing rapidly, bringing significant hope for improving human health and quality of life. Decoding and visualizing visually evoked electroencephalography (EEG) signals into corresponding images plays a crucial role in the practical application of BCI technology. The recent emergence of diffusion models provides a good modeling basis for this work. However, the existing diffusion models still have great challenges in generating high-quality images from EEG, due to the low signal-to-noise ratio and strong randomness of EEG signals. The purpose of this study is to address the above-mentioned challenges by proposing a framework named NeuroDM that can decode human brain responses to visual stimuli from EEG-recorded brain activity. Methods: In NeuroDM, an EEG-Visual-Transformer (EV-Transformer) is used to extract the visual-related features with high classification accuracy from EEG signals, then an EEG-Guided Diffusion Model (EG-DM) is employed to synthesize high-quality images from the EEG visual-related features. Results: We conducted experiments on two EEG datasets (one is a forty-class dataset, and the other is a four-class dataset). In the task of EEG decoding, we achieved average accuracies of 99.80% and 92.07% on two datasets, respectively. In the task of EEG visualization, the Inception Score of the images generated by NeuroDM reached 15.04 and 8.67, respectively. All the above results outperform existing methods. Conclusions: The experimental results on two EEG datasets demonstrate the effectiveness of the NeuroDM framework, achieving state-of-the-art performance in terms of classification accuracy and image quality. Furthermore, our NeuroDM exhibits strong generalization capabilities and the ability to generate diverse images. © 2024 Elsevier B.V.},
	author_keywords = {Diffusion model; Electroencephalography; Feature extraction; Image generation},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Decoding; Diffusion; Electrophysiology; Neurophysiology; Signal to noise ratio; Brain activity; Classification accuracy; Diffusion model; Features extraction; High quality images; Human brain; Human health; Image generations; Interface technology; Quality of life; Electroencephalography; algorithm; Article; autoencoder; data visualization; deep learning; diffusion; electroencephalogram; electroencephalography; feature extraction; human; signal noise ratio; visual stimulation; brain; brain computer interface; diagnostic imaging; physiology; signal processing; visual evoked potential; Algorithms; Brain-Computer Interfaces; Evoked Potentials, Visual; Humans; Signal Processing, Computer-Assisted; Signal-To-Noise Ratio},
	correspondence_address = {J. Pan; School of Sciences, Hangzhou Dianzi University, Hangzhou, Zhejiang, China; email: mathpan

@ARTICLE{Liu2024Multimodal,
	author = {Liu, Lei and Li, Jian and Ouyang, Rui and Zhou, Danya and Fan, Cunhang and Liang, Wen and Li, Fan and Lv, Zhao and Wu, Xiaopei},
	title = {Multimodal brain-controlled system for rehabilitation training: Combining asynchronous online brain–computer interface and exoskeleton},
	year = {2024},
	journal = {Journal of Neuroscience Methods},
	volume = {406},
	pages = {},
	doi = {10.1016/j.jneumeth.2024.110132},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190162521&doi=10.1016%2Fj.jneumeth.2024.110132&partnerID=40&md5=b9c1b5123526d7f25896f827b39bc8c5},
	affiliations = {Anhui University, School of Computer Science and Technology, Hefei, Anhui, China; Anhui University, Hefei, Anhui, China; Zhengzhou University, School of Basic Medical Sciences, Zhengzhou, Henan, China; Civil Aviation Flight University of China, Guanghan, Sichuan, China; Google LLC, Mountain View, CA, United States},
	abstract = {Background: Traditional therapist-based rehabilitation training for patients with movement impairment is laborious and expensive. In order to reduce the cost and improve the treatment effect of rehabilitation, many methods based on human–computer interaction (HCI) technology have been proposed, such as robot-assisted therapy and functional electrical stimulation (FES). However, due to the lack of active participation of brain, these methods have limited effects on the promotion of damaged nerve remodeling. New method: Based on the neurofeedback training provided by the combination of brain–computer interface (BCI) and exoskeleton, this paper proposes a multimodal brain-controlled active rehabilitation system to help improve limb function. The joint control mode of steady-state visual evoked potential (SSVEP) and motor imagery (MI) is adopted to achieve self-paced control and thus maximize the degree of brain involvement, and a requirement selection function based on SSVEP design is added to facilitate communication with aphasia patients. Comparison with existing methods: In addition, the Transformer is introduced as the MI decoder in the asynchronous online BCI to improve the global perception of electroencephalogram (EEG) signals and maintain the sensitivity and efficiency of the system. Results: In two multi-task online experiments for left hand, right hand, foot and idle states, subject achieves 91.25% and 92.50% best accuracy, respectively. Conclusion: Compared with previous studies, this paper aims to establish a high-performance and low-latency brain-controlled rehabilitation system, and provide an independent and autonomous control mode of the brain, so as to improve the effect of neural remodeling. The performance of the proposed method is evaluated through offline and online experiments. © 2024 Elsevier B.V.},
	author_keywords = {Brain–computer interface; Motor imagery; Movement impairment; Rehabilitation; Steady-state visual evoked potential},
	keywords = {accuracy; aphasia; Article; brain; conceptual framework; electroencephalogram; electroencephalography; exoskeleton; foot; functional electrical stimulation; hand; human; human computer interaction; imagery; latent period; limb function; medical parameters; motor imagery; multimodal brain controlled system; neurofeedback; rehabilitation care; steady state; steady state visual evoked potential; therapy effect; training; visual evoked potential; visual feedback; visual stimulation; workflow; adult; devices; female; guided imagery; imagination; male; pathophysiology; physiology; procedures; young adult; Adult; Brain; Brain-Computer Interfaces; Electroencephalography; Evoked Potentials, Visual; Exoskeleton Device; Female; Humans; Imagery, Psychotherapy; Imagination; Male; Neurofeedback; Young Adult},
	correspondence_address = {C. Fan; School of Computer Science and Technology, Anhui University, Hefei, 230601, China; email: cunhang.fan

@ARTICLE{Bian2024Deeplearningbased,
	author = {Bian, Doudou and Ma, Yue and Huang, Jiayin and Xu, Dongyang and Wang, Zhi and Cai, Shengsheng and Wang, Jiajun and Hu, Nan},
	title = {Deep-learning-based motor imagery EEG classification by exploiting the functional connectivity of cortical source imaging},
	year = {2024},
	journal = {Signal, Image and Video Processing},
	volume = {18},
	number = {4},
	pages = {2991 - 3007},
	doi = {10.1007/s11760-023-02965-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184477004&doi=10.1007%2Fs11760-023-02965-6&partnerID=40&md5=87a8526b57f793d2f03db6b4ac918782},
	affiliations = {Soochow University, School of Electronics and Information Engineering, Suzhou, Jiangsu, China; Zhejiang University, Center for Intelligent Acoustics and Signal Processing, Hangzhou, Zhejiang, China; National Key Laboratory of Industrial Control Technology, Hangzhou, Zhejiang, China; Ltd., Suzhou, Jiangsu, China},
	abstract = {Motor imagery (MI) is a commonly used brain–computer interface paradigm, and decoding the MI-EEG signals has been an active research area in recent years. The existing methods involved various feature extraction and machine learning schemes, while classification accuracy and inter-individual model adaptation still need to be improved. To address these issues, a novel source-domain MI-EEG classification algorithm is proposed in this paper. First, the Champagne algorithm with noise self-learning, is adopted to achieve high-spatial-resolution denoised electrophysiological source imaging (ESI) on the cortex. Second, a kind of brain functional connectivity metric, imaginary coherence (iCOH), is used to exploit the source spatial features in the motor cortex. The iCOH in the motor cortex is calculated to form the graph structure of motor cortical source space during MI, by which graph convolutional networks (GCNs) are constructed to extract the spatial features. Multi-scale temporal features are also derived by temporal convolutional network (TCN) along with multi-head attention mechanism, and spatial attention based on GCN is also used for the interaction of spatio-temporal features. Finally, all the extracted features are combined to give the ultimate classification result. The MI-EEG classification performance of the proposed algorithm is evaluated on the PhysioNet EEG Motor Movement/Imagery Datasetis superior, both for the results of intra-subject fivefold cross validation experiments and subject-specific model training experiments. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.},
	author_keywords = {Brain functional connectivity; Electrophysiological source imaging; Graph convolutional network; Motor imagery EEG; Temporal convolutional network},
	keywords = {Brain; Classification (of information); Convolution; Convolutional neural networks; Electrophysiology; Image classification; Learning systems; Brain functional connectivity; Convolutional networks; EEG classification; Electrophysiological source imaging; Functional connectivity; Graph convolutional network; Motor imagery EEG; Source imaging; Temporal convolutional network; Deep learning},
	correspondence_address = {N. Hu; School of Electronics and Information Engineering, Soochow University, Suzhou, 215006, China; email: hunan

@ARTICLE{Huang2024Decoding,
	author = {Huang, Dingyong and Wang, Yingjie and Fan, Liangwei and Yu, Yang and Zhao, Ziyu and Zeng, Pu and Wang, Kunqing and Li, Na and Shen, Hui},
	title = {Decoding Subject-Driven Cognitive States from EEG Signals for Cognitive Brain–Computer Interface},
	year = {2024},
	journal = {Brain Sciences},
	volume = {14},
	number = {5},
	pages = {},
	doi = {10.3390/brainsci14050498},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194420828&doi=10.3390%2Fbrainsci14050498&partnerID=40&md5=edb2903c32fbca3c896b6db82f7c9b3f},
	affiliations = {National University of Defense Technology China, College of Intelligence Science and Technology, Changsha, Hunan, China; Hebei Normal University of Science and Technology, College of Physical Education and Health, Hebei, China; Central South University, Department of Radiology, Changsha, Hunan, China},
	abstract = {In this study, we investigated the feasibility of using electroencephalogram (EEG) signals to differentiate between four distinct subject-driven cognitive states: resting state, narrative memory, music, and subtraction tasks. EEG data were collected from seven healthy male participants while performing these cognitive tasks, and the raw EEG signals were transformed into time–frequency maps using continuous wavelet transform. Based on these time–frequency maps, we developed a convolutional neural network model (TF-CNN-CFA) with a channel and frequency attention mechanism to automatically distinguish between these cognitive states. The experimental results demonstrated that the model achieved an average classification accuracy of 76.14% in identifying these four cognitive states, significantly outperforming traditional EEG signal processing methods and other classical image classification algorithms. Furthermore, we investigated the impact of varying lengths of EEG signals on classification performance and found that TF-CNN-CFA demonstrates consistent performance across different window lengths, indicating its strong generalization capability. This study validates the ability of EEG to differentiate higher cognitive states, which could potentially offer a novel BCI paradigm. © 2024 by the authors.},
	author_keywords = {brain–computer interface; channel and frequency attention; EEG signals; subject-driven cognitive states; time–frequency map},
	keywords = {accuracy; adult; algorithm; Article; artifact; artificial neural network; attention; clinical article; cognition; continuous wavelet transform; controlled study; convolutional neural network; deep learning; diagnostic test accuracy study; electroencephalogram; entropy; event related potential; eye movement; feasibility study; human; human experiment; independent component analysis; learning algorithm; machine learning; male; memory; muscle contraction; prediction; qualitative analysis; receiver operating characteristic; sigmoid; signal processing; temperature; training; true positive rate; visual field},
	correspondence_address = {H. Shen; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, 410073, China; email: shenhui

@ARTICLE{Liu2024Aacwgan,
	author = {Liu, Junjie and Xie, Jun and Tao, Qing and Zhang, Huanqing and Wang, Hu and Hu, Bo},
	title = {AAC-WGAN: A Novel Attention-Enhanced GAN Framework for SSVEP Augmentation and Classification},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {182627 - 182639},
	doi = {10.1109/ACCESS.2024.3509519},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211608727&doi=10.1109%2FACCESS.2024.3509519&partnerID=40&md5=a00ffebc7eb6c8369aedb25064dfc401},
	affiliations = {Xinjiang University, Urumqi, Xinjiang, China; Xi'an Jiaotong University, School of Mechanical Engineering, Xi'an, Shaanxi, China; State Key Laboratory for Manufacturing Systems Engineering, Xi'an, Shaanxi, China},
	abstract = {The rapid advancement of brain-computer interface (BCI) technology has created interest in steady-state visual evoked potential (SSVEP)-based BCIs, which are valued for their high information transfer rates and ability to manage multiple targets. Nonetheless, the efficacy of SSVEP decoding is often constrained by the volume and duration of user calibration data, limiting its practical application. Generative adversarial networks (GANs) have shown promise in synthesizing SSVEP electroencephalogram (EEG) data. However, they face challenges, such as low signal-to-noise ratio and capturing both temporal and spatial features. To address the need for high-quality data generation, the Attention-Aided Classifier Wasserstein GAN (AAC-WGAN) is proposed, which is a novel GAN model that combines an Attention Mechanism and an Auxiliary Classifier to improve data quality and classification performance. Our experiments on the Direction and Dial datasets reveal that our model performs obviously better, particularly with a training set ratio of 25% synthetic data and 75% real data. It achieves a classification accuracy of 91.65% on the Direction dataset, which is a significant improvement over the baseline accuracy of 84.32% (p = 0.008) and outperforms other comparable models (p < 0.05 for all comparisons). On the Dial dataset, our model achieves a classification accuracy of 83.48%, outperforming both the baseline of 80.86% and other models. Additional analyses, such as t-SNE visualization and FFT, confirm our model’s effectiveness in feature representation and frequency domain characteristics, with significant improvements in both data quality and classification stability. This work advances the state of SSVEP generation and classification, resulting in significant improvements in data quality and accuracy. It also represents a significant step forward in the development of BCI technologies. © 2024 The Authors.},
	author_keywords = {auxiliary classifier; Brain–computer interface (BCI); data augmentation; generative adversarial network (GAN); global attention mechanism; steady-state visual evoked potential (SSVEP)},
	keywords = {Brain; Brain computer interface; Electrophysiology; Adversarial networks; Attention mechanisms; Auxiliary classifier; Brain-computer interface; Data augmentation; Data quality; Generative adversarial network; Global attention mechanism; Steady-state visual evoked potential; Steady-state visual evoked potentials; Electroencephalography},
	correspondence_address = {J. Xie; School of Mechanical Engineering, Xinjiang University, Urumqi, 830049, China; email: xiejun

@ARTICLE{Rong2024Decoding,
	author = {Rong, Fenqi and Yang, Banghua and Guan, Cuntai},
	title = {Decoding Multi-Class Motor Imagery from Unilateral Limbs Using EEG Signals},
	year = {2024},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {32},
	pages = {3399 - 3409},
	doi = {10.1109/TNSRE.2024.3454088},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203541858&doi=10.1109%2FTNSRE.2024.3454088&partnerID=40&md5=59dc29ae6a052076719aa83f66659761},
	affiliations = {Shanghai University, School of Mechatronic Engineering and Automation, Shanghai, China; Shanghai University, Research Center of Brain Computer Engineering, Shanghai, China; Ministry of Education of the People's Republic of China, Engineering Research Center of Traditional Chinese Medicine Intelligent Rehabilitation, Beijing, Beijing, China; Nanyang Technological University, College of Computing and Data Science, Singapore City, Singapore},
	abstract = {The EEG is a widely utilized neural signal source, particularly in motor imagery-based brain-computer interface (MI-BCI), offering distinct advantages in applications like stroke rehabilitation. Current research predominantly concentrates on the bilateral limbs paradigm and decoding, but the use scenarios for stroke rehabilitation are typically for unilateral upper limbs. There is a significant challenge to decoding unilateral MI of multitasks due to the overlapped spatial neural activities of the tasks. This study aims to formulate a novel MI-BCI experimental paradigm for unilateral limbs with multitasks. The paradigm encompasses four imagined movement directions: top-bottom, left-right, top right-bottom left, and top left-bottom right. Forty-six healthy subjects participated in this experiment. Commonly used machine learning techniques, such as FBCSP, EEGNet, deepConvNet, and FBCNet, were employed for evaluation. To improve decoding accuracy, we propose an MVCA method that introduces temporal convolution and attention mechanism to effectively capture temporal features from multiple perspectives. With the MVCA model, we have achieved 40.6% and 64.89% classification accuracies for the four-class and two-class scenarios (top right-bottom left and top left-bottom right), respectively. Conclusion: This is the first study demonstrating that motor imagery of multiple directions in unilateral limbs can be decoded. In particular, decoding two directions, right top to left bottom and left top to right bottom, provides the best accuracy, which sheds light on future studies. This study advances the development of the MI-BCI paradigm, offering preliminary evidence for the feasibility of decoding multiple directional information from EEG. This, in turn, enhances the dimensions of MI control commands. © 2001-2011 IEEE.},
	author_keywords = {EEG; motor direction; motor imagery-based brain-computer interface (MI-BCI); multitasks; unilateral upper limbs},
	keywords = {Brain; Brain mapping; Breath controlled devices; Deep learning; Image enhancement; Image reconstruction; Neurons; Patient rehabilitation; EEG signals; Motor direction; Motor imagery; Motor imagery-based brain-computer interface; Multitask; Neural signals; Signal source; Stroke rehabilitation; Unilateral upper limb; Upper limbs; Brain computer interface; accuracy; adult; Article; artifact; cross validation; deep learning; electrocardiography; electroencephalogram; electroencephalography; electrooculography; extraction; feasibility study; female; handedness; human; human experiment; imagery; independent component analysis; learning algorithm; limb; low frequency noise; machine learning; male; multiclass motor imagery; normal human; questionnaire; signal noise ratio; stroke rehabilitation; topography; upper limb; algorithm; brain computer interface; imagination; movement (physiology); physiology; procedures; young adult; Adult; Algorithms; Brain-Computer Interfaces; Electroencephalography; Female; Healthy Volunteers; Humans; Imagination; Machine Learning; Male; Movement; Stroke Rehabilitation; Upper Extremity; Young Adult},
	correspondence_address = {B. Yang; Shanghai University, School of Mechatronic Engineering and Automation, Research Center of Brain-Computer Engineering, Shanghai, 200444, China; email: yangbanghua

@ARTICLE{Zhou2024Multiscale,
	author = {Zhou, Ben and Wang, Lei and Xu, Wenchang and Jiang, Chenyu},
	title = {Multi-Scale Convolutional Attention and Riemannian Geometry Network for EEG-Based Motor Imagery Classification},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {79731 - 79740},
	doi = {10.1109/ACCESS.2024.3410036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195389985&doi=10.1109%2FACCESS.2024.3410036&partnerID=40&md5=73f1c3e4a861e5f79632cd291af66c2c},
	affiliations = {Shandong University of Traditional Chinese Medicine, Jinan, Shandong, China; Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, Jiangsu, China},
	abstract = {The electroencephalogram (EEG) is a non-invasive technique with high temporal resolution that has become the research frontier of brain-computer interface (BCI) systems. It is widely used in medical rehabilitation, gaming, and other industries. However, decoding EEG signals remains a challenging task. A network called MSCARNet, which combines multi-scale convolution and Riemannian geometry, was proposed for classifying motor imagery based on EEG. The network is supplemented by an attention mechanism and sliding window technique. The MSCARNet utilizes sliding windows to expand data dimensions and multiple convolution kernels to obtain spatial and temporal features. These features are then mapped to Riemannian space and undergo bilinear mapping and logarithmic operations for dimensionality reduction. This approach is beneficial in reducing the impact of noise and outliers and provides convenience for classification. Subject-dependent and subject-independent experiments were conducted using the BCI-IV-2a dataset to validate the effectiveness of the MSCARNet. The results show that the accuracy improved by approximately 4% compared to existing state-of-the-art methods. The hybrid network based on Riemannian space can effectively improve the accuracy of EEG motor imagery classification tasks without excessive preprocessing. © 2013 IEEE.},
	author_keywords = {convolution neural network; deep learning; Electroencephalogram; motor imagery; Riemannian geometry},
	keywords = {Brain computer interface; Classification (of information); Convolution; Deep learning; Electrophysiology; Feature extraction; Geometry; Image classification; Image enhancement; Interfaces (computer); Learning algorithms; Brain modeling; Convolution neural network; Features extraction; Machine learning algorithms; Motor imagery; Motor imagery classification; Multi-scales; Riemannian geometry; Task analysis; Electroencephalography},
	correspondence_address = {C. Jiang; Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, Suzhou, 215163, China; email: jingcy

@ARTICLE{Chen2024Threebranch,
	author = {Chen, Weiming and Luo, Yiqing and Wang, Jie},
	title = {Three-Branch Temporal-Spatial Convolutional Transformer for Motor Imagery EEG Classification},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {79754 - 79764},
	doi = {10.1109/ACCESS.2024.3405652},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194882509&doi=10.1109%2FACCESS.2024.3405652&partnerID=40&md5=9ed9d0d8c45a0e6a7210ad199e679a17},
	affiliations = {Jilin University, College of Software, Changchun, Jilin, China},
	abstract = {In the classification of motor imagery Electroencephalogram (MI-EEG) signals through deep learning models, challenges such as the insufficiency of feature extraction due to the limited receptive field of single-scale convolutions, and overfitting due to small training sets, can hinder the perception of global dependencies in EEG signals. In this paper, we introduce a network called EEG TBTSCTnet, which represents Three-Branch Temporal-Spatial Convolutional Transformer. This approach expands the size of the training set through Data Augmentation, and then combines local and global features for classification. Specifically, Data Augmentation aims to mitigate the overfitting issue, whereas the Three-Branch Temporal-Spatial Convolution module captures a broader range of multi-scale, low-level local information in EEG signals more effectively than conventional CNNs. The Transformer Encoder module is directly connected to extract global correlations within local temporal-spatial features, utilizing the multi-head attention mechanism to effectively enhance the network's ability to represent relevant EEG signal features. Subsequently, a classifier module based on fully connected layers is used to predict the categories of EEG signals. Finally, extensive experiments were conducted on two public MI-EEG datasets to evaluate the proposed method. The study also allowed for an optimal selection of channels to balance accuracy and cost through weight visualization. © 2013 IEEE.},
	author_keywords = {data augmentation; EEG classification; motor imagery; temporal-spatial convolutional network; transformer},
	keywords = {Biomedical signal processing; Classification (of information); Convolution; Deep learning; Electrophysiology; Extraction; Feature extraction; Image classification; Neural networks; Brain modeling; Convolutional networks; Convolutional neural network; Data augmentation; EEG classification; Features extraction; Motor imagery; Temporal-spatial convolutional network; Transformer; Electroencephalography},
	correspondence_address = {W. Chen; Jilin University, College of Software, Changchun, 130012, China; email: chenwm02

@ARTICLE{Ding2024Novel,
	author = {Ding, Wenlong and Liu, Aiping and Guan, Ling and Chen, Xun},
	title = {A Novel Data Augmentation Approach Using Mask Encoding for Deep Learning-Based Asynchronous SSVEP-BCI},
	year = {2024},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {32},
	pages = {875 - 886},
	doi = {10.1109/TNSRE.2024.3366930},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185623760&doi=10.1109%2FTNSRE.2024.3366930&partnerID=40&md5=22ebb7dca252466552b2d9d2215b6ab5},
	affiliations = {University of Science and Technology of China, Department of Electronic Engineering and Information Science, Hefei, Anhui, China; Beijing Tiantan Hospital, Capital Medical University, National Center for Neurological Disorders, Beijing, China; The University of British Columbia, Department of Medicine, Vancouver, BC, Canada},
	abstract = {Deep learning (DL)-based methods have been successfully employed as asynchronous classification algorithms in the steady-state visual evoked potential (SSVEP)-based brain-computer interface (BCI) system. However, these methods often suffer from the limited amount of electroencephalography (EEG) data, leading to overfitting. This study proposes an effective data augmentation approach called EEG mask encoding (EEG-ME) to mitigate overfitting. EEG-ME forces models to learn more robust features by masking partial EEG data, leading to enhanced generalization capabilities of models. Three different network architectures, including an architecture integrating convolutional neural networks (CNN) with Transformer (CNN-Former), time domain-based CNN (tCNN), and a lightweight architecture (EEGNet) are utilized to validate the effectiveness of EEG-ME on publicly available benchmark and BETA datasets. The results demonstrate that EEG-ME significantly enhances the average classification accuracy of various DL-based methods with different data lengths of time windows on two public datasets. Specifically, CNN-Former, tCNN, and EEGNet achieve respective improvements of 3.18%, 1.42%, and 3.06% on the benchmark dataset as well as 11.09%, 3.12%, and 2.81% on the BETA dataset, with the 1-second time window as an example. The enhanced performance of SSVEP classification with EEG-ME promotes the implementation of the asynchronous SSVEP-BCI system, leading to improved robustness and flexibility in human-machine interaction. © 2001-2011 IEEE.},
	author_keywords = {Asynchronous brain-computer interface; data augmentation; deep learning; electroencephalography mask encoding; steady-state visual evoked potential},
	keywords = {Biomedical signal processing; Classification (of information); Data visualization; Deep learning; Electroencephalography; Electrophysiology; Encoding (symbols); Filter banks; Interface states; Interfaces (computer); Network architecture; Neural networks; Signal encoding; Time domain analysis; Asynchronoi brain-computer interface; Benchmark testing; Brain modeling; Data augmentation; Electroencephalography mask encoding; Encodings; Filters bank; Steady-state visual evoked potentials; Brain computer interface; Article; augmentation index; convolution algorithm; convolutional neural network; deep learning; electroencephalography; human; algorithm; artificial neural network; procedures; visual evoked potential; Algorithms; Brain-Computer Interfaces; Deep Learning; Evoked Potentials, Visual; Humans; Neural Networks, Computer},
	correspondence_address = {X. Chen; University of Science and Technology of China, Department of Electronic Engineering and Information Science, Hefei, 230027, China; email: xunchen

@ARTICLE{Xie2024Bidirectional,
	author = {Xie, Xinghe and Chen, Liyan and Qin, Shujia and Zha, Fusheng and Fan, Xinggang},
	title = {Bidirectional feature pyramid attention-based temporal convolutional network model for motor imagery electroencephalogram classification},
	year = {2024},
	journal = {Frontiers in Neurorobotics},
	volume = {18},
	pages = {},
	doi = {10.3389/fnbot.2024.1343249},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184692878&doi=10.3389%2Ffnbot.2024.1343249&partnerID=40&md5=bbb62b924598aee79327bb0a1322bf2c},
	affiliations = {Shenzhen Academy of Robotics, Shenzhen, Guangdong, China; Macao Polytechnic University, Faculty of Applied Sciences, Macau, Macao; Harbin Institute of Technology, Harbin, Heilongjiang, China; Zhejiang University of Technology, College of Information Engineering, Hangzhou, Zhejiang, China},
	abstract = {Introduction: As an interactive method gaining popularity, brain-computer interfaces (BCIs) aim to facilitate communication between the brain and external devices. Among the various research topics in BCIs, the classification of motor imagery using electroencephalography (EEG) signals has the potential to greatly improve the quality of life for people with disabilities. Methods: This technology assists them in controlling computers or other devices like prosthetic limbs, wheelchairs, and drones. However, the current performance of EEG signal decoding is not sufficient for real-world applications based on Motor Imagery EEG (MI-EEG). To address this issue, this study proposes an attention-based bidirectional feature pyramid temporal convolutional network model for the classification task of MI-EEG. The model incorporates a multi-head self-attention mechanism to weigh significant features in the MI-EEG signals. It also utilizes a temporal convolution network (TCN) to separate high-level temporal features. The signals are enhanced using the sliding-window technique, and channel and time-domain information of the MI-EEG signals is extracted through convolution. Results: Additionally, a bidirectional feature pyramid structure is employed to implement attention mechanisms across different scales and multiple frequency bands of the MI-EEG signals. The performance of our model is evaluated on the BCI Competition IV-2a dataset and the BCI Competition IV-2b dataset, and the results showed that our model outperformed the state-of-the-art baseline model, with an accuracy of 87.5 and 86.3% for the subject-dependent, respectively. Discussion: In conclusion, the BFATCNet model offers a novel approach for EEG-based motor imagery classification in BCIs, effectively capturing relevant features through attention mechanisms and temporal convolutional networks. Its superior performance on the BCI Competition IV-2a and IV-2b datasets highlights its potential for real-world applications. However, its performance on other datasets may vary, necessitating further research on data augmentation techniques and integration with multiple modalities to enhance interpretability and generalization. Additionally, reducing computational complexity for real-time applications is an important area for future work. © © 2024 Xie, Chen, Qin, Zha and Fan.},
	author_keywords = {deep learning; electroencephalogram; motion imagery; multihead attention; temporal convolutional networks},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Convolution; Deep learning; Electrophysiology; Image classification; Image enhancement; Motion analysis; Prosthetics; Time domain analysis; Attention mechanisms; Convolutional networks; Feature pyramid; Motion imagery; Motor imagery; Motor imagery EEG; Multihead; Multihead attention; Temporal convolutional network; Electroencephalography; ablation therapy; accuracy; Article; attention; bidirectional feature pyramid attention; convolutional neural network; deep learning; drone; electroencephalogram; electroencephalography; functional magnetic resonance imaging; human; human experiment; image analysis; motion imagery; multihead attention; nerve cell network; network analysis; quality of life; spatial attention; temporal convolution network; temporal convolutional network model; training},
	correspondence_address = {S. Qin; Shenzhen Academy of Robotics, Shenzhen, Guangdong Province, China; email: qinshujia

@ARTICLE{Kim2024Toward,
	author = {Kim, Sung-jin and Lee, Dae-hyeok and Kwak, Heongyu and Lee, Seongwhan},
	title = {Toward Domain-Free Transformer for Generalized EEG Pre-Training},
	year = {2024},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {32},
	pages = {482 - 492},
	doi = {10.1109/TNSRE.2024.3355434},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182950583&doi=10.1109%2FTNSRE.2024.3355434&partnerID=40&md5=9a5542091c048a6a3d19b34aeec1b6b1},
	affiliations = {Korea University, Department of Artificial Intelligence, Seoul, South Korea; Korea University, Department of Brain and Cognitive Engineering, Seoul, South Korea},
	abstract = {Electroencephalography (EEG) signals are the brain signals acquired using the non-invasive approach. Owing to the high portability and practicality, EEG signals have found extensive application in monitoring human physiological states across various domains. In recent years, deep learning methodologies have been explored to decode the intricate information embedded in EEG signals. However, since EEG signals are acquired from humans, it has issues with acquiring enormous amounts of data for training the deep learning models. Therefore, previous research has attempted to develop pre-trained models that could show significant performance improvement through fine-tuning when data are scarce. Nonetheless, existing pre-trained models often struggle with constraints, such as the necessity to operate within datasets of identical configurations or the need to distort the original data to apply the pre-trained model. In this paper, we proposed the domain-free transformer, called DFformer, for generalizing the EEG pre-trained model. In addition, we presented the pre-trained model based on DFformer, which is capable of seamless integration across diverse datasets without necessitating architectural modification or data distortion. The proposed model achieved competitive performance across motor imagery and sleep stage classification datasets. Notably, even when fine-tuned on datasets distinct from the pre-training phase, DFformer demonstrated marked performance enhancements. Hence, we demonstrate the potential of DFformer to overcome the conventional limitations in pre-trained model development, offering robust applicability across a spectrum of domains. © 2001-2011 IEEE.},
	author_keywords = {autoencoder; Electroencephalogram; motor imagery; sleep stage classification; transformer},
	keywords = {Biomedical signal processing; Classification (of information); Deep learning; Electrophysiology; Image classification; Job analysis; Sleep research; Auto encoders; Brain modeling; Domain free; Motor imagery; Pre-training; Sleep; Sleep stages classifications; Task analysis; Transformer; Electroencephalography; Article; cardiovascular disease; convolutional neural network; cross validation; data accuracy; data analysis; data base; deep learning; Deep sleepnet; electroencephalography; electroencephalography phase synchronization; embedding; evaluation research; evaluation study; frequency modulation; heart; human; model; monitoring; polysomnography; Robust sleepnet; sine wave; sleep; sleep apnea syndromes; sleep heart health study; spatial analysis; training; U sleepnet; algorithm; brain; physiology; power supply; procedures; Algorithms; Brain; Brain-Computer Interfaces; Electric Power Supplies; Humans},
	correspondence_address = {S.-W. Lee; Korea University, Seongbuk-ku, Department of Artificial Intelligence, Seoul, 02841, South Korea; email: sw.lee

@ARTICLE{Liu2024Imhnet,
	author = {Liu, Menghao and Li, Tingting and Zhang, Xu and Yang, Yang and Zhou, Zhiyong and Fu, Tianhao},
	title = {IMH-Net: a convolutional neural network for end-to-end EEG motor imagery classification},
	year = {2024},
	journal = {Computer Methods in Biomechanics and Biomedical Engineering},
	volume = {27},
	number = {15},
	pages = {2175 - 2188},
	doi = {10.1080/10255842.2023.2275244},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176007695&doi=10.1080%2F10255842.2023.2275244&partnerID=40&md5=c4691cf2212deb0f3d73477723c1436a},
	affiliations = {Shanghai Dianji University, College of Mechanical Engineering, Shanghai, China; Shanghai Chest Hospital, Department of Anesthesiology, Shanghai, China; Ltd., Shanghai, Shanghai, China},
	abstract = {As the main component of Brain-computer interface (BCI) technology, the classification algorithm based on EEG has developed rapidly. The previous algorithms were often based on subject-dependent settings, resulting in BCI needing to be calibrated for new users. In this work, we propose IMH-Net, an end-to-end subject-independent model. The model first uses Inception blocks extracts the frequency domain features of the data, then further compresses the feature vectors to extract the spatial domain features, and finally learns the global information and classification through Multi-Head Attention mechanism. On the OpenBMI dataset, IMH-Net obtained 73.90 ± 13.10% accuracy and 73.09 ± 14.99% F1-score in subject-independent manner, which improved the accuracy by 1.96% compared with the comparison model. On the BCI competition IV dataset 2a, this model also achieved the highest accuracy and F1-score in subject-dependent manner. The IMH-Net model we proposed can improve the accuracy of subject-independent Motor Imagery (MI), and the robustness of the algorithm is high, which has strong practical value in the field of BCI. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
	author_keywords = {brain-computer interface (BCI); Deep learning (DL); end-to-end; motor imagery (MI); subject-independent},
	keywords = {Brain computer interface; Classification (of information); Convolutional neural networks; Deep learning; Image classification; Image enhancement; Brain-computer interface; Convolutional neural network; Domain feature; End to end; F1 scores; Motor imagery; Motor imagery classification; Subject-independent; Frequency domain analysis; accuracy; algorithm; Article; attention; convolutional neural network; cross validation; deep learning; electroencephalogram; electroencephalography; electrooculography; event related potential; evoked response; human; human experiment; imagery; signal noise ratio; artificial neural network; brain computer interface; imagination; physiology; procedures; signal processing; Algorithms; Brain-Computer Interfaces; Electroencephalography; Humans; Imagination; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	correspondence_address = {Z. Zhou; Mechanical College, Shanghai Dianji University, Shanghai, China; email: zhouzhiyong789

@ARTICLE{Chen2024Attentionbased,
	author = {Chen, Jiannan and Sun, Fuchun and Zhang, Wenjun and Zhang, Shubin and Liu, Kai and Qi, Chunpeng},
	title = {Attention-Based Multimodal tCNN for Classification of Steady-State Visual Evoked Potentials and Its Application to Gripper Control},
	year = {2024},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	volume = {35},
	number = {12},
	pages = {18263 - 18271},
	doi = {10.1109/TNNLS.2023.3313691},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173298479&doi=10.1109%2FTNNLS.2023.3313691&partnerID=40&md5=158076d6338c53a6a0738e1449f7c5f4},
	affiliations = {Yanshan University, Qinhuangdao, Hebei, China; Tsinghua University, Beijing, China; Tsinghua University, State Key Laboratory of Intelligent Technology and Systems, Beijing, China; China Agricultural University, Key Laboratory of Smart Farming Technologies for Aquatic Animal and Livestock, Beijing, China},
	abstract = {The classification problem for short time-window steady-state visual evoked potentials (SSVEPs) is important in practical applications because shorter time-window often means faster response speed. By combining the advantages of the local feature learning ability of convolutional neural network (CNN) and the feature importance distinguishing ability of attention mechanism, a novel network called AttentCNN is proposed to further improve the classification performance for short time-window SSVEP. Considering the frequency-domain features extracted from short time-window signals are not obvious, this network starts with the time-domain feature extraction module based on the filter bank (FB). The FB consists of four sixth-order Butterworth filters with different bandpass ranges. Then extracted multimodal features are aggregated together. The second major module is a set of residual squeeze and excitation blocks (RSEs) that has the ability to improve the quality of extracted features by learning the interdependence between features. The final major module is time-domain CNN (tCNN) that consists of four CNNs for further feature extraction and followed by a fully connected (FC) layer for output. Our designed networks are validated over two large public datasets, and necessary comparisons are given to verify the effectiveness and superiority of the proposed network. In the end, in order to demonstrate the application potential of the proposed strategy in the medical rehabilitation field, we design a novel five-finger bionic hand and connect it to our trained network to achieve the control of bionic hand by human brain signals directly. Our source codes are available on Github: https://github.com/JiannanChen/AggtCNN.git. © 2023 IEEE.},
	author_keywords = {Bionic hand; convolutional neural network (CNN); multimodal fusion; residual squeeze and excitation block (RSE); steady-state visual evoked potentials (SSVEPs)},
	keywords = {Biomedical signal processing; Bionics; Butterworth filters; Convolution; Electroencephalography; Electrophysiology; Extraction; Fast Fourier transforms; Filter banks; Frequency domain analysis; Large dataset; Neural networks; Time domain analysis; Bionic hand; Convolutional neural network; Features extraction; Filters bank; Kernel; Multi-modal fusion; Residual squeeze and excitation block; Steady-state visual evoked potential; Steady-state visual evoked potentials; Time-domain analysis; Feature extraction; adult; algorithm; artificial neural network; attention; brain computer interface; classification; electroencephalography; female; hand strength; human; male; physiology; procedures; signal processing; visual evoked potential; Adult; Algorithms; Attention; Brain-Computer Interfaces; Evoked Potentials, Visual; Female; Hand Strength; Humans; Male; Neural Networks, Computer; Signal Processing, Computer-Assisted},
	correspondence_address = {J. Chen; Yanshan University, School of Electrical Engineering, Qinhuangdao, 066000, China; email: cocoachen1992

@ARTICLE{Hameed2024Temporalspatial,
	author = {Hameed, Adel and Fourati, Rahma and Ammar, B. and Ksibi, Amel and Saleh Alluhaidan, Ala Saleh D. and Ayed, Mounir Ben and Khleaf, Hussain Kareem},
	title = {Temporal–spatial transformer based motor imagery classification for BCI using independent component analysis},
	year = {2024},
	journal = {Biomedical Signal Processing and Control},
	volume = {87},
	pages = {},
	doi = {10.1016/j.bspc.2023.105359},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169604789&doi=10.1016%2Fj.bspc.2023.105359&partnerID=40&md5=b4317311ffe3e845897a169e002ef15e},
	affiliations = {Ecole Nationale d'Ingénieurs de Sfax, Research Groups in Intelligent Machines, Sfax, Sfax, Tunisia; ENET'Com-National School of Electronics and Telecommunications of Sfax, Sfax, Sfax, Tunisia; Faculté des Sciences Juridiques, Economiques et de Gestion de Jendouba, Faculty of Law, Jendouba, Jendouba, Tunisia; Princess Nourah Bint Abdulrahman University, Department of Information Systems, Riyadh, Riyad, Saudi Arabia; University of Sfax, Computer Sciences and Communication Department, Sfax, Sfax, Tunisia; University of Technology- Iraq, Department of Electrical Engineering, Baghdad, Baghdad, Iraq},
	abstract = {Motor Imagery (MI) classification with electroencephalography (EEG) is a critical aspect of Brain–Computer Interface (BCI) systems, enabling individuals with mobility limitations to communicate with the outside world. However, the complexity, variability, and low signal-to-noise ratio of EEG data present significant challenges in decoding these signals, particularly in a subject-independent manner. To overcome these challenges, we propose a transformer-based approach that employs a self-attention process to extract features in the temporal and spatial domains. To establish spatial correlations across MI EEG channels, the self-attention module periodically updates each channel by averaging its features across all channels. This weighted averaging improves classification accuracy and removes artifacts generated by manually selecting channels. Furthermore, the temporal self-attention mechanism encodes global sequential information into the features for each sample time step, allowing for the extraction of superior temporal properties in the time domain from MI EEG data. The effectiveness of the proposed strategy has been confirmed through testing against the BCI Competition IV 2a and 2b benchmarks. Overall, our proposed model outperforms state-of-the-art methods and demonstrates greater stability in both subject-dependent and subject-independent strategies. © 2023 Elsevier Ltd},
	author_keywords = {Brain–computer interface; Electroencephalography; Motor imagery; Self-attention; Spatio-temporal; Transformer},
	keywords = {Biomedical signal processing; Electroencephalography; Electrophysiology; Image classification; Independent component analysis; Signal to noise ratio; Time domain analysis; Independent components analysis; Interface system; Low signal-to-noise ratio; Motor imagery; Motor imagery classification; Self-attention; Spatio-temporal; Temporal and spatial; Temporal domain; Transformer; Brain computer interface; Article; artificial neural network; calibration; classification error; classifier; controlled study; deep learning; diagnostic accuracy; disease classification; electroencephalography; electrooculogram; feature extraction; functional connectivity; human; imagery; independent component analysis; performance indicator; signal noise ratio; spatiotemporal analysis; surface property; visual feedback; walking difficulty},
	correspondence_address = {A. Ksibi; Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia; email: amelksibi

@ARTICLE{Zhu2023Graph,
	author = {Zhu, Jun and Liu, Qingshan and Xu, Chentao},
	title = {Graph Convolutional Neural Network with Multi-Scale Attention Mechanism for EEG-Based Motion Imagery Classification},
	year = {2023},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	volume = {37},
	number = {14},
	pages = {},
	doi = {10.1142/S0218001423540204},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180507393&doi=10.1142%2FS0218001423540204&partnerID=40&md5=0753a86871414a181661605eca8c733f},
	affiliations = {Southeast University, School of Mathematics, Nanjing, Jiangsu, China; Southeast University, School of Cyber Science and Engineering, Nanjing, Jiangsu, China},
	abstract = {Recently, deep learning has been widely used in the classification of EEG signals and achieved satisfactory results. However, the correlation between EEG electrodes is rarely considered, which has been proved that there are indeed connections between different brain regions. After considering the connections between EEG electrodes, the graph convolutional neural network is applied to detect human motor intents from EEG signals, where EEG data are transformed into graph data through phase lag index, time-domain and frequency-domain features with different signal bands. Meanwhile, a multi-scale attention mechanism is proposed to the network to improve the accuracy of classification. By using the multi-scale attention-based graph convolutional neural network, the accuracy of 93.22% is achieved with 10-fold cross-validation, which is higher than the compared methods which ignore the spatial correlations of EEG signals. © 2023 World Scientific Publishing Company.},
	author_keywords = {classification; EEG; Graph convolutional neural network; multi-scale attention mechanism},
	keywords = {Brain; Convolution; Convolutional neural networks; Deep learning; Electrodes; Image classification; Attention mechanisms; Brain regions; Convolutional neural network; EEG signals; Graph convolutional neural network; Graph data; Human motor; Motion imagery; Multi-scale attention mechanism; Multi-scales; Frequency domain analysis},
	correspondence_address = {Q. Liu; School of Mathematics, Southeast University, Nanjing, 210096, China; email: qsliu

@ARTICLE{Altaheri2023Dynamic,
	author = {Altaheri, Hamdi and Muhammad, Ghulam and Alsulaiman, Mansour M.},
	title = {Dynamic Convolution With Multilevel Attention for EEG-Based Motor Imagery Decoding},
	year = {2023},
	journal = {IEEE Internet of Things Journal},
	volume = {10},
	number = {21},
	pages = {18579 - 18588},
	doi = {10.1109/JIOT.2023.3281911},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161490241&doi=10.1109%2FJIOT.2023.3281911&partnerID=40&md5=e6600406180d19942fd9534669419e64},
	affiliations = {King Saud University, Department of Computer Engineering, Riyadh, Riyad, Saudi Arabia},
	abstract = {Brain-computer interface (BCI) is an innovative technology that utilizes artificial intelligence (AI) and wearable electroencephalography (EEG) sensors to decode brain signals and enhance the quality of life. EEG-based motor imagery (MI) brain signal is used in many BCI applications, including smart healthcare, smart homes, and robotics control. However, the restricted ability to decode brain signals is a major factor preventing BCI technology from expanding significantly. In this study, we introduce a dynamic attention temporal convolutional network (D-ATCNet) for decoding EEG-based MI signals. The D-ATCNet model uses dynamic convolution (Dy-conv) and multilevel attention to enhance the performance of MI classification with a relatively small number of parameters. D-ATCNet has two main blocks: 1) dynamic and 2) temporal convolution. Dy-conv uses multilevel attention to encode low-level MI-EEG information and temporal convolution uses shifted window with self-attention to extract high-level temporal information from the encoded signal. The proposed model performs better than the existing methods with an accuracy of 71.3% for subject independent and 87.08% for subject dependent using the BCI competition IV-2a data set. © 2014 IEEE.},
	author_keywords = {Attention mechanism; classification; convolutional neural network (CNN); dynamic convolution (Dy-conv); electroencephalography (EEG); motor imagery (MI); multihead self-attention (MSA); smart healthcare},
	keywords = {Automation; Biomedical signal processing; Brain computer interface; Classification (of information); Decoding; Electroencephalography; Electrophysiology; Image classification; Image enhancement; Neural networks; Attention mechanisms; Brain modeling; Convolutional neural network; Dynamic convolution; Features extraction; Kernel; Motor imagery; Multi-head self-attention; Smart healthcare; Convolution},
	correspondence_address = {G. Muhammad; King Saud University, College of Computer and Information Sciences, Department of Computer Engineering, Riyadh, 11543, Saudi Arabia; email: ghulam

@ARTICLE{Ma2023Mbganet,
	author = {Ma, Weifeng and Wang, Chuanlai and Sun, Xiaoyong and Lin, Xuefen and Niu, Lei and Wang, Yuchen},
	title = {MBGA-Net: A multi-branch graph adaptive network for individualized motor imagery EEG classification},
	year = {2023},
	journal = {Computer Methods and Programs in Biomedicine},
	volume = {240},
	pages = {},
	doi = {10.1016/j.cmpb.2023.107641},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162904681&doi=10.1016%2Fj.cmpb.2023.107641&partnerID=40&md5=bdd14bd1a35a1f53c70ffedfa9d56422},
	affiliations = {Zhejiang University of Science and Technology, School of Electronic and Information Engineering, Hangzhou, Zhejiang, China; Central China Normal University Wollongong Joint Institute, Faculty of Artificial Intelligence in Education, Wuhan, Hubei, China},
	abstract = {Background and objective: The development of deep learning has led to significant improvements in the decoding accuracy of Motor Imagery (MI) EEG signal classification. However, current models are inadequate in ensuring high levels of classification accuracy for an individual. Since MI EEG data is primarily used in medical rehabilitation and intelligent control, it is crucial to ensure that each individual's EEG signal is recognized with precision. Methods: We propose a multi-branch graph adaptive network (MBGA-Net), which matches each individual EEG signal with a suitable time-frequency domain processing method based on spatio-temporal domain features. We then feed the signal into the relevant model branch using an adaptive technique. Through an enhanced attention mechanism and deep convolutional method with residual connectivity, each model branch more effectively harvests the features of the related format data. Results: We validate the proposed model using the BCI Competition IV dataset 2a and dataset 2b. On dataset 2a, the average accuracy and kappa values are 87.49% and 0.83, respectively. The standard deviation of individual kappa values is only 0.08. For dataset 2b, the average classification accuracies obtained by feeding the data into the three branches of MBGA-Net are 85.71%, 85.83%, and 86.99%, respectively. Conclusions: The experimental results demonstrate that MBGA-Net could effectively perform the classification task of motor imagery EEG signals, and it exhibits strong generalization performance. The proposed adaptive matching technique enhances the classification accuracy of each individual, which is beneficial for the practical application of EEG classification. © 2023 Elsevier B.V.},
	author_keywords = {Adaptive matching technique; Data processing; Deep learning; Graph convolutional network(GCN); Motor imagery; Multi-branch graph adaptive network},
	keywords = {Biomedical signal processing; Classification (of information); Convolution; Convolutional neural networks; Data handling; Deep learning; Image classification; Image enhancement; Adaptive matching; Adaptive matching technique; Adaptive networks; Convolutional networks; Graph convolutional network; Matching techniques; Motor imagery; Motor imagery EEG; Multi-branch graph adaptive network; Frequency domain analysis; article; attention; competition; deep learning; electroencephalogram; feeding; human; human experiment; imagery; algorithm; electroencephalography; imagination; movement (physiology); procedures; Algorithms; Brain-Computer Interfaces; Electroencephalography; Imagination; Movement},
	correspondence_address = {Y. Wang; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, 310023, China; email: yuchen

@ARTICLE{Luo2023Shallow,
	author = {Luo, Jing and Wang, Yaojie and Xia, Shuxiang and Lü, Na and Ren, Xiaoyong and Shi, Zhenghao and Hei, Xinghong},
	title = {A shallow mirror transformer for subject-independent motor imagery BCI},
	year = {2023},
	journal = {Computers in Biology and Medicine},
	volume = {164},
	pages = {},
	doi = {10.1016/j.compbiomed.2023.107254},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165661719&doi=10.1016%2Fj.compbiomed.2023.107254&partnerID=40&md5=b26804180fd1b4ea4f937aa5db35cd1c},
	affiliations = {Xi'an University of Technology, School of Computer Science and Engineering, Xi'an, Shaanxi, China; State Key Laboratory for Manufacturing Systems Engineering, Systems Engineering Institute, Xi'an, Shaanxi, China; The Second Hospital of Xian Jiaotong University, Department of Otolaryngology-Head and Neck Surgery, Xi'an, Shaanxi, China},
	abstract = {Objective: Motor imagery BCI plays an increasingly important role in motor disorders rehabilitation. However, the position and duration of the discriminative segment in an EEG trial vary from subject to subject and even trial to trial, and this leads to poor performance of subject-independent motor imagery classification. Thus, determining how to detect and utilize the discriminative signal segments is crucial for improving the performance of subject-independent motor imagery BCI. Approach: In this paper, a shallow mirror transformer is proposed for subject-independent motor imagery EEG classification. Specifically, a multihead self-attention layer with a global receptive field is employed to detect and utilize the discriminative segment from the entire input EEG trial. Furthermore, the mirror EEG signal and the mirror network structure are constructed to improve the classification precision based on ensemble learning. Finally, the subject-independent setup was used to evaluate the shallow mirror transformer on motor imagery EEG signals from subjects existing in the training set and new subjects. Main results: The experiments results on BCI Competition IV datasets 2a and 2b and the OpenBMI dataset demonstrated the promising effectiveness of the proposed shallow mirror transformer. The shallow mirror transformer obtained average accuracies of 74.48% and 76.1% for new subjects and existing subjects, respectively, which were highest among the compared state-of-the-art methods. In addition, visualization of the attention score showed the ability of discriminative EEG segment detection. This paper demonstrated that multihead self-attention is effective in capturing global EEG signal information in motor imagery classification. Significance: This study provides an effective model based on a multihead self-attention layer for subject-independent motor imagery-based BCIs. To the best of our knowledge, this is the shallowest transformer model available, in which a small number of parameters promotes the performance in motor imagery EEG classification for such a small sample problem. © 2023 Elsevier Ltd},
	author_keywords = {Brain–computer interfaces (BCI); Motor imagery (MI); Multihead self-attention; Subject-independent BCI},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Image classification; Image enhancement; Brain–computer interface; EEG signals; Motor imagery; Motor imagery classification; Motor imagery EEG; Multihead; Multihead self-attention; Performance; Subject-independent brain–computer interface; Mirrors; Article; classification; comparative study; controlled study; convolutional neural network; electroencephalography; evoked response; feature extraction; female; human; imagery; learning; male; measurement accuracy; motor performance; normal human; signal detection; task performance; visual evoked potential; algorithm; imagination; procedures; Algorithms; Brain-Computer Interfaces; Electroencephalography; Humans; Imagination; Learning},
	correspondence_address = {J. Luo; Shaanxi Key Laboratory for Network Computing and Security Technology and Human–Machine Integration Intelligent Robot Shaanxi University Engineering Research Center, School of Computer Science and Engineering, Xi'an University of Technology, Xi'an, Shaanxi, China; email: luojing

@ARTICLE{Zeynali2023Classification,
	author = {Zeynali, Mahsa and Seyedarabi, Hadi and Afrouzian, Reza},
	title = {Classification of EEG signals using Transformer based deep learning and ensemble models},
	year = {2023},
	journal = {Biomedical Signal Processing and Control},
	volume = {86},
	pages = {},
	doi = {10.1016/j.bspc.2023.105130},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162165920&doi=10.1016%2Fj.bspc.2023.105130&partnerID=40&md5=79947c878a2f15cae5a496876d38c49b},
	affiliations = {University of Tabriz, Faculty of Electrical and Computer Engineering, Tabriz, East Azerbaijan, Iran; University of Tabriz, Faculty of Engineering, Tabriz, East Azerbaijan, Iran},
	abstract = {A Brain-Computer Interface (BCI) is a communication and control system designed to provide interaction between a user and a computer device. This interaction is based on the brain's electrical signals that are generated when users do specific tasks. Different categories of visual stimuli evoke distinct activation patterns in the human brain. The generated patterns can be recorded with EEG signals for use in BCI applications. Recently, deep learning-based Transformer models have demonstrated significant potential for analyzing diverse data. In this paper, a new Transformer-based model has been presented that extracts temporal and spectral features from EEG signals for classification purposes. The proposed Spectral Transformer model converts the EEG signal to the frequency domain using PSD before applying Transformer models to extract frequency features. Deep ensemble learning models are used to enhance the generalization performance of the final model by combining the benefits of both deep learning models and ensemble learning. The proposed ensemble model combines Temporal and Spectral Transformers to simultaneously utilize the time and frequency features of the signal. The accuracy of 96.1 %, 94.20 %, and 93.60 % are achieved using an ensemble model, Temporal Transformer, and Spectral Transformer, respectively. These results demonstrate the effectiveness of the proposed model for accurately classifying EEG signals in BCI applications. © 2023 Elsevier Ltd},
	author_keywords = {Brain-Computer Interface (BCI); Electroencephalography (EEG); Ensemble learning; Transformer},
	keywords = {Biomedical signal processing; Classification (of information); Computer control systems; Deep learning; Electroencephalography; Electrophysiology; Frequency domain analysis; Learning systems; Brain-computer interface; Brain-computer interface applications; Communication and control; Ensemble learning; Ensemble models; Frequency features; Learning models; Transformer; Transformer modeling; Brain computer interface; Article; deep learning; electroencephalogram; model; signal processing; transformer model},
	correspondence_address = {H. Seyedarabi; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; email: seyedarabi

@ARTICLE{Wang2023Multimodal,
	author = {Wang, Kangning and Qiu, Shuang and Wei, Wei and Zhang, Yukun and Wang, Shengpei and He, Huiguang and Xu, Minpeng and Jung, Tzyy Ping and Ming, Dong},
	title = {A multimodal approach to estimating vigilance in SSVEP-based BCI},
	year = {2023},
	journal = {Expert Systems with Applications},
	volume = {225},
	pages = {},
	doi = {10.1016/j.eswa.2023.120177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153572159&doi=10.1016%2Fj.eswa.2023.120177&partnerID=40&md5=00f63ada3c788e7dc777b61082d2fab8},
	affiliations = {Tianjin University, Academy of Medical Engineering and Translational Medicine, Tianjin, China; Institute of Automation Chinese Academy of Sciences, Beijing, Beijing, China; University of Chinese Academy of Sciences, School of Artificial Intelligence, Beijing, China; Tianjin University, Tianjin, China; University of California, San Diego, Swartz Center for Computational Neuroscience, La Jolla, CA, United States},
	abstract = {Brain-computer interface (BCI) is a communication system that allows a direct connection between the human brain and external devices, which is able to provide assistance and improve the quality of life for people with disabilities. Vigilance is an important cognitive state and plays an important role in human–computer interaction. In BCI tasks, the low-vigilance state of the BCI user would lead to the performance degradation. Therefore, it is desirable to develop an efficient method to estimate the vigilance state of BCI users. In this study, we built a 4-target BCI system based on steady-state visual evoked potential (SSVEP) for cursor control. Electroencephalogram (EEG) and electrooculogram (EOG) were recorded simultaneously from 18 subjects during a 90-min continuous cursor-control BCI task. We proposed a multimodal vigilance estimating network, named MVENet, to estimate the vigilance state of BCI users through the multimodal signals. In this architecture, a spatial-temporal convolution module with an attention mechanism was adopted to explore the temporal-spatial information of the EEG features, and a long short-term memory module was utilized to learn the temporal dependencies of EOG features. Moreover, a fusion mechanism was built to fuse the EEG representations and EOG representations effectively. Experimental results showed that the proposed network achieved a better performance than the compared methods. These results demonstrate the feasibility and effectiveness of our methods for estimating the vigilance state of BCI users. © 2023 Elsevier Ltd},
	author_keywords = {Brain-computer interface (BCI); Electroencephalogram (EEG); Graph neural network; Multimodal fusion; Steady-state visual evoked potential (SSVEP); Vigilance estimation},
	keywords = {Brain computer interface; Electroencephalography; Brain-computer interface; Cursor control; Electro-oculogram; Electroencephalogram; Graph neural networks; Multi-modal; Multi-modal fusion; Steady-state visual evoked potential; Steady-state visual evoked potentials; Vigilance estimation; Interface states},
	correspondence_address = {S. Qiu; Laboratory of Brain Atlas and Brain-Inspired Intelligence, State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; email: shuang.qiu

@ARTICLE{Zhang2023Micat,
	author = {Zhang, Dongxue and Li, Huiying and Xie, Jingmeng},
	title = {MI-CAT: A transformer-based domain adaptation network for motor imagery classification},
	year = {2023},
	journal = {Neural Networks},
	volume = {165},
	pages = {451 - 462},
	doi = {10.1016/j.neunet.2023.06.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162147840&doi=10.1016%2Fj.neunet.2023.06.005&partnerID=40&md5=68a04a4e2c57092886ead1c6eb8d5df6},
	affiliations = {Jilin University, College of Computer Science and Technology, Changchun, Jilin, China; Jilin University, Key Laboratory of Symbolic Computation and Knowledge Engineering, Changchun, Jilin, China; Xi'an Jiaotong University, College of Electronic and Information Engineering, Xi'an, Shaanxi, China},
	abstract = {Due to its convenience and safety, electroencephalography (EEG) data is one of the most widely used signals in motor imagery (MI) brain–computer interfaces (BCIs). In recent years, methods based on deep learning have been widely applied to the field of BCIs, and some studies have gradually tried to apply Transformer to EEG signal decoding due to its superior global information focusing ability. However, EEG signals vary from subject to subject. Based on Transformer, how to effectively use data from other subjects (source domain) to improve the classification performance of a single subject (target domain) remains a challenge. To fill this gap, we propose a novel architecture called MI-CAT. The architecture innovatively utilizes Transformer's self-attention and cross-attention mechanisms to interact features to resolve differential distribution between different domains. Specifically, we adopt a patch embedding layer for the extracted source and target features to divide the features into multiple patches. Then, we comprehensively focus on the intra-domain and inter-domain features by stacked multiple Cross-Transformer Blocks (CTBs), which can adaptively conduct bidirectional knowledge transfer and information exchange between domains. Furthermore, we also utilize two non-shared domain-based attention blocks to efficiently capture domain-dependent information, optimizing the features extracted from the source and target domains to assist in feature alignment. To evaluate our method, we conduct extensive experiments on two real public EEG datasets, Dataset IIb and Dataset IIa, achieving competitive performance with an average classification accuracy of 85.26% and 76.81%, respectively. Experimental results demonstrate that our method is a powerful model for decoding EEG signals and facilitates the development of the Transformer for brain–computer interfaces (BCIs). © 2023 Elsevier Ltd},
	author_keywords = {Brain–computer interfaces (BCIs); Domain adaptation; Electroencephalograph (EEG); Motor imagery (MI); Transformer},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Decoding; Deep learning; Electroencephalography; Image classification; Knowledge management; Network architecture; Brain–computer interface; Domain adaptation; Electroencephalograph; Electroencephalograph signals; Global informations; Motor imagery; Motor imagery classification; Target domain; Transformer; Electrophysiology; Article; artificial neural network; classification algorithm; electroencephalogram; feature extraction; human; imagery; measurement accuracy; motor imagery; algorithm; electroencephalography; imagination; procedures; Algorithms; Brain-Computer Interfaces; Imagination},
	correspondence_address = {H. Li; Jilin University, College of Computer Science and Technology, Changchun, Jilin Province, China; email: lihuiying

@ARTICLE{Chen2023Transformerbased,
	author = {Chen, Jianbo and Zhang, Yangsong and Pan, Yudong and Xu, Peng and Guan, Cuntai},
	title = {A transformer-based deep neural network model for SSVEP classification},
	year = {2023},
	journal = {Neural Networks},
	volume = {164},
	pages = {521 - 534},
	doi = {10.1016/j.neunet.2023.04.045},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159755865&doi=10.1016%2Fj.neunet.2023.04.045&partnerID=40&md5=70ced04b7f133bd5a8f729c29edf800f},
	affiliations = {Southwest University of Science and Technology, School of Computer Science and Technology, Mianyang, Sichuan, China; University of Electronic Science and Technology of China, MOE Key Lab for Neuroinformation, Chengdu, Sichuan, China; School of Computer Science and Engineering, Singapore City, Singapore},
	abstract = {Steady-state visual evoked potential (SSVEP) is one of the most commonly used control signals in the brain–computer interface (BCI) systems. However, the conventional spatial filtering methods for SSVEP classification highly depend on the subject-specific calibration data. The need for the methods that can alleviate the demand for the calibration data becomes urgent. In recent years, developing the methods that can work in inter-subject scenario has become a promising new direction. As a popular deep learning model nowadays, Transformer has been used in EEG signal classification tasks owing to its excellent performance. Therefore, in this study, we proposed a deep learning model for SSVEP classification based on Transformer architecture in inter-subject scenario, termed as SSVEPformer, which was the first application of Transformer on the SSVEP classification. Inspired by previous studies, we adopted the complex spectrum features of SSVEP data as the model input, which could enable the model to simultaneously explore the spectral and spatial information for classification. Furthermore, to fully utilize the harmonic information, an extended SSVEPformer based on the filter bank technology (FB-SSVEPformer) was proposed to improve the classification performance. Experiments were conducted using two open datasets (Dataset 1: 10 subjects, 12 targets; Dataset 2: 35 subjects, 40 targets). The experimental results show that the proposed models could achieve better results in terms of classification accuracy and information transfer rate than other baseline methods. The proposed models validate the feasibility of deep learning models based on Transformer architecture for SSVEP data classification, and could serve as potential models to alleviate the calibration procedure in the practical application of SSVEP-based BCI systems. © 2023 Elsevier Ltd},
	author_keywords = {Brain–computer interface; Deep learning; Filter bank; Steady-state visual evoked potential; Transformer},
	keywords = {Calibration; Classification (of information); Computer control systems; Deep neural networks; Filter banks; Interface states; Learning systems; Network architecture; Spectrum analysis; Calibration data; Control signal; Deep learning; Filters bank; Interface system; Learning models; Neural network model; Spatial filtering methods; Steady-state visual evoked potentials; Transformer; Brain computer interface; accuracy; Article; classification; convolutional neural network; deep learning; deep neural network; electroencephalogram; feasibility study; information processing; model; technology; validation process; visual evoked potential; algorithm; artificial neural network; electroencephalography; human; photostimulation; procedures; Algorithms; Brain-Computer Interfaces; Electroencephalography; Evoked Potentials, Visual; Humans; Neural Networks, Computer; Photic Stimulation},
	correspondence_address = {Y. Zhang; School of Computer Science and Technology, Laboratory for Brain Science and Medical Artificial Intelligence, Southwest University of Science and Technology, Mianyang, China; email: zhangysacademy

@ARTICLE{Zhu2023Imageevoked,
	author = {Zhu, Mu and Jin, Haonan and Bai, Zhongli and Li, Zhiwei and Song, Yu},
	title = {Image-Evoked Emotion Recognition for Hearing-Impaired Subjects with EEG Signals},
	year = {2023},
	journal = {Sensors},
	volume = {23},
	number = {12},
	pages = {},
	doi = {10.3390/s23125461},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163955350&doi=10.3390%2Fs23125461&partnerID=40&md5=1bceea4aaa62bae6b2056c9a4a229965},
	affiliations = {Tianjin University of Technology, School of Electrical Engineering and Automation, Tianjin, Tianjin, China},
	abstract = {In recent years, there has been a growing interest in the study of emotion recognition through electroencephalogram (EEG) signals. One particular group of interest are individuals with hearing impairments, who may have a bias towards certain types of information when communicating with those in their environment. To address this, our study collected EEG signals from both hearing-impaired and non-hearing-impaired subjects while they viewed pictures of emotional faces for emotion recognition. Four kinds of feature matrices, symmetry difference, and symmetry quotient based on original signal and differential entropy (DE) were constructed, respectively, to extract the spatial domain information. The multi-axis self-attention classification model was proposed, which consists of local attention and global attention, combining the attention model with convolution through a novel architectural element for feature classification. Three-classification (positive, neutral, negative) and five-classification (happy, neutral, sad, angry, fearful) tasks of emotion recognition were carried out. The experimental results show that the proposed method is superior to the original feature method, and the multi-feature fusion achieved a good effect in both hearing-impaired and non-hearing-impaired subjects. The average classification accuracy for hearing-impaired subjects and non-hearing-impaired subjects was 70.2% (three-classification) and 50.15% (five-classification), and 72.05% (three-classification) and 51.53% (five-classification), respectively. In addition, by exploring the brain topography of different emotions, we found that the discriminative brain regions of the hearing-impaired subjects were also distributed in the parietal lobe, unlike those of the non-hearing-impaired subjects. © 2023 by the authors.},
	author_keywords = {EEG signals; emotion classification; emotion faces; hearing-impaired subjects; self-attention mechanism},
	keywords = {Audition; Biomedical signal processing; Brain; Classification (of information); Electroencephalography; Electrophysiology; Image classification; Speech recognition; Topography; Attention mechanisms; Electroencephalogram signals; Emotion classification; Emotion face; Emotion recognition; Feature matrices; Hearing impaired; Hearing impairments; Hearing-impaired subject; Self-attention mechanism; Emotion Recognition; brain; electroencephalography; emotion; fear; human; physiology; procedures; recognition; Emotions; Fear; Humans; Recognition, Psychology},
	correspondence_address = {Z. Li; Tianjin Key Laboratory for Control Theory and Applications in Complicated Systems, School of Electrical Engineering and Automation, Tianjin University of Technology, Tianjin, 300384, China; email: lzw

@ARTICLE{Zhao2023Epileptic,
	author = {Zhao, Xuyang and Yoshida, Noboru and Ueda, Tetsuya and Sugano, Hidenori Sugano and Tanaka, Toshihisa Tanaka},
	title = {Epileptic seizure detection by using interpretable machine learning models},
	year = {2023},
	journal = {Journal of Neural Engineering},
	volume = {20},
	number = {1},
	pages = {},
	doi = {10.1088/1741-2552/acb089},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148480090&doi=10.1088%2F1741-2552%2Facb089&partnerID=40&md5=5968663c33e34b15ae9096c2c3cf535e},
	affiliations = {Tokyo University of Agriculture and Technology, Department of Electrical Engineering and Computer Science, Fuchu, Tokyo, Japan; Nerima Hospital, Juntendo University School of Medicine, Tokyo, Japan; Juntendo University School of Medicine, Tokyo, Japan},
	abstract = {Objective. Accurate detection of epileptic seizures using electroencephalogram (EEG) data is essential for epilepsy diagnosis, but the visual diagnostic process for clinical experts is a time-consuming task. To improve efficiency, some seizure detection methods have been proposed. Regardless of traditional or machine learning methods, the results identify only seizures and non-seizures. Our goal is not only to detect seizures but also to explain the basis for detection and provide reference information to clinical experts. Approach. In this study, we follow the visual diagnosis mechanism used by clinical experts that directly processes plotted EEG image data and apply some commonly used models of LeNet, VGG, deep residual network (ResNet), and vision transformer (ViT) to the EEG image classification task. Before using these models, we propose a data augmentation method using random channel ordering (RCO), which adjusts the channel order to generate new images. The Gradient-weighted class activation mapping (Grad-CAM) and attention layer methods are used to interpret the models. Main results. The RCO method can balance the dataset in seizure and non-seizure classes. The models achieved good performance in the seizure detection task. Moreover, the Grad-CAM and attention layer methods explained the detection basis of the model very well and calculate a value that measures the seizure degree. Significance. Processing EEG data in the form of images can flexibility to use a variety of machine learning models. The imbalance problem that exists widely in clinical practice is well solved by the RCO method. Since the method follows the visual diagnosis mechanism of clinical experts, the model interpretation results can be presented to clinical experts intuitively, and the quantitative information provided by the model is also a good diagnostic reference. © 2023 The Author(s). Published by IOP Publishing Ltd.},
	author_keywords = {EEG; epilepsy; interpretable deep learning; seizure detection},
	keywords = {Cams; Data handling; Deep learning; Learning systems; Neurophysiology; Activation mapping; Channel order; Epilepsy; Epileptic seizure detection; Epileptic seizures; Interpretable deep learning; Layer method; Machine learning models; Random channel; Seizure-detection; Electroencephalography; adult; Article; clinical article; clinical practice; computer vision; convolutional neural network; deep learning; deep residual network; diagnostic accuracy; diagnostic test accuracy study; electroencephalogram; electroencephalography monitoring; entropy; epilepsy; false positive result; female; human; machine learning; male; middle aged; vision transformer; young adult; electroencephalography; procedures; seizure; signal processing; Humans; Machine Learning; Seizures; Signal Processing, Computer-Assisted},
	correspondence_address = {T. Tanaka; Department of Electrical Engineering and Computer Science, Tokyo University of Agriculture and Technology, Tokyo, Japan; email: tanakat

@ARTICLE{Altaheri2023Physicsinformed,
	author = {Altaheri, Hamdi and Muhammad, Ghulam and Alsulaiman, Mansour M.},
	title = {Physics-Informed Attention Temporal Convolutional Network for EEG-Based Motor Imagery Classification},
	year = {2023},
	journal = {IEEE Transactions on Industrial Informatics},
	volume = {19},
	number = {2},
	pages = {2249 - 2258},
	doi = {10.1109/TII.2022.3197419},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136127820&doi=10.1109%2FTII.2022.3197419&partnerID=40&md5=a1e379af1e1fc9f6e7e001b280f7149a},
	affiliations = {King Saud University, Department of Computer Engineering, Riyadh, Riyad, Saudi Arabia},
	abstract = {The brain-computer interface (BCI) is a cutting-edge technology that has the potential to change the world. Electroencephalogram (EEG) motor imagery (MI) signal has been used extensively in many BCI applications to assist disabled people, control devices or environments, and even augment human capabilities. However, the limited performance of brain signal decoding is restricting the broad growth of the BCI industry. In this article, we propose an attention-based temporal convolutional network (ATCNet) for EEG-based motor imagery classification. The ATCNet model utilizes multiple techniques to boost the performance of MI classification with a relatively small number of parameters. ATCNet employs scientific machine learning to design a domain-specific deep learning model with interpretable and explainable features, multihead self-attention to highlight the most valuable features in MI-EEG data, temporal convolutional network to extract high-level temporal features, and convolutional-based sliding window to augment the MI-EEG data efficiently. The proposed model outperforms the current state-of-the-art techniques in the BCI Competition IV-2a dataset with an accuracy of 85.38% and 70.97% for the subject-dependent and subject-independent modes, respectively. © 2005-2012 IEEE.},
	author_keywords = {Attention; classification; convolution neural network (CNN); deep learning; electroencephalography (EEG); motor imagery; scientific machine learning; temporal convolution networks (TCN)},
	keywords = {Brain; Brain computer interface; Brain mapping; Convolution; Data mining; Deep learning; Electrophysiology; Image classification; Job analysis; Neural networks; Attention; Brain modeling; Convolution neural network; Convolutional neural network; Features extraction; Machine-learning; Motor imagery; Scientific machine learning; Task analysis; Temporal convolution network; Electroencephalography},
	correspondence_address = {G. Muhammad; King Saud University, Department of Computer Engineering, College of Computer and Information Sciences, Riyadh, 11543, Saudi Arabia; email: ghulam

@ARTICLE{Zhang2023Spatiotemporal,
	author = {Zhang, Guangyi and Etemad, S. Ali},
	title = {Spatio-Temporal EEG Representation Learning on Riemannian Manifold and Euclidean Space},
	year = {2023},
	journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
	volume = {8},
	number = {2},
	pages = {1469 - 1483},
	doi = {10.1109/TETCI.2023.3332549},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178035482&doi=10.1109%2FTETCI.2023.3332549&partnerID=40&md5=a0772b75ce349ccff00f1b7eecf0b3c5},
	affiliations = {Queen’s University, Department of Electrical & Computer Engineering, Kingston, ON, Canada},
	abstract = {We present a novel deep neural architecture for learning electroencephalogram (EEG). To learn the spatial information, our model first obtains the Riemannian mean and distance from spatial covariance matrices (SCMs) on a Riemannian manifold. We then project the spatial information onto a Euclidean space via tangent space learning. Following, two fully connected layers are used to learn the spatial information embeddings. Moreover, our proposed method learns the temporal information via differential entropy and logarithm power spectrum density features extracted from EEG signals in a Euclidean space using a deep long short-term memory network with a soft attention mechanism. To combine the spatial and temporal information, we use an effective fusion strategy, which learns attention weights applied to embedding-specific features for decision making. We evaluate our proposed framework on four public datasets across three popular EEG-related tasks, notably emotion recognition, vigilance estimation, and motor imagery classification, containing various types of tasks such as binary classification, multi-class classification, and regression. Our proposed architecture outperforms other methods on SEED-VIG, and approaches the state-of-the-art on the other three datasets (SEED, BCI-IV 2 A, and BCI-IV 2B), showing the robustness of our framework in EEG representation learning. © 2023 IEEE.},
	author_keywords = {EEG representations learning; emotion recognition; motor imagery classification; riemannian manifold},
	keywords = {Biomedical signal processing; Classification (of information); Computer architecture; Covariance matrix; Decision making; Electrophysiology; Embeddings; Emotion Recognition; Feature extraction; Geometry; Image classification; Job analysis; Speech recognition; Electroencephalogram representation learning; Emotion recognition; Features extraction; Learn+; Manifold; Motor imagery classification; Neural-networks; Riemannian manifold; Task analysis; Electroencephalography},
	correspondence_address = {G. Zhang; The Department of Electrical and Computer Engineering, Ingenuity Labs Research Institute, Queen’s University, Kingston, K7L 3N9, Canada; email: guangyi.zhang

@ARTICLE{Wang2023Compact,
	author = {Wang, Ze and Wong, Chi Man and Wang, Boyu and Feng, Zhao and Cong, Fengyu and Wan, Feng},
	title = {Compact Artificial Neural Network Based on Task Attention for Individual SSVEP Recognition With Less Calibration},
	year = {2023},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {31},
	pages = {2525 - 2534},
	doi = {10.1109/TNSRE.2023.3276745},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161048228&doi=10.1109%2FTNSRE.2023.3276745&partnerID=40&md5=8b5b80ce2ac22609e25d89b2b18124d2},
	affiliations = {Macau University of Science and Technology, Macao Center for Mathematical Sciences, Taipa, Macao; University of Macau, Department of Electrical & Computer Engineering, Taipa, Macao; University of Macau, Centre for Cognitive and Brain Sciences, Taipa, Macao; Western University, Department of Computer Science, London, ON, Canada; Dalian University of Technology, School of Biomedical Engineering, Dalian, Liaoning, China},
	abstract = {Objective: Recently, artificial neural networks (ANNs) have been proven effective and promising for the steady-state visual evoked potential (SSVEP) target recognition. Nevertheless, they usually have lots of trainable parameters and thus require a significant amount of calibration data, which becomes a major obstacle due to the costly EEG collection procedures. This paper aims to design a compact network that can avoid the over-fitting of the ANNs in the individual SSVEP recognition. Method: This study integrates the prior knowledge of SSVEP recognition tasks into the attention neural network design. First, benefiting from the high model interpretability of the attention mechanism, the attention layer is applied to convert the operations in conventional spatial filtering algorithms to the ANN structure, which reduces network connections between layers. Then, the SSVEP signal models and the common weights shared across stimuli are adopted to design constraints, which further condenses the trainable parameters. Results: A simulation study on two widely-used datasets demonstrates the proposed compact ANN structure with proposed constraints effectively eliminates redundant parameters. Compared to existing prominent deep neural network (DNN)-based and correlation analysis (CA)-based recognition algorithms, the proposed method reduces the trainable parameters by more than 90% and 80% respectively, and boosts the individual recognition performance by at least 57% and 7% respectively. Conclusion: Incorporating the prior knowledge of task into the ANN can make it more effective and efficient. The proposed ANN has a compact structure with less trainable parameters and thus requires less calibration with the prominent individual SSVEP recognition performance. © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
	author_keywords = {Artificial neural network; attention layer; brain-computer interface; steady-state visual evoked potential},
	keywords = {Calibration; Deep neural networks; Electroencephalography; Electrophysiology; Interface states; Interfaces (computer); Job analysis; Network layers; Attention layer; Brain modeling; Correlation; Network-based; Neural networks structure; Performance; Prior-knowledge; Steady-state visual evoked potentials; Target recognition; Task analysis; Brain computer interface; Article; artificial neural network; attention; attention layer; calibration; correlation analysis; deep neural network; discriminant analysis; electroencephalogram; human; human experiment; learning algorithm; nerve cell network; simulation; steady state visual evoked potential; task performance; visual evoked potential; algorithm; electroencephalography; photostimulation; procedures; Algorithms; Brain-Computer Interfaces; Evoked Potentials, Visual; Humans; Neural Networks, Computer; Photic Stimulation},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	issn = {15344320},
	coden = {ITNSB},
	pmid = {37247319},
	language = {English},
	abbrev_source_title = {IEEE Trans. Neural Syst. Rehabil. Eng.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Wan2023Eegformer,
	author = {Wan, Zhijiang and Li, Manyu and Liu, Shichang and Huang, Jiajin and Tan, Hai and Duan, Wenfeng},
	title = {EEGformer: A transformer–based brain activity classification method using EEG signal},
	year = {2023},
	journal = {Frontiers in Neuroscience},
	volume = {17},
	pages = {},
	doi = {10.3389/fnins.2023.1148855},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152560053&doi=10.3389%2Ffnins.2023.1148855&partnerID=40&md5=bfd3a62c8921bc74881e58b23531e153},
	affiliations = {Nanchang University, Nanchang, Jiangxi, China; Nanchang University, School of Information Engineering, Nanchang, Jiangxi, China; Nanchang University, Industrial Institute of Artificial Intelligence, Nanchang, Jiangxi, China; Shaanxi Normal University, School of Computer Science, Xi'an, Shaanxi, China; Beijing University of Technology, Faculty of Information Technology, Beijing, China; Nanjing Audit University, School of Computer Science, Nanjing, Jiangsu, China},
	abstract = {Background: The effective analysis methods for steady-state visual evoked potential (SSVEP) signals are critical in supporting an early diagnosis of glaucoma. Most efforts focused on adopting existing techniques to the SSVEPs-based brain–computer interface (BCI) task rather than proposing new ones specifically suited to the domain. Method: Given that electroencephalogram (EEG) signals possess temporal, regional, and synchronous characteristics of brain activity, we proposed a transformer–based EEG analysis model known as EEGformer to capture the EEG characteristics in a unified manner. We adopted a one-dimensional convolution neural network (1DCNN) to automatically extract EEG-channel-wise features. The output was fed into the EEGformer, which is sequentially constructed using three components: regional, synchronous, and temporal transformers. In addition to using a large benchmark database (BETA) toward SSVEP-BCI application to validate model performance, we compared the EEGformer to current state-of-the-art deep learning models using two EEG datasets, which are obtained from our previous study: SJTU emotion EEG dataset (SEED) and a depressive EEG database (DepEEG). Results: The experimental results show that the EEGformer achieves the best classification performance across the three EEG datasets, indicating that the rationality of our model architecture and learning EEG characteristics in a unified manner can improve model classification performance. Conclusion: EEGformer generalizes well to different EEG datasets, demonstrating our approach can be potentially suitable for providing accurate brain activity classification and being used in different application scenarios, such as SSVEP-based early glaucoma diagnosis, emotion recognition and depression discrimination. © © 2023 Wan, Li, Liu, Huang, Tan and Duan.},
	author_keywords = {brain activity classification; deep learning; EEG characteristics; EEGformer; SSVEPs},
	keywords = {Article; artifact; brain function; clinical article; controlled study; convolutional neural network; deep learning; depression; electroencephalogram; entropy; Fourier transform; glaucoma; human; human experiment; learning algorithm; machine learning; measurement accuracy; nerve cell network; receiver operating characteristic; sensitivity and specificity; support vector machine; visual evoked potential},
	correspondence_address = {W. Duan; The First Affiliated Hospital of Nanchang University, Nanchang University, Nanchang, Jiangxi, China; email: ndyfy02345

@ARTICLE{Jia2023Model,
	author = {Jia, Hai and Yu, Shiqi and Yin, Shunjie and Liu, Lanxin and Yi, Chanlin and Xue, Kaiqing and Li, Fali and Yao, Dezhong and Xu, Peng and Zhang, Tao},
	title = {A Model Combining Multi Branch Spectral-Temporal CNN, Efficient Channel Attention, and LightGBM for MI-BCI Classification},
	year = {2023},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {31},
	pages = {1311 - 1320},
	doi = {10.1109/TNSRE.2023.3243992},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149176488&doi=10.1109%2FTNSRE.2023.3243992&partnerID=40&md5=ba5974faac540df05045101dbceb4359},
	affiliations = {Xihua University, Mental Health Education Center, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, MOE Key Lab for Neuroinformation, Chengdu, Sichuan, China; Xihua University, School of Computer and Software Engineering, Chengdu, Sichuan, China},
	abstract = {Accurately decoding motor imagery (MI) brain-computer interface (BCI) tasks has remained a challenge for both neuroscience research and clinical diagnosis. Unfortunately, less subject information and low signal-to-noise ratio of MI electroencephalography (EEG) signals make it difficult to decode the movement intentions of users. In this study, we proposed an end-to-end deep learning model, a multi-branch spectral-temporal convolutional neural network with channel attention and LightGBM model (MBSTCNN-ECA-LightGBM), to decode MI-EEG tasks. We first constructed a multi branch CNN module to learn spectral-temporal domain features. Subsequently, we added an efficient channel attention mechanism module to obtain more discriminative features. Finally, LightGBM was applied to decode the MI multi-classification tasks. The within-subject cross-session training strategy was used to validate classification results. The experimental results showed that the model achieved an average accuracy of 86% on the two-class MI-BCI data and an average accuracy of 74% on the four-class MI-BCI data, which outperformed current state-of-the-art methods. The proposed MBSTCNN-ECA-LightGBM can efficiently decode the spectral and temporal domain information of EEG, improving the performance of MI-based BCIs. © 2001-2011 IEEE.},
	author_keywords = {attention mechanism; deep learning; LightGBM; Motor imagery; spectral-temporal},
	keywords = {Biomedical signal processing; Classification (of information); Clinical research; Decoding; Deep learning; Electroencephalography; Electrophysiology; Image classification; Neural networks; Signal to noise ratio; Attention mechanisms; Clinical diagnosis; Convolutional neural network; Efficient channels; Interface data; Lightgbm; Motor imagery; Spectral-temporal; Temporal domain; Brain computer interface; Article; cognition; controlled study; convolutional neural network; deep learning; electroencephalography; Fourier transform; human; human experiment; imagery; mathematical model; mental performance; natural language processing; normal human; signal noise ratio; stacked autoencoder; steady state; visual evoked potential; wavelet transform},
	correspondence_address = {T. Zhang; Xihua University, Mental Health Education Center and School of Science, Chengdu, 610039, China; email: zhangtao1698

@ARTICLE{Ahn2023Multiscale,
	author = {Ahn, Hyung-ju and Lee, Dae-hyeok and Jeong, Ji-hoon and Lee, Seongwhan},
	title = {Multiscale Convolutional Transformer for EEG Classification of Mental Imagery in Different Modalities},
	year = {2023},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {31},
	pages = {646 - 656},
	doi = {10.1109/TNSRE.2022.3229330},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144786150&doi=10.1109%2FTNSRE.2022.3229330&partnerID=40&md5=447d4fcdc25ab293e461d3966d6a1985},
	affiliations = {Korea University, Department of Brain and Cognitive Engineering, Seoul, South Korea; Chungbuk National University, School of Computer Science, Cheongju, Chungcheongbuk-do, South Korea; Korea University, Department of Artificial Intelligence, Seoul, South Korea},
	abstract = {A new kind of sequence-to-sequence model called a transformer has been applied to electroencephalogram (EEG) systems. However, the majority of EEG-based transformer models have applied attention mechanisms to the temporal domain, while the connectivity between brain regions and the relationship between different frequencies have been neglected. In addition, many related studies on imagery-based brain-computer interface (BCI) have been limited to classifying EEG signals within one type of imagery. Therefore, it is important to develop a general model to learn various types of neural representations. In this study, we designed an experimental paradigm based on motor imagery, visual imagery, and speech imagery tasks to interpret the neural representations during mental imagery in different modalities. We conducted EEG source localization to investigate the brain networks. In addition, we propose the multiscale convolutional transformer for decoding mental imagery, which applies multi-head attention over the spatial, spectral, and temporal domains. The proposed network shows promising performance with 0.62, 0.70, and 0.72 mental imagery accuracy with the private EEG dataset, BCI competition IV 2a dataset, and Arizona State University dataset, respectively, as compared to the conventional deep learning models. Hence, we believe that it will contribute significantly to overcoming the limited number of classes and low classification performances in the BCI system. © 2001-2011 IEEE.},
	author_keywords = {Brain-computer interface; electroencephalogram; mental imagery; self-attention; transformer},
	keywords = {Brain; Convolution; Deep learning; Electroencephalography; Electrophysiology; Image classification; Interfaces (computer); Job analysis; Brain modeling; Features extraction; Mental imagery; Neural representations; Self–attention; Sequence models; Task analysis; Temporal domain; Transformer; Brain computer interface; adult; Article; attention; brain region; brainstorming; clinical article; controlled study; data analysis; deep learning; electric potential; electroencephalogram; electrooculogram; human; imagery; inferior frontal gyrus; inferior parietal lobule; low resolution brain electromagnetic tomography; magnetic field; male; mental imagery; nerve cell network; occipital gyrus; prefrontal cortex; reference electrode; somatosensory cortex; spatial attention; supplementary motor area},
	correspondence_address = {S.-W. Lee; Korea University, Department of Artificial Intelligence, Seoul, Seongbuk, 02841, South Korea; email: sw.lee

@ARTICLE{Song2023Eeg,
	author = {Song, Yonghao and Zheng, Qingqing and Liu, Bingchuan and Gao, Xiaorong Rong},
	title = {EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization},
	year = {2023},
	journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
	volume = {31},
	pages = {710 - 719},
	doi = {10.1109/TNSRE.2022.3230250},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144761752&doi=10.1109%2FTNSRE.2022.3230250&partnerID=40&md5=37b6b4a006ba24a15b9c27525933603b},
	affiliations = {Tsinghua School of Medicine, Department of Biomedical Engineering, Beijing, China; Shenzhen Institutes of Advanced Technology, Shenzhen, Guangdong, China},
	abstract = {Due to the limited perceptual field, convolutional neural networks (CNN) only extract local temporal features and may fail to capture long-term dependencies for EEG decoding. In this paper, we propose a compact Convolutional Transformer, named EEG Conformer, to encapsulate local and global features in a unified EEG classification framework. Specifically, the convolution module learns the low-level local features throughout the one-dimensional temporal and spatial convolution layers. The self-attention module is straightforwardly connected to extract the global correlation within the local temporal features. Subsequently, the simple classifier module based on fully-connected layers is followed to predict the categories for EEG signals. To enhance interpretability, we also devise a visualization strategy to project the class activation mapping onto the brain topography. Finally, we have conducted extensive experiments to evaluate our method on three public datasets in EEG-based motor imagery and emotion recognition paradigms. The experimental results show that our method achieves state-of-the-art performance and has great potential to be a new baseline for general EEG decoding. The code has been released in https://github.com/eeyhsong/EEG-Conformer. © 2001-2011 IEEE.},
	author_keywords = {brain-computer interface (BCI); EEG classification; motor imagery; self-attention; transformer},
	keywords = {Brain mapping; Convolution; Decoding; Emotion Recognition; Image classification; Interfaces (computer); Neural networks; Visualization; Brain-computer interface; Convolutional neural network; EEG classification; Global feature; Local feature; Long-term dependencies; Motor imagery; Self-attention; Temporal features; Transformer; Brain computer interface; Article; artificial neural network; classification algorithm; classifier; computer simulation; convolutional neural network; electroencephalogram; electroencephalography; entropy; glycolysis; human; human experiment; imagery; learning algorithm; machine learning; mathematical model; measurement accuracy; signal noise ratio; stimulus; support vector machine; topography; training; working memory; article; attention; emotion},
	correspondence_address = {X. Gao; Tsinghua University, School of Medicine, Department of Biomedical Engineering, Beijing, 100084, China; email: gxr-dea

@ARTICLE{Jia2023Excellent,
	author = {Jia, Xueyu and Song, Yonghao and Xie, Longhan},
	title = {Excellent fine-tuning: From specific-subject classification to cross-task classification for motor imagery},
	year = {2023},
	journal = {Biomedical Signal Processing and Control},
	volume = {79},
	pages = {},
	doi = {10.1016/j.bspc.2022.104051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136137395&doi=10.1016%2Fj.bspc.2022.104051&partnerID=40&md5=01cf8849aaf4b9b1944a951cca33019b},
	affiliations = {South China University of Technology, School of Intelligent Engineering, Guangzhou, Guangdong, China},
	abstract = {With the popularity of deep learning, motor imagery electroencephalogram (MI-EEG) recognition based on feature extractors and classifiers has performed well. However, the features extracted by most models are not discriminative enough and are limited to specific-subject classifi-cation. We proposed a novel model Metric-based Spatial Filtering Transformer (MSFT) that utilizes additive angular margin loss to enforce the deep model to improve inter-class separability while enhancing intra-class compactness. Besides, a data augmentation method called EEG pyramid was applied to the model. Our model not only outperforms many recent benchmarks in specific-subject classifi-cation, but also is used for cross-subject and even cross-task classification. We did some experiments using BCI competition IV 2a and 2b datasets to evaluate the average accuracy. The Specific-subject: 86.11 % for 2a, 88.39 % for 2b. The Cross-subject: 61.92 % for 2a. The Cross-task: training the feature extractor with 2a data and then fine-tuning the classifier with 2b can achieve an average accuracy of 83.38 %. Our method is more general than most benchmarks and can deal with different kinds of classification situations. © 2022 Elsevier Ltd},
	author_keywords = {Common spatial pattern; Cross subjects; Metric learning; Motor imagery; Transformer},
	keywords = {Deep learning; Image classification; Positive ions; Common spatial patterns; Cross subject; Feature classifiers; Feature extractor; Fine tuning; Metric learning; Motor imagery; Subject classification; Task classification; Transformer; Electroencephalography; Article; binary classification; chemical parameters; cross linking; deep learning; electroencephalogram; facial recognition; human; kappa statistics; motor imagery electroencephalogram; performance indicator; transfer of learning},
	correspondence_address = {L. Xie; School of Intelligent Engineering, South China University of Technology, Guangzhou, 510460, China; email: xielonghan

@ARTICLE{Li2023Parallel,
	author = {Li, Hongli and Chen, Hongyu and Jia, Ziyu and Zhang, Ronghua and Yin, Feichao},
	title = {A parallel multi-scale time-frequency block convolutional neural network based on channel attention module for motor imagery classification},
	year = {2023},
	journal = {Biomedical Signal Processing and Control},
	volume = {79},
	pages = {},
	doi = {10.1016/j.bspc.2022.104066},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135925248&doi=10.1016%2Fj.bspc.2022.104066&partnerID=40&md5=9d7dcec5e8c06bcf7180137b083ba5e6},
	affiliations = {Tiangong University, School of Control Science and Engineering, Tianjin, China; National University of Singapore, School of Computing, Singapore City, Singapore; Tiangong University, School of Artificial Intelligence, Tianjin, China},
	abstract = {The motor imagery brain-computer interface (MI-BCI) based on electroencephalography (EEG) enables direct communication between the human brain and external devices. In this paper, the MTFB-CNN, a parallel multi-scale time-frequency block convolutional neural network based on the channel attention module, is proposed for EEG signals decoding, which can adaptively extract the time, frequency, and time-frequency domain features through parallel multi-scale time-frequency blocks, and then fuses and filters the features through attention mechanism and residual module. Experimental results based on the BCI Competition IV 2a and 2b datasets and the high gamma dataset show that the model achieves the highest average accuracy and kappa compared with existing baseline models. The MTFB-CNN is a novel and effective end-to-end model for decoding EEG signals without complex signals pre-processing operations, which has multi-scale feature extraction capability, making it successful in MI-BCI applications. © 2022 Elsevier Ltd},
	author_keywords = {Attention mechanism; Brain-computer interface; Convolutional Neural Networks; Deep learning; Motor imagery},
	keywords = {Biomedical signal processing; Brain computer interface; Convolution; Convolutional neural networks; Decoding; Deep neural networks; Electrophysiology; Frequency domain analysis; Image classification; Attention mechanisms; Convolutional neural network; Deep learning; Direct communications; Human brain; Motor imagery; Motor imagery classification; Multi-scales; Network-based; Time frequency blocks; Electroencephalography; Article; attention; classification; convolutional neural network; electroencephalogram; feature extraction; feature selection; human; human experiment; imagery; normal human; signal processing},
	publisher = {Elsevier Ltd},
	issn = {17468094},
	language = {English},
	abbrev_source_title = {Biomed. Signal Process. Control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 51}
}

@ARTICLE{Taori2023Crosstask,
	author = {Taori, Trupti J. and Gupta, Shankar S. and Bhagat, Sandesh and Gajre, Suhas S. and Manthalkar, Ramchandra R.},
	title = {Cross-Task Cognitive Load Classification with Identity Mapping-Based Distributed CNN and Attention-Based RNN Using Gabor Decomposed Data Images},
	year = {2023},
	journal = {IETE Journal of Research},
	volume = {69},
	number = {12},
	pages = {8753 - 8769},
	doi = {10.1080/03772063.2022.2098191},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135211239&doi=10.1080%2F03772063.2022.2098191&partnerID=40&md5=894f3573f01661d52e69f2609801174c},
	affiliations = {Shri Guru Gobind Singhji Institute of Engineering and Technology, Department of Electronics and Telecommunication Engineering, Nanded, MH, India},
	abstract = {The cognitive workload is a key to developing a logical and conscious thinking system. Maintaining an optimum workload improves the performance of an individual. The individuals’ psycho-social factors are responsible for creating significant variability in the performance of a task, which poses a significant challenge in developing a consistent model for the classification of cross-task cognitive workload using physiological signal, Electroencephalogram (EEG). The primary focus of the proposed work is to develop a robust classification model CARNN, by employing the concatenated deep structure of distributed branches of convolutional neural networks with residual blocks through identity mappings, and recurrent neural network with an attention mechanism. EEG data is divided into milliseconds duration overlap segments. The segmented EEG data is converted into images using Gabor decomposition with two spatial frequency scales and four orientations and supplied as input to CARNN. The images are formed by interlacing the respective left and right electrode data to capture the data variations effectively. Efficient feature aggregation with learning of spatial and temporal domain discriminative features through Gabor decomposed data images improve the training of CARNN. CARNN achieves outstanding performance over traditional classifiers; support vector machine, k-nearest neighbor (KNN), ensemble subspace KNN and the pre-trained networks; AlexNet, ResNet18/50, VGG16/19, and Inception-v3. The proposed method results in 94.2%, 92.5%, 95.9%, 92.8%, 94.3% classification accuracy, specificity, sensitivity, precision, and F1-score, respectively. Two visual task levels apart in their complexity are used for cross-task classification of cognitive workload. The proposed method is validated on raw EEG data of 44 participants. © 2023 IETE.},
	author_keywords = {Attention mechanism; Cognitive load; Convolutional neural network (CNN); Cross- task; Electroencephalogram; Long short- term memory; Residual block},
	keywords = {Convolution; Convolutional neural networks; Electrophysiology; Image classification; Image enhancement; Mapping; Nearest neighbor search; Physiological models; Recurrent neural networks; Support vector machines; Attention mechanisms; Cognitive loads; Cognitive workloads; Convolutional neural network; Cross- task; Data images; Identity mappings; Performance; Residual block; Electroencephalography},
	correspondence_address = {T. Taori; Department of Electronics and Telecommunication, Shri Guru Gobind Singhji Institute of Engineering and Technology, Nanded, India; email: trupti.mohota

@ARTICLE{Wen2022New,
	author = {Wen, Yingtang and He, Wenjing and Zhang, Yuyan},
	title = {A new attention-based 3D densely connected cross-stage-partial network for motor imagery classification in BCI},
	year = {2022},
	journal = {Journal of Neural Engineering},
	volume = {19},
	number = {5},
	pages = {},
	doi = {10.1088/1741-2552/ac93b4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139380579&doi=10.1088%2F1741-2552%2Fac93b4&partnerID=40&md5=56bd01c109ae962c50fd61256c0333a1},
	affiliations = {Yanshan University, Qinhuangdao, Hebei, China; Yanshan University, Qinhuangdao, Hebei, China},
	abstract = {Objective. The challenge for motor imagery (MI) in brain-computer interface (BCI) systems is finding a reliable classification model that has high classification accuracy and excellent robustness. Currently, one of the main problems leading to degraded classification performance is the inaccuracy caused by nonstationarities and low signal-to-noise ratio in electroencephalogram (EEG) signals. Approach. This study proposes a novel attention-based 3D densely connected cross-stage-partial network (DCSPNet) model to achieve efficient EEG-based MI classification. This is an end-to-end classification model framework based on the convolutional neural network (CNN) architecture. In this framework, to fully utilize the complementary features in each dimension, the optimal features are extracted adaptively from the EEG signals through the spatial-spectral-temporal (SST) attention mechanism. The 3D DCSPNet is introduced to reduce the gradient loss by segmenting the extracted feature maps to strengthen the network learning capability. Additionally, the design of the densely connected structure increases the robustness of the network. Main results. The performance of the proposed method was evaluated using the BCI competition IV 2a and the high gamma dataset, achieving an average accuracy of 84.45% and 97.88%, respectively. Our method outperformed most state-of-the-art classification algorithms, demonstrating its effectiveness and strong generalization ability. Significance. The experimental results show that our method is promising for improving the performance of MI-BCI. As a general framework based on time-series classification, it can be applied to BCI-related fields. © 2022 IOP Publishing Ltd.},
	author_keywords = {attention mechanism; convolutional neural networks (CNN); electroencephalogram (EEG); motor imagery (MI)},
	keywords = {Biomedical signal processing; Brain computer interface; Convolution; Convolutional neural networks; Image classification; Signal to noise ratio; Attention mechanisms; Classification models; Convolutional neural network; Electroencephalogram; Electroencephalogram signals; Motor imagery; Motor imagery classification; Performance; Electroencephalography; Article; attention; classification algorithm; convolutional neural network; cross validation; data accuracy; data visualization; deep learning; electroencephalogram; experiment; feature extraction; human; human experiment; imagery; information processing; motor imagery; motor performance; network learning; normal human; performance; three-dimensional imaging; algorithm; brain computer interface; electroencephalography; imagination; procedures; Algorithms; Brain-Computer Interfaces; Imagination; Neural Networks, Computer},
	correspondence_address = {Y. Zhang; School of Electrical Engineering, Yanshan University, Qinghuangdao, Hebei, 066004, China; email: yyzhang

@ARTICLE{Liu2022Tcacnet,
	author = {Liu, Xiaolin and Shi, Rongye and Hui, Qianxin and Xu, Susu and Wang, Shuai and Na, Rui and Sun, Ying and Ding, Wenbo and Zheng, Dezhi and Chen, Xinlei},
	title = {TCACNet: Temporal and channel attention convolutional network for motor imagery classification of EEG-based BCI},
	year = {2022},
	journal = {Information Processing and Management},
	volume = {59},
	number = {5},
	pages = {},
	doi = {10.1016/j.ipm.2022.103001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132853751&doi=10.1016%2Fj.ipm.2022.103001&partnerID=40&md5=4d4be59bee6d9c0f11697af86294dc43},
	affiliations = {Beihang University, Beijing, China; Beijing Institute of Technology, Beijing, China; Stony Brook University, Stony Brook, NY, United States; Tsinghua University, Shenzhen International Graduate School, Beijing, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China},
	abstract = {Brain–computer interface (BCI) is a promising intelligent healthcare technology to improve human living quality across the lifespan, which enables assistance of movement and communication, rehabilitation of exercise and nerves, monitoring sleep quality, fatigue and emotion. Most BCI systems are based on motor imagery electroencephalogram (MI-EEG) due to its advantages of sensory organs affection, operation at free will and etc. However, MI-EEG classification, a core problem in BCI systems, suffers from two critical challenges: the EEG signal's temporal non-stationarity and the nonuniform information distribution over different electrode channels. To address these two challenges, this paper proposes TCACNet, a temporal and channel attention convolutional network for MI-EEG classification. TCACNet leverages a novel attention mechanism module and a well-designed network architecture to process the EEG signals. The former enables the TCACNet to pay more attention to signals of task-related time slices and electrode channels, supporting the latter to make accurate classification decisions. We compare the proposed TCACNet with other state-of-the-art deep learning baselines on two open source EEG datasets. Experimental results show that TCACNet achieves 11.4% and 7.9% classification accuracy improvement on two datasets respectively. Additionally, TCACNet achieves the same accuracy as other baselines with about 50% less training data. In terms of classification accuracy and data efficiency, the superiority of the TCACNet over advanced baselines demonstrates its practical value for BCI systems. © 2022 The Author(s)},
	author_keywords = {Attention mechanism; Brain–computer interface; Deep learning; Electroencephalogram; Motor imagery classification},
	keywords = {Biomedical signal processing; Brain computer interface; Classification (of information); Convolution; Convolutional neural networks; Deep learning; Electrodes; Image classification; Network architecture; Attention mechanisms; Classification accuracy; Convolutional networks; EEG signals; Healthcare technology; Interface system; Living quality; Motor imagery; Motor imagery classification; Electroencephalography},
	correspondence_address = {D. Zheng; Beihang University, Beijing, Xueyuan Road No. 37, 100191, China; email: zhengdezhi

@ARTICLE{Bagchi2022Eegconvtransformer,
	author = {Bagchi, Subhranil and Bathula, Deepti R.},
	title = {EEG-ConvTransformer for single-trial EEG-based visual stimulus classification},
	year = {2022},
	journal = {Pattern Recognition},
	volume = {129},
	pages = {},
	doi = {10.1016/j.patcog.2022.108757},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130825163&doi=10.1016%2Fj.patcog.2022.108757&partnerID=40&md5=dfc05cb7c5cf2dbb31dadf6c1260ab8f},
	affiliations = {Indian Institute of Technology Ropar, Department of Computer Science and Engineering, Rupnagar, PB, India},
	abstract = {Different categories of visual stimuli evoke distinct activation patterns in the human brain. These patterns can be captured with EEG for utilization in application such as Brain-Computer Interface (BCI). However, accurate classification of these patterns acquired using single-trial data is challenging due to the low signal-to-noise ratio of EEG. Recently, deep learning-based transformer models with multi-head self-attention have shown great potential for analyzing variety of data. This work introduces an EEG-ConvTranformer network that is based on both multi-headed self-attention and temporal convolution. The novel architecture incorporates self-attention modules to capture inter-region interaction patterns and convolutional filters to learn temporal patterns in a single module. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over state-of-the-art techniques across five different visual stimulus classification tasks. Finally, quantitative analysis of inter-head diversity also shows low similarity in representational space, emphasizing the implicit diversity of multi-head attention. © 2022},
	author_keywords = {Deep learning; EEG; Head representations; Inter-head diversity; Inter-region similarity; Multi-head attention; Temporal convolution; Transformer; Visual stimulus classification},
	keywords = {Brain computer interface; Deep learning; Signal to noise ratio; Head representation; Inter-head diversity; Inter-region similarity; Multi-head attention; Region similarity; Temporal convolution; Transformer; Visual stimulus; Visual stimulus classification; Convolution},
	correspondence_address = {D.R. Bathula; Department of Computer Science and Engineering, Indian Institute of Technology Ropar, Rupnagar, Bara Phool, Punjab, 140001, India; email: bathula

@ARTICLE{Tekalp2022Deep,
	author = {Tekalp, Murat A.},
	title = {Deep Learning for Image/Video Restoration and Super-resolution},
	year = {2022},
	journal = {Foundations and Trends in Computer Graphics and Vision},
	volume = {13},
	number = {1},
	pages = {1 - 110},
	doi = {10.1561/0600000100},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130244443&doi=10.1561%2F0600000100&partnerID=40&md5=d998cebcbf33d160e4f611a9aa9f2562},
	affiliations = {Koç University, Department of Electrical and Electronic Engineering, Istanbul, Turkey},
	abstract = {Recent advances in neural signal processing led to significant improvements in the performance of learned image/video restoration and super-resolution (SR). An important benefit of data-driven deep learning approaches to image processing is that neural models can be optimized for any differentiable loss function, including perceptual loss functions, leading to perceptual image/video restoration and SR, which cannot be easily handled by traditional model-based methods. We start with a brief problem statement and a short discussion on traditional vs. data-driven solutions. We next review recent advances in neural architectures, such as residual blocks, dense connections, residual-in-residual dense blocks, residual blocks with generative neurons, self-attention and visual transformers. We then discuss loss functions and evaluation (assessment) criteria for image/video restoration and SR, including fidelity (distortion) and perceptual criteria, and the relation between them, where we briefly review the perception vs. distortion trade-off. We can consider learned image/video restoration and SR as learning either a nonlinear regressive mapping from degraded to ideal images based on the universal approximation theorem, or a generative model that captures the probability distribution of ideal images. We first review regressive inference via residual and/or dense convolutional networks (ConvNet). We also show that using a new architecture with residual blocks based on a generative neuron model can outperform classical residual ConvNets in peak-signal-to-noise ratio (PSNR). We next discuss generative inference based on adversarial training, such as SRGAN and ESRGAN, which can reproduce realistic textures, or based on normalizing flow such as SRFlow by optimizing log-likelihood. We then discuss problems in applying supervised training to real-life restoration and SR, including overfitting image priors and overfitting the degradation model seen in the training set. We introduce multiple-model SR and real-world SR (from unpaired training data) formulations to overcome these problems. Integration of traditional model-based methods and deep learning for non-blind restoration/SR is introduced as another solution to model overfitting in supervised learning. In learned video restoration and SR (VSR), we first discuss how to best exploit temporal correlations in video, including sliding temporal window vs. recurrent architectures for propagation, and aligning frames in the pixel domain using optical flow vs. in the feature space using deformable convolutions. We next introduce early fusion with feature-space alignment, employed by the EDVR network, which obtains excellent PSNR performance. However, it is well-known that videos with the highest PSNR may not be the most appealing to humans, since minimizing the mean-square error may result in blurring of details. We then address perceptual optimization of VSR models to obtain natural texture and motion. Although perception-distortion tradeoff has been well studied for images, few works address perceptual VSR. In addition to using perceptual losses, such as MS-SSIM, LPIPS, and/or adversarial training, we also discuss explicit loss functions/criteria to enforce and evaluate temporal consistency. We conclude with a discussion of open problems. © 2022 A. M. Tekalp.},
	keywords = {Backpropagation; Convolution; Data handling; Deep learning; Economic and social effects; Image enhancement; Image reconstruction; Network architecture; Optical data processing; Optical resolving power; Probability distributions; Signal to noise ratio; Textures; Data driven; Feature space; Ideal images; Loss functions; Model-based method; Overfitting; Peak signal to noise ratio; Superresolution; Traditional models; Video restoration; Restoration},
	correspondence_address = {A.M. Tekalp; Department of Electrical and Electronics Engineering, Koç University, Turkey; email: mtekalp

@ARTICLE{Yu2022Motor,
	author = {Yu, Zihang and Chen, Wanzhong and Zhang, Tao},
	title = {Motor imagery EEG classification algorithm based on improved lightweight feature fusion network},
	year = {2022},
	journal = {Biomedical Signal Processing and Control},
	volume = {75},
	pages = {},
	doi = {10.1016/j.bspc.2022.103618},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126275538&doi=10.1016%2Fj.bspc.2022.103618&partnerID=40&md5=769784a207c872ebdc24dce15e9170ba},
	affiliations = {Jilin University, College of Communication Engineering, Changchun, Jilin, China},
	abstract = {When deep learning techniques are introduced for Motor Imagery(MI) EEG signal classification, a multitude of state-of-the-art models, cannot be trained effectively because of the relatively small datasets. Proposing a model specialized for MI EEG signals classification plays a prominent role in promoting the combination of deep learning technology and MI EEG signal classification. In this paper, a novel Lightweight Feature Fusion Network(LFANN) based on an improved attention mechanism and tensor decomposition approach has been introduced. The proposed algorithm has been evaluated on a public benchmark dataset from BCI Competition IV, and the original dataset has been augmented with Enhance-Super-Resolution Generative Adversarial Network(ESRGAN). The experimental results demonstrate that the average accuracy of 91.58% and the average Kappa value of 0.881 can be achieved through the proposed algorithm. Furthermore, the compressed LAFFN, whose parameters have been compressed nearly ten times, creates no significant difference in performance compared to LAFFN. The investigation carried out through this experiment has provided novel insights into the classification research for MI EEG signals. © 2022 Elsevier Ltd},
	author_keywords = {Attention mechanism; Data augmentation; Deep learning; motor imagery; Tensor decomposition},
	keywords = {Biomedical signal processing; Classification (of information); Deep learning; Image classification; Image enhancement; Tensors; Attention mechanisms; Classification algorithm; Data augmentation; EEG classification; EEG signals classification; Features fusions; Motor imagery; Motor imagery EEG; Tensor decomposition; Generative adversarial networks; article; attention; classification algorithm; competition; decomposition; deep learning; electroencephalogram; human; human experiment; imagery},
	correspondence_address = {W. Chen; College of Communication Engineering, Jilin University, Changchun, 130025, China; email: chenwz

@ARTICLE{Alquraishi2022Decoding,
	author = {Al-Quraishi, Maged S. and Elamvazhuthi, I. and Tang, Tong Boon and Al-Qurishi, Muhammad S. and Adil, Syed Hasan and Ebrahim, Mansoor and Borboni, Alberto},
	title = {Decoding the User's Movements Preparation From EEG Signals Using Vision Transformer Architecture},
	year = {2022},
	journal = {IEEE Access},
	volume = {10},
	pages = {109446 - 109459},
	doi = {10.1109/ACCESS.2022.3213996},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139856720&doi=10.1109%2FACCESS.2022.3213996&partnerID=40&md5=569871672fcc41d4cd189013640058dd},
	affiliations = {Universiti Teknologi PETRONAS, Department of Electrical and Electronic Engineering, Seri Iskandar, Perak, Malaysia; Thamar University, Faculty of Engineering, Thamar, Yemen; Elm Company, Department of Research, Riyadh, Riyad, Saudi Arabia; Iqra University, Sciences and Technology, Karachi, Pakistan; Università degli Studi di Brescia, Department of Mechanical and Industrial Engineering, Brescia, BS, Italy},
	abstract = {Electroencephalography (EEG) signals have a major impact on how well assistive rehabilitation devices work. These signals have become a common technique in recent studies to investigate human motion functions and behaviors. However, incorporating EEG signals to investigate motor planning or movement intention could benefit all patients who can plan motion but are unable to execute it. In this paper, the movement planning of the lower limb was investigated using EEG signal and bilateral movements were employed, including dorsiflexion and plantar flexion of the right and left ankle joint movements. The proposed system uses Continuous Wavelet Transform (CWT) to generate a time-frequency (TF) map of each EEG signal in the motor cortex and then uses the extracted images as input to a deep learning model for classification. Deep Learning (DL) models are created based on vision transformer architecture (ViT) which is the state-of-the-art of image classification and also the proposed models were compared with residual neural network (ResNet). The proposed technique reveals a significant classification performance for the multiclass problem (p < 0.0001) where the classification accuracy was 97.33±1.86 % and the F score, recall and precision were 97.32±1.88 %, 97.30±1.90 % and 97.36±1.81 % respectively. These results show that DL is a promising technique that can be applied to investigate the user's movements intention from EEG signals and highlight the potential of the proposed model for the development of future brain-machine interface (BMI) for neurorehabilitation purposes. © 2013 IEEE.},
	author_keywords = {Continuous wavelet transform; deep learning; electroencephalography; motor-related cortical potentials; vision transformers architecture},
	keywords = {Behavioral research; Biomedical signal processing; Brain computer interface; Deep learning; Electrophysiology; Network architecture; Wavelet transforms; Brain modeling; Continuous Wavelet Transform; Cortical potentials; Features extraction; Foot; Motor-related cortical potential; Task analysis; Transformer; Vision transformer architecture; Electroencephalography},
	correspondence_address = {I. Elamvazuthi; Universiti Teknologi Petronas, Smart Assistive and Rehabilitative Technology (SMART) Research Group, Department of Electrical and Electronic Engineering, Seri Iskandar, Bandar, 32610, Malaysia; email: irraivan_elamvazuthi

@ARTICLE{Li2022Attentionbased,
	author = {Li, Li and Sun, Nan},
	title = {Attention-Based DSC-ConvLSTM for Multiclass Motor Imagery Classification},
	year = {2022},
	journal = {Computational Intelligence and Neuroscience},
	volume = {2022},
	pages = {},
	doi = {10.1155/2022/8187009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130067158&doi=10.1155%2F2022%2F8187009&partnerID=40&md5=0a89653f0ddb58a52ab4b4717ea0469a},
	affiliations = {Beijing University of Posts and Telecommunications, Beijing, Beijing, China},
	abstract = {With the rapid development of deep learning, researchers have gradually applied it to motor imagery brain computer interface (MI-BCI) and initially demonstrated its advantages over traditional machine learning. However, its application still faces many challenges, and the recognition rate of electroencephalogram (EEG) is still the bottleneck restricting the development of MI-BCI. In order to improve the accuracy of EEG classification, a DSC-ConvLSTM model based on the attention mechanism is proposed for the multi-classification of motor imagery EEG signals. To address the problem of the small sample size of well-labeled and accurate EEG data, the preprocessing uses sliding windows for data augmentation, and the average prediction loss of each sliding window is used as the final prediction loss for that trial. This not only increases the training sample size and is beneficial to train complex neural network models, but also the network no longer extracts the global features of the whole trial so as to avoid learning the difference features among trials, which can effectively eliminate the influence of individual specificity. In the aspect of feature extraction and classification, the overall network structure is designed according to the characteristics of the EEG signals in this paper. Firstly, depth separable convolution (DSC) is used to extract spatial features of EEG signals. On the one hand, this reduces the number of parameters and improves the response speed of the system. On the other hand, the network structure we designed is more conducive to extract directly the direct extraction of spatial features of EEG signals. Secondly, the internal structure of the Long Short-Term Memory (LSTM) unit is improved by using convolution and attention mechanism, and a novel bidirectional convolution LSTM (ConvLSTM) structure is proposed by comparing the effects of embedding convolution and attention mechanism in the input and different gates, respectively. In the ConvLSTM module, the convolutional structure is only introduced into the input-to-state transition, while the gates still remain the original fully connected mechanism, and the attention mechanism is introduced into the input to further improve the overall decoding performance of the model. This bidirectional ConvLSTM extracts the time-domain features of EEG signals and integrates the feature extraction capability of the CNN and the sequence processing capability of LSTM. The experimental results show that the average classification accuracy of the model reaches 73.7% and 92.6% on two datasets, BCI Competition IV Dataset 2a and High Gamma Dataset, respectively, which proves the robustness and effectiveness of the model we proposed. It can be seen that the model in this paper can deeply excavate significant EEG features from the original EEG signals, show good performance in different subjects and different datasets, and improve the influence of individual variability on the classification performance, which is of practical significance for promoting the development of brain-computer interface technology towards a practical and marketable direction. © 2022 Li Li and Nan Sun.},
	keywords = {Biomedical signal processing; Brain; Brain computer interface; Classification (of information); Decoding; Electroencephalography; Extraction; Feature extraction; Image classification; Image enhancement; Long short-term memory; Attention mechanisms; Electroencephalogram signals; ITS applications; Model-based OPC; Motor imagery; Motor imagery classification; Multi-classification; Network structures; Sliding Window; Spatial features; Convolution; algorithm; brain computer interface; electroencephalography; human; imagination; procedures; Algorithms; Brain-Computer Interfaces; Humans; Imagination; Neural Networks, Computer},
	correspondence_address = {L. Li; State Key Laboratory Of Networking And Switching Technology, Beijing Laboratory Of Advanced Information Networks, Beijing University Of Posts And Telecommunications, Beijing, China; email: lili66

@ARTICLE{Gao2021Attentionbased,
	author = {Gao, Zhongke and Sun, Xinlin and Liu, Mingxu and Dang, Weidong and Ma, Chao and Chen, Guanrong (Ron)},
	title = {Attention-Based Parallel Multiscale Convolutional Neural Network for Visual Evoked Potentials EEG Classification},
	year = {2021},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {25},
	number = {8},
	pages = {2887 - 2894},
	doi = {10.1109/JBHI.2021.3059686},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100933990&doi=10.1109%2FJBHI.2021.3059686&partnerID=40&md5=672af9ba6e5cd0d4e90f6e7537ef4f69},
	affiliations = {Tianjin University, Tianjin, China; City University of Hong Kong, Department of Electrical Engineering, Hong Kong, Hong Kong; Ministry of Education of the People's Republic of China, Beijing, Beijing, China},
	abstract = {Electroencephalography (EEG) decoding is an important part of Visual Evoked Potentials-based Brain-Computer Interfaces (BCIs), which directly determines the performance of BCIs. However, long-time attention to repetitive visual stimuli could cause physical and psychological fatigue, resulting in weaker reliable response and stronger noise interference, which exacerbates the difficulty of Visual Evoked Potentials EEG decoding. In this state, subjects' attention could not be concentrated enough and the frequency response of their brains becomes less reliable. To solve these problems, we propose an attention-based parallel multiscale convolutional neural network (AMS-CNN). Specifically, the AMS-CNN first extract robust temporal representations via two parallel convolutional layers with small and large temporal filters respectively. Then, we employ two sequential convolution blocks for spatial fusion and temporal fusion to extract advanced feature representations. Further, we use attention mechanism to weight the features at different moments according to the output-related interest. Finally, we employ a full connected layer with softmax activation function for classification. Two fatigue datasets collected from our lab are implemented to validate the superior classification performance of the proposed method compared to the state-of-the-art methods. Analysis reveals the competitiveness of multiscale convolution and attention mechanism. These results suggest that the proposed framework is a promising solution to improving the decoding performance of Visual Evoked Potential BCIs. © 2013 IEEE.},
	author_keywords = {Attention mechanism; brain-computer interface (BCI); convolutional neural network; fatigue; visual evoked potentials},
	keywords = {Brain computer interface; Classification (of information); Convolution; Decoding; Electroencephalography; Electrophysiology; Frequency response; Attention mechanisms; Brain computer interfaces (BCIs); Classification performance; Decoding performance; Feature representation; State-of-the-art methods; Temporal representations; Visual evoked potential; Convolutional neural networks; adult; Article; artificial neural network; convolutional neural network; dysthymia; electroencephalography; entropy; event related potential; fatigue; feature extraction; female; Fourier transform; functional connectivity; functional magnetic resonance imaging; human; learning algorithm; machine learning; male; mathematical model; nerve cell network; questionnaire; signal noise ratio; support vector machine; visual evoked potential; visual field; visual stimulation; young adult; algorithm; brain; brain computer interface; Algorithms; Brain; Brain-Computer Interfaces; Evoked Potentials, Visual; Humans; Neural Networks, Computer},
	correspondence_address = {C. Ma; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; email: chao.ma

@ARTICLE{Zhang2021Affective,
	author = {Zhang, Yunxia and Li, Xin and Zhao, Changming and Zheng, Wenyin and Wang, Manqing and Zhang, Yongqing and Ma, Hongjiang and Gao, Dongrui},
	title = {Affective EEG-Based Person Identification Using Channel Attention Convolutional Neural Dense Connection Network},
	year = {2021},
	journal = {Security and Communication Networks},
	volume = {2021},
	pages = {},
	doi = {10.1155/2021/7568460},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120911582&doi=10.1155%2F2021%2F7568460&partnerID=40&md5=f1af02d1ceda6a35c9a908a45a819177},
	affiliations = {Chengdu University of Information Technology, School of Computer Science, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, School of Life Science and Technology, Chengdu, Sichuan, China},
	abstract = {In the biometric recognition mode, the use of electroencephalogram (EEG) for biometric recognition has many advantages such as anticounterfeiting and nonsteal ability. Compared with traditional biometrics, EEG biometric recognition is safer and more concealed. Generally, EEG-based biometric recognition is to perform person identification (PI) through EEG signals collected by performing motor imagination and visual evoked tasks. The aim of this paper is to improve the performance of different affective EEG-based PI using a channel attention mechanism of convolutional neural dense connection network (CADCNN net) approach. Channel attention mechanism (CA) is used to handle the channel information from the EEG, while convolutional neural dense connection network (DCNN net) extracts the unique biological characteristics information for PI. The proposed method is evaluated on the state-of-the-art affective data set HEADIT. The results indicate that CADCNN net can perform PI from different affective states and reach up to 95%-96% mean correct recognition rate. This significantly outperformed a random forest (RF) and multilayer perceptron (MLP). We compared our method with the state-of-the-art EEG classifiers and models of EEG biometrics. The results show that the further extraction of the feature matrix is more robust than the direct use of the feature matrix. Moreover, the CADCNN net can effectively and efficiently capture discriminative traits, thus generalizing better over diverse human states. © 2021 Yunxia Zhang et al.},
	keywords = {Biometrics; Convolution; Convolutional neural networks; Decision trees; Anti-counterfeiting; Attention mechanisms; Biological characteristic; Biometric recognition; Channel information; Electroencephalogram signals; Feature matrices; Performance; Person identification; State of the art; Electroencephalography},
	correspondence_address = {D. Gao; School of Computer Science, Chengdu University of Information Technology, Chengdu, 610225, China; email: 735913121

@ARTICLE{Zheng2021Attentionbased,
	author = {Zheng, Xiao and Chen, Wanzhong},
	title = {An Attention-based Bi-LSTM Method for Visual Object Classification via EEG},
	year = {2021},
	journal = {Biomedical Signal Processing and Control},
	volume = {63},
	pages = {},
	doi = {10.1016/j.bspc.2020.102174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090752054&doi=10.1016%2Fj.bspc.2020.102174&partnerID=40&md5=969dc4f2398370ff3b64c3b825da88dd},
	affiliations = {Jilin University, College of Communication Engineering, Changchun, Jilin, China},
	abstract = {Background and Objective: Despite many models have been proposed for brain visual perception and content understanding via electroencephalograms (EEGs), due to the lack of research on the inherent temporal relationship, EEG-based visual object classification still demands the improvement on its accuracy and computation complexity. Methods: To take full advantage of the uneven visual feature saturation between time segments, an end-to-end attention-based Bi-LSTM Method is proposed, named Bi-LSTM-AttGW. Two attention strategies are introduced to Bi-LSTM framework. The attention gate replaces the forget gate in traditional LSTM. It is only relevant to the historical cell state, and not related to the current input. Hence, the attention gate can greatly reduce the number of training parameters. Moreover, the attention weighting method is applied to Bi-LSTM output, and it can explore the most decisive information. Results: The best classification accuracy achieved by Bi-LSTM-AttGW model is 99.50%. Compared with the state-of-art algorithms and baseline models, the proposed method has great advantages in classification performance and computational complexity. Considering brain region level contribution on visual cognition task, we also verify our method using EEG signals collected from the frontal and occipital regions, that are highly correlated with visual perception tasks. Conclusions: The results show promise towards the idea that human brain activity related to visual recognition can be more effectively decoded by neural networks with neural mechanism. The experimental results not only could provide strong support for the modularity theory about the brain cognitive function, but show the superiority of the proposed Bi-LSTM model with attention mechanism again. © 2020 Elsevier Ltd},
	author_keywords = {Attention mechanism; Bi-LSTM; Deep learning; EEG; Visual perception},
	keywords = {Bioelectric phenomena; Brain; Complex networks; Electroencephalography; Vision; Attention mechanisms; Classification accuracy; Classification performance; Computation complexity; Decisive information; Temporal relationships; Training parameters; Visual object classification; Long short-term memory; algorithm; article; attention; brain region; controlled study; deep learning; electroencephalogram; frontal cortex; human; human experiment; long short term memory network; nerve cell network; occipital cortex; visual memory},
	correspondence_address = {W. Chen; College of Communication Engineering, Jilin University, Changchun, 130012, China; email: chenwz

